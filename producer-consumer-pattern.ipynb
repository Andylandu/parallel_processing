{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279929b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:10.326011Z",
     "iopub.status.busy": "2025-03-04T18:21:10.325711Z",
     "iopub.status.idle": "2025-03-04T18:21:16.597451Z",
     "shell.execute_reply": "2025-03-04T18:21:16.596366Z"
    },
    "papermill": {
     "duration": 6.277789,
     "end_time": "2025-03-04T18:21:16.599001",
     "exception": false,
     "start_time": "2025-03-04T18:21:10.321212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\r\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\r\n",
      "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pymupdf\r\n",
      "Successfully installed pymupdf-1.25.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc4c52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:16.607523Z",
     "iopub.status.busy": "2025-03-04T18:21:16.607234Z",
     "iopub.status.idle": "2025-03-04T18:21:17.557016Z",
     "shell.execute_reply": "2025-03-04T18:21:17.556209Z"
    },
    "papermill": {
     "duration": 0.955657,
     "end_time": "2025-03-04T18:21:17.558701",
     "exception": false,
     "start_time": "2025-03-04T18:21:16.603044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "# import argparse\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28ddecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:17.567066Z",
     "iopub.status.busy": "2025-03-04T18:21:17.566670Z",
     "iopub.status.idle": "2025-03-04T18:21:17.570781Z",
     "shell.execute_reply": "2025-03-04T18:21:17.569792Z"
    },
    "papermill": {
     "duration": 0.009434,
     "end_time": "2025-03-04T18:21:17.571930",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.562496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = '/kaggle/input/dataset-125-pdf-files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce1e168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:17.579928Z",
     "iopub.status.busy": "2025-03-04T18:21:17.579682Z",
     "iopub.status.idle": "2025-03-04T18:21:17.582558Z",
     "shell.execute_reply": "2025-03-04T18:21:17.581909Z"
    },
    "papermill": {
     "duration": 0.008341,
     "end_time": "2025-03-04T18:21:17.584121",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.575780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "# CPU-bound code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4bd66",
   "metadata": {
    "papermill": {
     "duration": 0.003155,
     "end_time": "2025-03-04T18:21:17.591164",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.588009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create a producer to add the pdf files to the shared queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b022b997",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:17.598957Z",
     "iopub.status.busy": "2025-03-04T18:21:17.598656Z",
     "iopub.status.idle": "2025-03-04T18:21:17.602909Z",
     "shell.execute_reply": "2025-03-04T18:21:17.602069Z"
    },
    "papermill": {
     "duration": 0.009512,
     "end_time": "2025-03-04T18:21:17.604054",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.594542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def producer(task_queue, pdf_directory):\n",
    "    \"\"\"Enqueue paths of all PDFs in the directory.\"\"\"\n",
    "    for root, _, files in os.walk(pdf_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                task_queue.put(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ea396",
   "metadata": {
    "papermill": {
     "duration": 0.003022,
     "end_time": "2025-03-04T18:21:17.610665",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.607643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create the consumers to take the PDF files from the shared queue (task_queue), process them and then send the extracted text to the results queue (results_queue) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2189fe06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:17.618218Z",
     "iopub.status.busy": "2025-03-04T18:21:17.617898Z",
     "iopub.status.idle": "2025-03-04T18:21:17.622390Z",
     "shell.execute_reply": "2025-03-04T18:21:17.621752Z"
    },
    "papermill": {
     "duration": 0.009333,
     "end_time": "2025-03-04T18:21:17.623439",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.614106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consumer(task_queue, results_queue):\n",
    "    \"\"\"Dequeue PDFs, process them and send extracted text to results_queue.\"\"\"    \n",
    "    while True:\n",
    "        pdf_path = task_queue.get()\n",
    "        if pdf_path is None:  # Poison pill received\n",
    "            task_queue.task_done()\n",
    "            results_queue.put(None)  # Send a sentinel value to the main process to signal completion\n",
    "            break\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                text = \" \"\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "                results_queue.put({\n",
    "                    \"filename\": os.path.basename(pdf_path),\n",
    "                    \"text\": text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "        finally:\n",
    "            task_queue.task_done()  # Mark task as complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9021e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:17.630471Z",
     "iopub.status.busy": "2025-03-04T18:21:17.630240Z",
     "iopub.status.idle": "2025-03-04T18:21:17.635043Z",
     "shell.execute_reply": "2025-03-04T18:21:17.634374Z"
    },
    "papermill": {
     "duration": 0.009484,
     "end_time": "2025-03-04T18:21:17.636141",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.626657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(pdf_directory, num_consumers):\n",
    "    task_queue = mp.JoinableQueue() # The shared queue (task_queue)\n",
    "    results_queue = mp.Queue()  # Queue to collect results\n",
    "    \n",
    "    # Start producer process\n",
    "    producer_proc = mp.Process(target=producer, args=(task_queue, pdf_directory))\n",
    "    producer_proc.start()\n",
    "\n",
    "    # Start consumer processes (daemonized to exit on main completion)\n",
    "    consumers = []\n",
    "    for _ in range(num_consumers):\n",
    "        cons = mp.Process(target=consumer, args=(task_queue,results_queue))\n",
    "        cons.daemon = True\n",
    "        cons.start()\n",
    "        consumers.append(cons)\n",
    "\n",
    "    # Wait for producer to finish enqueuing files\n",
    "    producer_proc.join()\n",
    "\n",
    "    # Add poison pills to task queue in order to stop consumers\n",
    "    for _ in range(num_consumers):\n",
    "        task_queue.put(None)\n",
    "\n",
    "    # Wait for all tasks (including poison pills) to complete\n",
    "    task_queue.join()\n",
    "\n",
    "    # Collect results and handle sentinels\n",
    "    data_text = []\n",
    "    sentinel_count = 0\n",
    "    while sentinel_count < num_consumers:\n",
    "        result = results_queue.get()\n",
    "        if result is None:\n",
    "            sentinel_count += 1\n",
    "        else:\n",
    "            data_text.append(result)\n",
    "\n",
    "    print(f'All PDFs were processed successfully and the process time is {10}.')\n",
    "    return data_text  # Now accessible in the main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd601ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:21:17.643216Z",
     "iopub.status.busy": "2025-03-04T18:21:17.643016Z",
     "iopub.status.idle": "2025-03-04T18:22:14.201751Z",
     "shell.execute_reply": "2025-03-04T18:22:14.198348Z"
    },
    "papermill": {
     "duration": 56.568051,
     "end_time": "2025-03-04T18:22:14.207534",
     "exception": false,
     "start_time": "2025-03-04T18:21:17.639483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of consumers is 4.\n",
      "All PDFs were processed successfully and the process time is 10.\n",
      "Processed 163 PDFs.\n",
      "First filename: chen22v.pdf\n"
     ]
    }
   ],
   "source": [
    "num_consumers = mp.cpu_count()\n",
    "print(f'The total number of consumers is {num_consumers}.')\n",
    "data_text = main(directory, num_consumers)\n",
    "\n",
    "# Example: Print the first result\n",
    "print(f\"Processed {len(data_text)} PDFs.\")\n",
    "\n",
    "if data_text:\n",
    "    print(f\"First filename: {data_text[0]['filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0207c3b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:14.219221Z",
     "iopub.status.busy": "2025-03-04T18:22:14.218532Z",
     "iopub.status.idle": "2025-03-04T18:22:14.223523Z",
     "shell.execute_reply": "2025-03-04T18:22:14.222723Z"
    },
    "papermill": {
     "duration": 0.012097,
     "end_time": "2025-03-04T18:22:14.224782",
     "exception": false,
     "start_time": "2025-03-04T18:22:14.212685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 0.3566 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.process_time()\n",
    "print(f\"CPU Time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d26a9ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:14.234636Z",
     "iopub.status.busy": "2025-03-04T18:22:14.234260Z",
     "iopub.status.idle": "2025-03-04T18:22:14.238198Z",
     "shell.execute_reply": "2025-03-04T18:22:14.237274Z"
    },
    "papermill": {
     "duration": 0.010611,
     "end_time": "2025-03-04T18:22:14.239555",
     "exception": false,
     "start_time": "2025-03-04T18:22:14.228944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = data_text[0]\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3df6ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:14.251443Z",
     "iopub.status.busy": "2025-03-04T18:22:14.251024Z",
     "iopub.status.idle": "2025-03-04T18:22:14.259249Z",
     "shell.execute_reply": "2025-03-04T18:22:14.258380Z"
    },
    "papermill": {
     "duration": 0.015652,
     "end_time": "2025-03-04T18:22:14.260554",
     "exception": false,
     "start_time": "2025-03-04T18:22:14.244902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87709d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:14.271833Z",
     "iopub.status.busy": "2025-03-04T18:22:14.271509Z",
     "iopub.status.idle": "2025-03-04T18:22:14.306063Z",
     "shell.execute_reply": "2025-03-04T18:22:14.305171Z"
    },
    "papermill": {
     "duration": 0.041415,
     "end_time": "2025-03-04T18:22:14.307281",
     "exception": false,
     "start_time": "2025-03-04T18:22:14.265866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chen22v.pdf</td>\n",
       "      <td>Faster Fundamental Graph Algorithms via Learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05311655a15b75fab86956663e1819cd-Paper.pdf</td>\n",
       "      <td>Practical Bayesian Optimization of Machine\\nL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chap2.pdf</td>\n",
       "      <td>i\\ni\\ni\\ni\\ni\\ni\\ni\\ni\\nMathematical Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>planlearn2012_submission_1.pdf</td>\n",
       "      <td>Combining Meta-Learning and Optimization Algo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26262688.pdf</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\nMachine Learning in \\nBusin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filename  \\\n",
       "0                                 chen22v.pdf   \n",
       "1  05311655a15b75fab86956663e1819cd-Paper.pdf   \n",
       "2                                   chap2.pdf   \n",
       "3              planlearn2012_submission_1.pdf   \n",
       "4                                26262688.pdf   \n",
       "\n",
       "                                                text  \n",
       "0   Faster Fundamental Graph Algorithms via Learn...  \n",
       "1   Practical Bayesian Optimization of Machine\\nL...  \n",
       "2   i\\ni\\ni\\ni\\ni\\ni\\ni\\ni\\nMathematical Engineer...  \n",
       "3   Combining Meta-Learning and Optimization Algo...  \n",
       "4    \\n \\n \\n \\n \\n \\nMachine Learning in \\nBusin...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3648d2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:14.315670Z",
     "iopub.status.busy": "2025-03-04T18:22:14.315305Z",
     "iopub.status.idle": "2025-03-04T18:22:14.318795Z",
     "shell.execute_reply": "2025-03-04T18:22:14.317825Z"
    },
    "papermill": {
     "duration": 0.009445,
     "end_time": "2025-03-04T18:22:14.320507",
     "exception": false,
     "start_time": "2025-03-04T18:22:14.311062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91485472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:14.329129Z",
     "iopub.status.busy": "2025-03-04T18:22:14.328863Z",
     "iopub.status.idle": "2025-03-04T18:22:15.094126Z",
     "shell.execute_reply": "2025-03-04T18:22:15.093346Z"
    },
    "papermill": {
     "duration": 0.771104,
     "end_time": "2025-03-04T18:22:15.095771",
     "exception": false,
     "start_time": "2025-03-04T18:22:14.324667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', 'ANDY LANDU NGOMA', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'www.\\S+|http\\S+', 'ANDY LANDU NGOMA', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4beb19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:15.104122Z",
     "iopub.status.busy": "2025-03-04T18:22:15.103823Z",
     "iopub.status.idle": "2025-03-04T18:22:15.108920Z",
     "shell.execute_reply": "2025-03-04T18:22:15.108167Z"
    },
    "papermill": {
     "duration": 0.01039,
     "end_time": "2025-03-04T18:22:15.109981",
     "exception": false,
     "start_time": "2025-03-04T18:22:15.099591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was born on November 15, 2021. URL \\nabs/2110.14094.\\nEsfandiari, H., Korula Smirth'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'He was born on November 15, 2021. URL https://arxiv.org/\\nabs/2110.14094.\\nEsfandiari, H., Korula Smirth'\n",
    "\n",
    "y = re.sub(r'https?://\\S+|www\\.\\S+', '', x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a4c098d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T18:22:15.118626Z",
     "iopub.status.busy": "2025-03-04T18:22:15.118347Z",
     "iopub.status.idle": "2025-03-04T18:22:15.139031Z",
     "shell.execute_reply": "2025-03-04T18:22:15.137912Z"
    },
    "papermill": {
     "duration": 0.048604,
     "end_time": "2025-03-04T18:22:15.162664",
     "exception": false,
     "start_time": "2025-03-04T18:22:15.114060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\n \\n \\n \\n \\n \\nMachine Learning in \\nBusiness: \\nAn Introduction to the World of Data \\nScience \\n \\n \\n \\n \\n \\n \\n \\n     \\n \\n \\n     \\n \\nMachine Learning in \\nBusiness: \\nAn Introduction to the World of Data \\nScience \\n \\nSecond Edition \\n \\n \\n \\nJohn C. Hull \\n \\nUniversity Professor \\nJoseph L. Rotman School of Management \\nUniversity of Toronto \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n     \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSecond Printing \\nCopyright © 2019, 2020 by John C. Hull \\nAll Rights Reserved \\nISBN: 9798644074372 \\n \\n     \\n \\n \\nTo my students \\n \\n \\n \\n \\n \\n \\n     \\n \\n \\nvii \\n \\nContents \\n \\n \\n \\nPreface \\n \\nxi \\n \\n \\n \\nChapter 1 \\nIntroduction \\n1 \\n \\n1.1   This book and the ancillary material \\n3 \\n \\n1.2   Types of machine learning models \\n4 \\n \\n1.3   Validation and testing \\n6 \\n \\n1.4   Data cleaning \\n14 \\n \\n1.5   Bayes’ theorem \\n16 \\n \\nSummary \\n19 \\n \\nShort concept questions \\n20 \\n \\nExercises \\n21 \\n \\n \\n \\nChapter 2 \\nUnsupervised Learning \\n23   \\n \\n2.1   Feature scaling \\n24 \\n \\n2.2   The k-means algorithm \\n25 \\n \\n2.3   Choosing k \\n28 \\n \\n2.4   The curse of dimensionality \\n31 \\n \\n2.5   Country risk \\n31 \\n \\n2.6   Alternative clustering algorithms \\n35 \\n \\n2.7   Principal components analysis \\n39 \\n \\nSummary \\n43 \\n \\nShort concept questions \\n44 \\n \\nExercises \\n45 \\n \\n \\n \\nChapter 3 \\nSupervised Learning: Linear and Logistic \\nRegression                                                                            \\n \\n47 \\n \\n3.1   Linear regression: one feature \\n48 \\n \\n3.2   Linear regression: multiple features \\n49 \\n \\n3.3   Categorical features \\n52 \\n \\n3.4   Regularization \\n53 \\n \\n3.5   Ridge regression \\n54 \\n \\n3.6   Lasso regression \\n58 \\n \\n3.7   Elastic Net regression \\n60 \\n \\n3.8   Results for house price data \\n62 \\n \\n3.9   Logistic regression \\n66 \\nviii                                                                                                                                         Contents \\n \\n \\n3.10 Decision criteria \\n69 \\n \\n3.11 Application to credit decisions \\n70 \\n \\n3.12 The k-nearest neighbor algorithm \\n76 \\n \\nSummary \\n76 \\n \\nShort concept questions \\n77 \\n \\nExercises \\n78 \\n \\n \\n \\nChapter 4 \\n Supervised Learning: Decision Trees  \\n81 \\n \\n4.1   Nature of decision trees \\n82 \\n \\n4.2   Information gain measures \\n83 \\n \\n4.3   Application to credit decisions \\n85 \\n \\n4.4   The naïve Bayes classifier \\n91 \\n \\n4.5   Continuous target variables \\n95 \\n \\n4.6   Ensemble learning \\n98 \\n \\nSummary \\n100 \\n \\nShort concept questions \\n101 \\n \\nExercises \\n101 \\n \\n \\n \\nChapter 5 \\nSupervised Learning: SVMs \\n103 \\n \\n5.1   Linear SVM classification \\n103 \\n \\n5.2   Modification for soft margin \\n109 \\n \\n5.3   Non-linear separation \\n112 \\n \\n5.4   Predicting a continuous variable \\n114 \\n \\nSummary \\n118 \\n \\nShort concept questions \\n118 \\n \\nExercises \\n119 \\n \\n \\n \\nChapter 6 \\nSupervised Learning: Neural Networks \\n121 \\n \\n6.1   Single layer ANNs \\n121 \\n \\n6.2   Multi-layer ANNs \\n125 \\n \\n6.3   Gradient descent algorithm \\n126 \\n \\n6.4   Variations on the basic method \\n131 \\n \\n6.5   The stopping rule \\n133 \\n \\n6.6   The Black−Scholes−Merton formula \\n133 \\n \\n6.7   Extensions \\n137 \\n \\n6.8   Autoencoders \\n138 \\n \\n6.9   Convolutional neural networks \\n140 \\n \\n6.10 Recurrent neural networks \\n142 \\n \\nSummary \\n143 \\n \\nShort concept questions \\n144 \\n \\nExercises \\n \\n \\n144 \\n \\nContents                                                                                                                                            ix \\n \\n \\n \\nChapter 7 \\nReinforcement Learning \\n147 \\n \\n7.1   The multi-armed bandit problem \\n148 \\n \\n7.2   Changing environment    \\n152 \\n \\n7.3   The game of Nim \\n154 \\n \\n7.4   Temporal difference learning \\n157 \\n \\n7.5   Deep Q-learning \\n159 \\n \\n7.6   Applications \\n159 \\n \\nSummary \\n161 \\n \\nShort concept questions \\n162 \\n \\nExercises \\n163 \\n \\n \\n \\nChapter 8 \\nNatural Language Processing \\n165 \\n \\n8.1   Sources of data \\n168 \\n \\n8.2   Pre-processing \\n169 \\n \\n8.3   Bag of words model \\n170 \\n \\n8.4   Application of naïve Bayes classifier \\n172 \\n \\n8.5   Application of other algorithms \\n176 \\n \\n8.6   Information retrieval \\n177 \\n \\n8.7   Other NLP applications \\n178 \\n \\nSummary \\n180 \\n \\nShort concept questions \\n181 \\n \\nExercises \\n181 \\n \\nChapter 9      \\n \\nModel Interpretability \\n \\n183 \\n \\n9.1   Linear regression \\n185 \\n \\n9.2   Logistic regression \\n189 \\n \\n9.3   Black-box models \\n192 \\n \\n9.4   Shapley values \\n193 \\n \\n9.5   LIME \\n196 \\n \\nSummary \\n196 \\n \\nShort concept questions \\n197 \\n \\nExercises \\n198 \\n \\n \\n \\nChapter 10 \\nApplications in Finance \\n199 \\n \\n10.1  Derivatives \\n199 \\n \\n10.2  Delta \\n202 \\n \\n10.3  Volatility surfaces \\n203 \\n \\n10.4  Understanding volatility surface movements \\n204 \\n \\n10.5  Using reinforcement learning for hedging \\n208 \\n \\n10.6  Extensions \\n210 \\n \\n10.7  Other finance applications \\n212 \\n \\nSummary \\n213 \\nx                                                                                                                                         Contents \\n \\n \\nShort concept questions \\n214 \\n \\nExercises \\n214 \\n \\n \\n \\nChapter 11 \\nIssues for Society \\n217 \\n \\n11.1  Data privacy \\n218 \\n \\n11.2  Biases \\n209 \\n \\n11.3  Ethics \\n220 \\n \\n11.4  Transparency \\n221 \\n \\n11.5  Adversarial machine learning \\n221 \\n \\n11.6  Legal issues \\n222 \\n \\n11.7  Man vs. machine \\n223 \\n \\n \\n \\nAnswers to End of Chapter Questions \\n \\n225 \\nGlossary of Terms \\n \\n243 \\nIndex \\n \\n253 \\n \\n \\n \\nxi \\n \\n \\n \\n \\nPreface \\n \\n \\n \\nThis book is based on my experience teaching introductory courses \\non machine learning to business school students and executive groups. \\nThe purpose of the material is not to convert the reader into a data sci-\\nentist. Instead, it is to give the reader an understanding of the tools used \\nby data scientists and how they can further the objectives of an organi-\\nzation. The second edition improves the presentation of material and \\ncontains three new chapters. \\nMost students recognize that they need some knowledge of machine \\nlearning to survive in a world where jobs will be increasingly impacted \\nby it. Today, all executives need to know how to use computers. Tomor-\\nrow, all executives will need to be comfortable managing large data sets \\nand working with data science professionals to improve their produc-\\ntivity. \\nI have used no matrix or vector algebra and no calculus in this book. \\nAlthough these areas of study can help specialists, it has been my expe-\\nrience that most business school students and most executives are not \\ncomfortable with them.  \\nThe book explains the most popular algorithms used by data scien-\\ntists. This will enable the reader to assess their strengths and weak-\\nnesses for a particular situation and work productively with data sci-\\nence professionals. The algorithms are illustrated with a number of dif-\\nferent data sets, which can be downloaded from my website:  \\nANDY LANDU NGOMA \\nBoth Excel worksheets and Python code accompany the data sets. Vir-\\ntually all my students are comfortable with Excel before taking my \\ncourses. I insist that all become comfortable with Python as well.  This is \\nnot a hard sell.  Students recognize that coding skills have become a \\nnecessary prerequisite for many jobs in business. \\nxii                                                                                                                                             Preface \\n \\n \\n \\nSeveral hundred PowerPoint slides can be downloaded from my \\nwebsite. Instructors who choose to adopt the book are welcome to \\nadapt the slides to meet their own needs.  \\nA number of people have helped me move this book to a second edi-\\ntion. I would particularly like to thank Emilio Barone, Jacky Chen, Peter \\nHull, Raymond Kan, Eddie Mizzi, and Jun Yuan, who made many sugges-\\ntions for improving the material. I am grateful to Jay Cao, Jeff Li, and Niti \\nMishra who worked on some of the Python code that accompanies the \\nbook. I would also like to thank Rotman’s FinHub center, the TD bank, \\nand the Global Risk Institute in Financial Services for providing funding \\nfor the development of research and teaching materials in machine \\nlearning and financial innovation. Peter Christoffersen (prior to his un-\\ntimely death in 2018) and Andreas Park have been great colleagues at \\nFinHub and provided much of the inspiration for the book.   \\nI welcome comments on the book from readers.  My email address is \\nhull@rotman.utoronto.ca. \\n \\nJohn Hull \\n \\n \\n \\nAbout the Author \\n \\nJohn Hull is a University Professor at the Joseph L. Rotman School of \\nManagement, University of Toronto. Prior to writing this book, he wrote \\nthree best-selling books in the derivatives and risk management area. \\nHis books have an applied focus and he is proud that they sell equally \\nwell in the practitioner and college markets. He is academic director of \\nFinHub, Rotman’s Financial Innovation Lab, which carries out research \\nand develops educational material in all aspects of financial innova-\\ntion.  He has consulted for many companies throughout the world and \\nhas won many teaching awards, including University of Toronto’s pres-\\ntigious Northrop Frye award.  \\n \\n \\n1 \\n \\n \\n \\n \\n \\nChapter 1 \\n \\nIntroduction \\n \\n \\n \\n \\n \\nMachine learning is becoming an increasingly important tool in \\nbusiness—so much so that almost all employees are likely to be impact-\\ned by it in one way or another over the next few years. Machine learning \\nis concerned with using large data sets to learn the relationships be-\\ntween variables, make predictions, and take decisions in a changing en-\\nvironment.  \\nThe data available for machine learning is growing exponentially. It \\nis estimated that in any two-year period we generate nine times as \\nmuch data as existed at the beginning of the two years.1 Companies now \\nhave more information than ever before about their customers and \\ntheir purchasing habits. Hedge funds and pension plans can collect large \\namounts of data and opinions about companies they invest in. Advances \\nin computer processing speeds and reductions in data storage costs al-\\nlow us process this data and reach conclusions in ways that were simply \\nnot possible in the past. \\nMachine learning is a branch of artificial intelligence (AI). AI is con-\\ncerned with developing ways in which machines can imitate human in-\\ntelligence, possibly improving on it. Machine learning involves the crea-\\ntion of intelligence by learning from large volumes of data. It is arguably \\n                                                 \\n1 For discussion of this see: ANDY LANDU NGOMA\\nof-todays-data-created-in-two-years.html   \\n2 \\n                                                                      Chapter 1 \\n \\n \\nthe most exciting development within AI and one that has the potential \\nto transform virtually all aspects of a business.2 \\nWhat are the advantages for society of replacing human decision \\nmaking by machines? One advantage is speed. Machines can process \\ndata and come to a conclusion much faster than humans. The results \\nproduced by a machine are consistent and easily replicated on other \\nmachines. By contrast, humans occasionally behave erratically and \\ntraining a human for a task can be quite time consuming and expensive.  \\nTo explain how machine learning differs from other AI approaches \\nconsider the simple task of programming a computer to play tic tac toe \\n(also known as noughts and crosses). One approach would be to pro-\\nvide the computer with a look-up table listing the positions that can \\narise and the move that would be made by an expert human player in \\neach of those positions.  Another would be to create for the computer a \\nlarge number of games (e.g., by arranging for the computer to play \\nagainst itself thousands of times) and let it learn the best move. The \\nsecond approach is an application of machine learning.  Either approach \\ncan be successfully used for a simple game such as tic tac toe. Machine \\nlearning approaches have been shown to work well for more complicat-\\ned games such as chess and Go where the first approach is clearly not \\npossible.  \\nA good illustration of the power of machine learning is provided by \\nlanguage translation. How can a computer be programmed to translate \\nbetween two languages, say from English to French? One idea is to give \\nthe computer an English to French dictionary and program it to trans-\\nlate word-by-word. Unfortunately, this produces very poor results. A \\nnatural extension of this idea is to develop a look up table for translat-\\ning phrases rather than individual words. The results from this are an \\nimprovement, but still far from perfect. Google has pioneered a better \\napproach using machine learning. This was announced in November \\n2016 and is known as “Google Neural Machine Translation” (GNMT).3 A \\ncomputer is given a large volume of material in English together with \\nthe French translation. It learns from that material and develops its own \\n(quite complex) translation rules. The results from this have been a big \\nimprovement over previous approaches. \\nData science is the field that includes machine learning but is some-\\ntimes considered to be somewhat broader including such tasks as the \\nsetting of objectives, implementing systems, and communicating with \\n                                                 \\n2 Some organizations now use the terms “machine learning” and “artificial intelli-\\ngence” interchangeably.  \\n3 See ANDY LANDU NGOMA for an explanation of GNMT by the \\nGoogle research team. \\nIntroduction                                                                                                                                  3   \\n \\n \\n \\n stakeholders.4  We will consider the terms “machine learning” and “da-\\nta science” to be interchangeable in this book.  This is because it is diffi-\\ncult to see how machine learning specialists can be effective in business \\nif they do not get involved in working toward the objectives of their \\nemployers. \\nMachine learning or data science can be portrayed as the new world \\nof statistics. Traditionally, statistics has been concerned with such top-\\nics as probability distributions, confidence intervals, significance tests, \\nand linear regression. A knowledge of these topics is important, but we \\nare now able to learn from large data sets in ways that were not possi-\\nble before.  For example: \\n \\n\\uf0b7 We can develop non-linear models for forecasting and improved \\ndecision making. \\n\\uf0b7 We can search for patterns in data to improve a company’s un-\\nderstanding of its customers and the environment in which it op-\\nerates. \\n\\uf0b7 We can develop decision rules where we are interacting with a \\nchanging environment. \\nAs mentioned earlier, these applications of machine learning are now \\npossible because of increases in computer processing speeds, reduc-\\ntions in data storage costs, and the increasing amounts of data that are \\nbecoming available.    \\nWhen a statistician or econometrician dabbles in machine learning \\nthe terminology is liable to seem strange at first. For example, statisti-\\ncians and econometricians talk about independent variables and de-\\npendent variables while decision scientists talk about features and tar-\\ngets. The terminology of data science will be explained as the book pro-\\ngresses and a glossary of terms is provided at the end. \\n \\n     1.1     This Book and the Ancillary Material \\n \\nThis book is designed to provide readers with the knowledge to ena-\\nble them to work effectively with data science professionals. It will not \\nconvert the reader into a data scientist, but it is hoped that the book will \\n                                                 \\n4 See, for example, H. Bowne-Anderson, “What data scientists really do, according to \\n35 data scientists,” Harvard Business Review, August 2018:  \\nANDY LANDU NGOMA\\nscientists \\n4 \\n                                                                      Chapter 1 \\n \\n \\ninspire some readers to learn more and develop their abilities in this \\narea. Data science may well prove to be the most rewarding and excit-\\ning profession in the 21st century.  \\nTo use machine learning effectively you have to understand how the \\nunderlying algorithms work.  It is tempting to learn a programming lan-\\nguage such as Python and apply various packages to your data without \\nreally understanding what the packages are doing or even how the re-\\nsults should be interpreted. This would be a bit like a finance specialist \\nusing the Black−Scholes−Merton model to value options without under-\\nstanding where it comes from or its limitations. \\nThe objective of this book is to explain the algorithms underlying \\nmachine learning so that the results from using the algorithms can be \\nassessed knowledgeably. Anyone who is serious about using machine \\nlearning will want to learn a language such as Python for which many \\npackages have been developed. This book takes the unusual approach \\nof using both Excel and Python to provide backup material. This is be-\\ncause it is anticipated that some readers will, at least initially, be much \\nmore comfortable with Excel than with Python. \\nThe backup material can be found on the author’s website: \\nANDY LANDU NGOMA \\nReaders can start by focusing on the Excel worksheets and then move to \\nPython as they become more comfortable with it. Python will enable \\nthem use machine learning packages, handle data sets that are too large \\nfor Excel, and benefit from Python’s faster processing speeds.  \\n \\n \\n    1.2     Types of Machine Learning Models \\n            \\nThere are four main categories of machine learning models \\n \\n\\uf0b7 \\nSupervised learning \\n\\uf0b7 \\nUnsupervised learning \\n\\uf0b7 \\nSemi-supervised learning \\n\\uf0b7 \\nReinforcement learning \\n \\nSupervised learning is concerned with using data to make predictions. \\nIn the next section, we will show how a simple regression model can be \\nused to predict salaries. This is an example of supervised learning. In \\nChapter 3, we will consider how a similar model can be used to predict \\nhouse prices. We can distinguish between supervised learning models \\nthat are used to predict a variable that can take a continuum of values  \\nIntroduction                                                                                                                                  5   \\n \\n \\n \\n(such as an individual’s salary or the price of a house) and models that \\nare used for classification. Classification models are very common in \\nmachine learning.  As an example, we will later look at an application of \\nmachine learning where potential borrowers are classified as accepta-\\nble or unacceptable credit risks.  \\nUnsupervised learning is concerned with recognizing patterns in da-\\nta. The main objective is not to forecast a particular variable. Rather it is \\nto understand the environment represented by the data better. Consid-\\ner a company that markets a range of products to consumers.  Data on \\nconsumer purchases could be used to determine the characteristics of \\nthe customers who buy different products. This in turn could influence \\nthe way the products are advertised. As we will see in Chapter 2, clus-\\ntering is the main tool used in unsupervised learning. \\nThe data for supervised learning contains what are referred to as \\nfeatures and labels. The labels are the values of the target that is to be \\npredicted. The features are the variables from which the predictions are \\nto be made. For example, when predicting the price of a house the fea-\\ntures could be the square feet of living space, the number of bedrooms, \\nthe number of bathrooms, the size of the garage, whether the basement \\nis finished, and so on. The label would be the house price. The data for \\nunsupervised learning consists of features but no labels because the \\nmodel is being used to identify patterns, not to forecast something. We \\ncould use an unsupervised learning model to understand the houses \\nthat exist in a certain neighborhood without trying to predict prices. We \\nmight find that there is a cluster of houses with 1,500 to 2,000 square \\nfeet of living space, three bedrooms, and a one-car garage and another \\ncluster of houses with 5,000 to 6,000 square feet of living area, six bed-\\nrooms, and a two-car garage. \\nSemi-supervised learning is a cross between supervised and un-\\nsupervised learning. It arises when we are trying to predict something \\nand we have some data with labels (i.e., values for the target) and some \\n(usually much more) unlabeled data. It might be thought that the unla-\\nbeled data is useless, but this is not necessarily the case.  The unlabeled \\ndata can be used in conjunction with the labeled data to produce clus-\\nters which help prediction. For example, suppose we are interested in \\npredicting whether a customer will purchase a particular product from \\nfeatures such as age, income level, and so on. Suppose further that we \\nhave a small amount of labeled data (i.e., data which indicates the fea-\\ntures of customers as well as whether they bought or did not buy the \\nproduct) and a much larger amount of unlabeled data (i.e., data which \\nindicates the features of potential customers, but does not indicate \\nwhether they bought the product).  We can apply unsupervised learning  \\n6 \\n                                                                      Chapter 1 \\n \\n \\n to use the features to cluster potential customers.  Imagine a simple \\nsituation where: \\n \\n\\uf0b7 There are two clusters, A and B, in the full data set. \\n\\uf0b7 The purchasers from the labeled data all correspond to points \\nin Cluster A while the non-purchasers from the labeled data all \\ncorrespond to points in the other Cluster B. \\n \\nWe might reasonably classify all individuals in Cluster A as buyers and \\nall individuals in Cluster B as non-buyers. \\nHuman beings use semi-supervised learning.  Imagine that you do \\nnot know the names “cat” and “dog,” but are observant. You notice two \\ndistinct clusters of domestic pets in your neighborhood. Finally some-\\none points at two particular animals and tells you one is a cat while the \\nother is a dog.  You will then have no difficulty in using semi-supervised \\nlearning to apply the labels to all the other animals you have seen. If \\nhumans use semi-supervised learning in this way, it should come as no \\nsurprise that machines can do so as well. Many machine learning algo-\\nrithms are based on studying the ways our brains process data. \\nThe final type of machine learning, reinforcement learning, is con-\\ncerned with situations where a series of decisions is to be taken. The \\nenvironment is typically changing in an uncertain way as the decisions \\nare being taken. Driverless cars use reinforcement learning algorithms. \\nThe algorithms underlie the programs mentioned earlier for playing \\ngames such as Go and chess. They are also used for some trading and \\nhedging decisions. We will discuss reinforcement learning in Chapter 7.   \\n1.3     Validation and Testing \\n \\nWhen a data set is used for forecasting or determining a decision \\nstrategy, there is a danger that the machine learning model will work \\nwell for the data set, but will not generalize well to other data. An obvi-\\nous point is that it is important that the data used in a machine learning \\nmodel be representative of the situations to which the model is to be \\napplied.  For example, using data for a region where customers have a \\nhigh income to predict the national sales for a product is likely to give \\nbiased results. \\nAs statisticians have realized for a long time, it is also important to \\ntest a model out-of-sample. By this we mean that the model should be \\ntested on data that is different from the sample data used to determine \\nthe parameters of the model.  \\nIntroduction                                                                                                                                  7   \\n \\n \\n \\nData scientists are typically not just interested in testing one model. \\nThey typically try several different models, choose between them, and \\nthen test the accuracy of the chosen model. For this, they need three \\ndata sets:  \\n \\n\\uf0b7 \\na training set \\n\\uf0b7 \\na validation set \\n\\uf0b7 \\na test set \\nThe training set is used to determine the parameters of the models \\nthat are under consideration. The validation set is used to determine \\nhow well each of the models generalizes to a different data set. The test \\nset is held back to provide a measure of the accuracy of the chosen \\nmodel. \\nWe will illustrate this with a simple example. Suppose that we are in-\\nterested in predicting the salaries of people working in a particular pro-\\nfession in a certain part of the United States from their age. We collect \\ndata on a random sample of 30 individuals. (This is a very small data set \\ncreated to provide a simple example. The data sets used in machine \\nlearning are many times larger than this.)  The first ten observations \\n(referred to in machine learning as instances) will be used to form the \\ntraining set. The next ten observations will be used for form the valida-\\ntion set and the final ten observations will be used to form the test set.   \\nThe training set is shown in Table 1.1 and plotted in Figure 1.1. It is \\ntempting to choose a model that fits the training set really well. Some \\nexperimentation shows that a polynomial of degree five does this. This \\nis the model:  \\n \\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\nY\\na b X\\nb X\\nb X\\nb X\\nb X\\n\\uf03d\\uf02b\\n\\uf02b\\n\\uf02b\\n\\uf02b\\n\\uf02b\\n \\n \\nwhere Y is salary and X is age. The result of fitting the polynomial to the \\ndata is shown in Figure 1.2. Details of all analyses carried out, are in \\nANDY LANDU NGOMA \\nThe model provides a good fit to the data. The standard deviation of \\nthe difference between the salary given by the model and the actual sal-\\nary for the ten individuals in the training data set, which is referred to \\nas the root-mean-squared error (rmse), is $12,902.  However, common \\nsense would suggest that we may have over-fitted the data. (This is be-\\ncause the curve in Figure 1.2 seems unrealistic. It declines, increases, \\ndeclines, and then increases again as age increases.) We need to check \\nthe model out-of-sample. To use the language of data science, we need  \\n8 \\n                                                                      Chapter 1 \\n \\n \\nto determine whether the model generalizes well to a validation data \\nset that is different from the training set in Table 1.1.  \\n \\nTable 1.1   The training data set: salaries for a random sample of ten \\npeople working in a particular profession in a certain area. \\n \\nAge (years) \\nSalary ($) \\n25 \\n135,000 \\n55 \\n260,000 \\n27 \\n105,000 \\n35 \\n220,000 \\n60 \\n240,000 \\n65 \\n265,000 \\n45 \\n270,000 \\n40 \\n300,000 \\n50 \\n265,000 \\n30 \\n105,000 \\n \\n \\n \\nFigure 1.1   Scatter plot of the training data set in Table 1.1 \\n \\n \\n \\n \\n0\\n50,000\\n100,000\\n150,000\\n200,000\\n250,000\\n300,000\\n350,000\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary ($)\\nAge (years)\\nIntroduction                                                                                                                                  9   \\n \\n \\n \\nFigure 1.2   Result of fitting a polynomial of degree 5 to the data in Ta-\\nble 1.1 and Figure 1.1 (see Salary vs. Age Excel file) \\n \\n \\nThe validation set is shown in Table 1.2.  The scatter plot for this da-\\nta is in Figure 1.3. When we use the model in Figure 1.2 for this data, we \\nfind that the root mean square error (rmse) is about $38,794, much \\nhigher than the $12,902 we obtained using the training data set in Table \\n1.1. This is a clear indication that the model in Figure 1.2 is over-fitting: \\nit does not generalize well to new data.   \\n \\n \\nTable 1.2   The validation data set  \\n  \\nAge (years) \\nSalary ($) \\n30 \\n166,000 \\n26 \\n78,000 \\n58 \\n310,000 \\n29 \\n100,000 \\n40 \\n260,000 \\n27 \\n150,000 \\n33 \\n140,000 \\n61 \\n220,000 \\n27 \\n86,000 \\n48 \\n276,000 \\n \\n \\n0\\n50,000\\n100,000\\n150,000\\n200,000\\n250,000\\n300,000\\n350,000\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary ($)\\nAge (years)\\n10 \\n                                                                      Chapter 1 \\n \\n \\nFigure 1.3   Scatter plot for data in Table 1.2 \\n \\n \\n \\n \\nThe natural next step is to look for a simpler model.  The scatter plot \\nin Figure 1.1 suggests that a quadratic model might be appropriate. This \\nmodel is: \\n \\n2\\n1\\n2\\nY\\na b X\\nb X\\n\\uf03d\\uf02b\\n\\uf02b\\n \\n \\ni.e., a polynomial of degree two. \\nThe best-fit quadratic model together with the training data set from \\nFigure 1.1 is shown in Figure 1.4. The fit to the training set is of course \\nnot as good as the model in Figure 1.2. The standard deviation of the \\nerror is $32,932. However, the model generalizes to new data reasona-\\nbly well. The standard deviation of the errors given by the quadratic \\nmodel for the validation data set in Table 1.2 and Figure 1.3 is $33,554, \\nonly a little worse than the $32,932 for the training data. The quadratic \\nmodel therefore generalizes better than the more elaborate model in \\nFigure 1.2. \\nThe model in Figure 1.4 is simpler than the model in Figure 1.2 and \\ngeneralizes well to the validation set.  However, this does not mean that \\nsimpler models are always better than more complex models. In the \\ncase of the data we are considering, we could use a linear model. This \\nwould lead to the predictions in Figure 1.5. \\n0\\n50,000\\n100,000\\n150,000\\n200,000\\n250,000\\n300,000\\n350,000\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary ($)\\nAge (years)\\nIntroduction                                                                                                                                  11   \\n \\n \\n \\nFigure 1.4   Result of fitting a quadratic model to the data in Table 1.1 \\nand Figure 1.1 (see Salary vs. Age Excel file) \\n \\n \\nVisually it can be seen that this model does not capture the decline in \\nsalaries as individuals age beyond 50. This observation is confirmed by \\nthe standard deviation of the error for the training data set, which is \\n$49,731, much worse than that for the quadratic model. \\n \\nFigure 1.5   Result of fitting a linear model to training data (see Sala-\\nry vs. Age Excel file) \\n \\n      \\n \\n0\\n50,000\\n100,000\\n150,000\\n200,000\\n250,000\\n300,000\\n350,000\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary($)\\nAge (years)\\n0\\n50,000\\n100,000\\n150,000\\n200,000\\n250,000\\n300,000\\n350,000\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary ($)\\nAge (years)\\n12 \\n                                                                      Chapter 1 \\n \\n \\nTable 1.3 summarizes the root mean square errors given by the \\nthree models we have considered.  Note that both the linear model and \\nthe quadratic model generalize well to the validation data set, but the \\nquadratic model is preferred because it is more accurate. By contrast, \\nthe five-degree polynomial model does not generalize well.   It over-fits \\nthe training set while the linear model under-fits the training set. \\n \\n \\nTable 1.3   Root mean square errors (see Excel file) \\n \\n \\nPolynomial \\nof degree 5 \\nQuadratic \\nmodel \\nLinear \\nmodel \\nTraining set  \\n(Table 1.1) \\n12, 902 \\n32,932 \\n49,731 \\nValidation set  \\n(Table 1.2) \\n38,794 \\n33,554 \\n49,990 \\n \\n \\nHow accurate is the quadratic model? We could rely on the results \\nfrom the validation set. But we used the validation set to help choose \\nthe best model and so it may overstate the accuracy of the model. We \\ntherefore use the test data set to produce an accuracy measure. This \\ndata set has played no role in analyses so far.  \\nSuppose the test data set results are as shown in Table 1.4.   The root \\nmean squared error for the test set is $34,273.  When information about \\nthe performance of the chosen model is presented, it should be based \\non results for the test data set, not on those for the validation set or the \\ntraining set. \\nHow should the balance between over-fitting and under-fitting be \\nachieved? This is an important issue in machine learning. Some machine \\nlearning algorithms, such as neural networks (see Chapter 6), can in-\\nvolve a very large number of parameters. It is then easy to over-fit, even \\nwhen the training data set is large.   \\nBased on the simple example we have looked at, a rule of thumb \\nwould seem to be as follows:  \\n \\nThe complexity of the model should be increased until out-of-\\nsample tests indicate that it does not generalize well.  \\n \\n \\n \\n \\nIntroduction                                                                                                                                  13   \\n \\n \\n \\nTable 1.4   Errors when quadratic model is applied to the test set \\n \\nAge (years) \\nSalary ($) \\nPredicted  \\nsalary ($) \\nError ($) \\n26 \\n110,000 \\n113,172 \\n−3,172 \\n52 \\n278,000 \\n279,589 \\n−1,589 \\n38 \\n314,000 \\n230,852 \\n       +83,148 \\n60 \\n302,000 \\n264,620 \\n       +37,380 \\n64 \\n261,000 \\n245,457 \\n       +15,543 \\n41 \\n227,000 \\n249325 \\n       −22,325 \\n34 \\n200,000 \\n199,411 \\n    +589 \\n46 \\n233,000 \\n270,380 \\n       −37,380 \\n57 \\n311,000 \\n273,883 \\n       −37,117 \\n55 \\n298,000 \\n277,625 \\n       +20,375 \\n \\n \\nThis rule is illustrated in Figure 1.6. The figure assumes that there is a \\ncontinuum of models that get progressively more complex. For each \\nmodel, we calculate a measure of the model’s error, such as root mean \\nsquare error, for both the training set and the validation set. When the \\ncomplexity of the model is less than X, the model generalizes well: the \\nerror of the model for the validation set is only a little more than that \\nfor the training set. As model complexity is increased beyond X, the er-\\nrors for the validation set start to increase.  \\n \\nFigure 1.6   Errors of a model for the training set and the vali-\\ndation set.  \\n \\n \\nModel Complexity\\nModel\\nError\\nTraining set\\nValidation set\\nX\\n14 \\n                                                                      Chapter 1 \\n \\n \\nThe best model is the one with model complexity X. This is because \\nthat model has the lowest error for the validation set. A further increase \\nin complexity lowers errors for the training set but increases them for \\nthe validation set, which is a clear indication of over-fitting.  \\n Finding the right balance between under-fitting and over-fitting is \\nreferred to as the bias-variance trade-off in machine learning. The bias is \\nthe error due the assumptions in the model that cause it to miss rele-\\nvant relations. The variance is the error due to the model over-fitting by \\nreflecting random noise in the training set.  \\nTo summarize the points we have made: \\n \\n\\uf0b7 \\nThe training set is used to develop alternative models.  \\n\\uf0b7 \\nThe validation set is used to investigate how well the \\nmodels generalize to new data and to choose between the \\nmodels.  \\n\\uf0b7 \\nThe test set is kept back and is used as an out-of-sample \\ntest of the accuracy of the chosen model at the end of the \\nanalysis. \\n \\nIn the simple example we have looked at, the training set, validation \\nset, and test set had equal numbers of observations.  In a typical ma-\\nchine learning application much more data is available and at least 60% \\nof it is allocated to the training set while 10% to 20% is allocated to \\neach of the validation set and the test set.   \\nIt is important to emphasize that the data sets in machine learning \\ninvolve many more observations that the baby data set we have used in \\nthis section. (Ten observations are obviously insufficient to reliably \\nlearn a relationship.) However, the baby data set does provide a simple \\nillustration of the bias-variance trade-off.  \\n \\n \\n1.4     Data Cleaning \\n \\nData cleaning is a very important, if not terribly exciting, aspect of \\nmachine learning. It has been estimated that data scientists spend 80% \\nof their time on collecting  and  cleaning  data.5   Large data sets typical-\\nly have issues that need to be fixed.  Good data cleaning can make all the \\ndifference between successful and unsuccessful machine learning. The\\n                                                 \\n5 See ANDY LANDU NGOMA\\ntime-consuming-least-enjoyable-data-science-task-survey-says/#2f8970aa6f63 for \\na discussion of this. \\nIntroduction                                                                                                                                  15   \\n \\n \\n \\nexpression “garbage-in, garbage-out” applies just as much to machine \\nlearning as to other analyses.  \\nAt this stage, it is appropriate to point out that there are two types of \\ndata:  numerical and categorical.  Numerical data consists of numbers. \\nCategorical data is data which can fall into a number of different catego-\\nries. For example, data to predict a house price might categorize drive-\\nways as asphalt, concrete, grass, etc. As we will see in Chapter 3, cate-\\ngorical data must be converted to numbers for the purposes of analysis.  \\nWe now list some data cleaning issues and how they can be handled. \\n \\nInconsistent Recording \\nEither numerical or categorical data can be subject to inconsistent \\nrecording. For example, numerical data for the square footage of a \\nhouse might be input manually as 3300, 3,300, 3,300 ft, or 3300+, and \\nso on. It is necessary to inspect the data to determine variations and \\ndecide the best approach to cleaning. Categorical data might list the \\ndriveway as “asphalt”, “Asphalt”, or even “aphalt.” The simplest ap-\\nproach here is to list the alternatives that have been input for a particu-\\nlar feature and merge them as appropriate. \\n \\nUnwanted Observations \\nIf you are developing a model to predict house prices in a certain ar-\\nea, some of your data might refer to the prices of apartments or to the \\nprices of houses that are not in the area of interest. It is important to \\nfind a way of identifying this data and removing it before any analysis is \\nattempted. \\n \\nDuplicate Observations \\nWhen data is merged from several different sources or several dif-\\nferent people have been involved in creating a data set there are liable \\nto be duplicate observations. These can bias results. It is therefore im-\\nportant to use a search algorithm to identify and remove duplicates as \\nfar as possible.  \\n \\nOutliers \\nIn the case of numerical data, outliers can be identified by either \\nplotting data or searching for data that is, say, six standard deviations \\naway from the mean. Sometimes it is clear that the outlier is a typo. For \\nexample, if the square footage of a house with three bedrooms is input\\n16 \\n                                                                      Chapter 1 \\n \\n \\nas 33,000, it is almost certainly a typo and should probably be 3,300. \\nHowever, outliers should be removed only if there is a good reason for \\ndoing so.  Unusually large or small values for features or targets, if cor-\\nrect, are likely to contain useful information. The impact of outliers on \\nthe results of machine learning depends on the model being used. Outli-\\ners tend to have a big effect on regression models such as those consid-\\nered in Chapter 3. Other models, such as those involving decision trees \\n(which will be explained in Chapter 4) are less influenced by outliers. \\n \\nMissing Data \\nIn any large data set there are likely to be missing data values. A \\nsimple approach is to remove data with missing values for one or more \\nfeatures. But this is probably undesirable because it reduces the sample \\nsize and may create biases. In the case of categorical data, a simple solu-\\ntion is to create a new category titled “Missing.” In the case of numerical \\ndata, one approach is to replace the missing data by the mean or median \\nof the non-missing data values. For example, if the square footage of a \\nhouse is missing and we calculate the median square footage for the \\nhouses for which this data is available to be 3,500, we could populate all \\nthe missing values with 3,500. More sophisticated approaches can in-\\nvolve regressing the target against non-missing values and then using \\nthe results to populate missing values. Sometimes it is reasonable to \\nassume that data is missing at random and sometimes the very fact that \\ndata is missing is itself informative. In the latter case it can be desirable \\nto create a new indicator variable which is zero if the data is present \\nand one if it is missing. \\n \\n \\n1.5     Bayes’ Theorem \\n \\nSometimes in machine learning we are interested in estimating the \\nprobability of an outcome from data. The outcome might be a customer \\ndefaulting on a loan or a transaction proving to be fraudulent.  Typically \\nthere is an initial probability of the outcome.  When data is received, the \\nprobability is updated to be a probability conditional on the data. A re-\\nsult known as Bayes’ theorem is sometimes useful for calculating condi-\\ntional probabilities. \\nThomas Bayes discovered Bayes’ theorem in about 1760. We will \\nwrite P(X) as the probability of event X happening and 𝑃(𝑌|𝑋) as the \\nprobability of event Y happening conditional that event X has happened. \\nBayes’ theorem states that \\n \\nIntroduction                                                                                                                                  17   \\n \\n \\n \\n𝑃(𝑌|𝑋) = 𝑃(𝑋|𝑌)𝑃(𝑌)\\n𝑃(𝑋)\\n                                  (1.1) \\n \\nThe proof of Bayes’ theorem is straightforward. From the meaning of \\nconditional probabilities: \\n \\n𝑃(𝑌|𝑋) = 𝑃(𝑋 and 𝑌)\\n𝑃(𝑋)\\n  \\nand \\n𝑃(𝑋|𝑌) = 𝑃(𝑋 and 𝑌)\\n𝑃(𝑌)\\n  \\n \\nSubstituting for 𝑃(𝑋 and 𝑌) from the second of these equations into the \\nfirst leads to the Bayes’ theorem result in equation (1.1). \\nFor an application of Bayes’ theorem, suppose that a bank is trying \\nto identify customers who are attempting to do fraudulent transactions \\nat branches. It observes that 90% of fraudulent transactions involve \\nover $100,000 and occur between 4pm and 5pm.  In total, only 1% of \\ntransactions are fraudulent and 3% of all transactions involve over \\n$100,000 and occur between 4pm and 5pm.  \\nIn this case we define: \\n \\nX:  transaction occurring between 4pm and 5pm involving over \\n$100,000 \\nY:  fraudulent transaction \\n \\nWe know that P(Y) = 0.01, 𝑃(𝑋|𝑌) = 0.9, and P(X) = 0.03. From Bayes’ \\ntheorem: \\n \\n𝑃(𝑌|𝑋) = 𝑃(𝑋|𝑌)𝑃(𝑌)\\n𝑃(𝑋)\\n = 0.9 × 0.01\\n0.03\\n= 0.3  \\n \\nThe probability of a random transaction being fraudulent transac-\\ntion is only 1%. But when it is known that the transaction is undertaken \\nbetween 4pm and 5pm and involves over $100,000, Bayes’ theorem \\nshows that this probability should be updated to 30%. The implications \\nof this are clear. If the bank has an on-line approval system for transac-\\ntions, it should not approve transactions between 4pm and 5pm where \\nover $100,000 is involved without further investigation.   \\nEffectively what Bayes’ theorem allows one to do is to invert the \\nconditionality when probabilities are measured. Sometimes this prod-  \\n18 \\n                                                                      Chapter 1 \\n \\n \\nuces counterintuitive results.  Suppose that a test for a certain disease is \\n“99% accurate.” By this it is usually meant that, when a person has the \\ndisease, it gives a positive result (i.e., it predicts that the person has the \\ndisease) 99% of the time. We also assume that, when a person does not \\nhave the disease, it gives a negative result (i.e., it predicts that the per-\\nson does not have the disease) 99% of the time.6 Suppose that the dis-\\nease is rare so that the (unconditional) probability of an individual hav-\\ning the disease is one in 10,000. If you are tested positive, what is the \\nprobability that you have the disease? \\nA natural response to this question is 99%. (After all, the test is 99% \\naccurate.) However, this is confusing the conditionality. Suppose that X \\nindicates that the test is positive and Y indicates that a person has the \\ndisease. We are interested in 𝑃(𝑌|𝑋).  We know that 𝑃(𝑋|𝑌) = 0.99. We \\nalso know that P(Y) = 0.0001. Let us extend our notation so that X  in-\\ndicates that the test result is negative and Y  indicates that the person \\ndoes not have the disease. We also know that \\n \\n𝑃(𝑌̅) = 0.9999 \\n \\nand \\n \\n𝑃(𝑋̅|𝑌̅) = 0.99 \\n \\nBecause either X or X  is true 𝑃(𝑋̅|𝑌̅) + 𝑃(𝑋|𝑌̅) = 1 so that \\n \\n𝑃(𝑋|𝑌̅) = 0.01 \\n \\nand we can calculate the probability of a positive test result as \\n \\n𝑃(𝑋) = 𝑃(𝑋|𝑌)𝑃(𝑌) + 𝑃(𝑋|𝑌̅)𝑃(𝑌̅) \\n \\n                                      = 0.99 × 0.0001 + 0.01 × 0.9999 = 0.0101 \\n                         \\nUsing the Bayes’ theorem result in equation (1.1), \\n \\n𝑃(𝑌|𝑋) = 𝑃(𝑋|𝑌)𝑃(𝑌)\\n𝑃(𝑋)\\n = 0.99 × 0.0001\\n0.0101\\n= 0.0098  \\n \\n                                                 \\n6 It does not have to be the case that that the accuracy measure is the same for posi-\\ntive and negative test results.  \\nIntroduction                                                                                                                                  19   \\n \\n \\n \\nThis shows that there is a less than 1% chance that you have the dis-\\nease if you get a positive test result. The test result increases the proba-\\nbility that you have the disease from the unconditional 0.0001 by a fac-\\ntor of about 98 but the probability is still low. The key point here is that \\n“accuracy\" is defined as the probability of getting the right result condi-\\ntional that a person has the disease, not the other way round.  \\nWe will use Bayes’ theorem to explain a popular tool known as the \\nnaïve Bayes classifier in Chapter 4 and use it in natural language pro-\\ncessing in Chapter 8. \\n \\n \\nSummary \\n \\nMachine learning is a branch of artificial intelligence concerned with \\nlearning from big data sets. It involves developing algorithms to make \\npredictions, cluster data, or develop a strategy for making a series of \\ndecisions.   \\nStatistical analysis has traditionally involved forming a hypothesis \\n(without looking at data) and then testing the hypothesis with data. Ma-\\nchine learning is different. There is no hypothesis. The model is derived \\nentirely from data.   \\nBefore using any machine learning algorithms, it is important to \\nclean the data. The features constituting the data are either numerical \\nor categorical.  In either case there may be inconsistencies in the way \\nthe data has been input. These inconsistencies need to be identified and \\ncorrected.  Some observations may be irrelevant to the task at hand and \\nshould be removed. The data should be checked for duplicate observa-\\ntions that can create biases. Outliers that are clearly a result of input \\nerrors should be removed. Finally, missing data must be dealt with in a \\nway that will not bias the results.  \\nBayes’ theorem is a result that is sometimes used when it is neces-\\nsary to quantify uncertainty. It is a way of inverting conditionality. Sup-\\npose we are interested in knowing the probability of an event Y occur-\\nring and can observe whether another related event X happens. Sup-\\npose also that from experience we know the probability of X happening \\nwhen Y happens. Bayes’ theorem allows us to calculate the probability \\nof Y conditional on X. \\n20 \\n                                                                      Chapter 1 \\n \\n \\nAs mentioned in this chapter, machine learning has its own termi-\\nnology which is different from that traditionally used in statistics.  We \\nclose this chapter by summarizing the new terminology that has been \\nintroduced so far. A feature is a variable on which we have observa-\\ntions.  Each observation is referred to as an instance. A target is a varia-\\nble about which we want to make predictions.  Labels are observations \\non the target. Supervised learning is a type of machine learning where \\nwe use data on the features and targets to predict the target for new \\ndata. Unsupervised learning is where we attempt to find patterns in data \\nto help us understand its structure. (There is no target and therefore \\nthere are no labels in unsupervised learning). Semi-supervised learning \\ninvolves making predictions about a target from data that is partly la-\\nbeled (so that values of the target are provided) and partly unlabeled \\n(so that values of the target are not provided). Finally, reinforcement \\nlearning is concerned with producing algorithms for sequential decision \\nmaking where the decision maker is interacting with a changing envi-\\nronment. Other terminology will be introduced as the book progresses. \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n1.1 \\nWhat is the difference between machine learning and artificial \\nintelligence? \\n1.2 \\nExplain two types of predictions that are made in supervised \\nlearning. \\n1.3 \\nWhen is unsupervised learning appropriate? \\n1.4 \\nWhen is reinforcement learning appropriate? \\n1.5 \\nWhen is semi-supervised learning appropriate? \\n1.6 \\nHow can you tell whether a machine learning model is over-\\nfitting data? \\n1.7 \\nExplain the role of the validation data set and the test data set. \\n1.8 \\nWhat is meant by a categorical feature? \\n1.9 \\nWhat is meant by the bias-variance trade-off? Does the linear \\nmodel in Figure 1.5 give a bias error or a variance error? Does the \\nfifth-order-polynomial model in Figure 1.2 give a bias error or a \\nvariance error? \\n1.10 List five different types of data cleaning. \\n1.11 “Bayes’ theorem allows one to invert the conditionality.” What is \\nmeant by this statement? \\n \\n \\n \\nIntroduction                                                                                                                                  21   \\n \\n \\n \\nEXERCISES \\n  \\n1.12 How well do polynomials of degree 3 and 4 work for the data on \\nsalary vs. age in Section 1.3.? Consider whether the best fit model \\ngeneralizes well from the training set to the validation set.  \\n1.13 Suppose that 25% of emails are spam and it is found that spam \\ncontains a particular word 40% of the time. Overall only 12.5% of \\nthe emails contain the word. What is the probability of an email \\nbeing spam when it contains the word? \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xa0\\n23 \\n \\n \\n    \\n \\nChapter 2 \\n \\nUnsupervised Learning \\n \\n \\n \\n \\n \\n \\n \\nAs explained in Chapter 1, unsupervised learning is concerned with \\nidentifying patterns in data. The immediate objective is not to predict \\nthe value of a target variable.  Rather it is to understand the structure of \\ndata and find clusters. This is a useful exercise for many businesses. \\nBanks, for example, often use unsupervised learning to cluster their \\ncustomers so that they can communicate with them better and provide \\nan improved level of service. One cluster might be young couples who \\nare likely to want a mortgage soon.   Another might be what are termed \\nHENRYs (High Earners, Not Rich Yet). These are families earning bet-\\nween $250,000 and $500,000 who may be in the market for wealth \\nmanagement services.  \\nThis chapter explains a popular clustering procedure known as the \\nk-means algorithm. It illustrates the algorithm by clustering countries \\naccording to their risk from the perspective of a foreign investor. Data \\non 122 countries and four features are used. The features are the real \\nGDP growth rate, a corruption index, a peace index, and a legal risk in-\\ndex. The chapter then mentions some alternative algorithms and ex-\\nplains principal components analysis, which is a useful tool for both su-\\nper-vised and unsupervised learning. \\n \\n \\n24 \\n                                                                  Chapter 2 \\n \\n \\n2.1     Feature Scaling \\n \\nBefore covering clustering algorithms, it is appropriate to discuss \\nwhat is known as feature scaling. This is also referred to as the normali-\\nzation or standardization of data. It is a necessary first step to for many \\nmachine learning algorithms, including the k-means algorithm. The \\npurpose of feature scaling is to ensure that the features are given equal \\nimportance in an algorithm. Suppose for example that we are clustering \\nmen according to two features: height in inches and weight in pounds. \\nHeights might range from 60 to 80 inches while weights range from 100 \\nto 350 pounds.  Without feature scaling, the two features will not be \\ntreated with equal importance because the range of heights is much less \\nthan the range of weights (20 inches vs 250 pounds).  \\nOne approach to feature scaling is to calculate the mean and stand-\\nard deviation of each feature and scale observations on the feature by \\nsubtracting the mean and dividing by the standard deviation. If V is a \\nfeature value for a particular observation, \\n \\n                  Scaled Feature Value =\\n\\uf02d\\uf06d\\n\\uf073\\nV\\n   \\n \\nwhere \\uf06d and \\uf073 are the mean and standard deviation calculated from \\nobservations on the feature. This method of feature scaling is some-\\ntimes referred to as Z-score scaling or Z-score normalization. The scaled \\nfeatures have means equal to zero and standard deviations equal to one. \\nIf we want a particular feature to have more effect than other features \\nin determining cluster separation, we could scale it so that its standard \\ndeviation is greater than one.   \\nAn alternative approach to feature scaling is to subtract the mini-\\nmum feature value and divide by the difference between the maximum \\nand minimum values so that: \\n \\n\\uf02d\\n\\uf02d\\nmin\\nScaled Feature Value =max\\nmin\\nV\\n \\n \\nwhere max and min denote the maximum and minimum feature values. \\nThis is referred to as min-max scaling. The scaled feature values lie be-\\ntween zero and one.  \\nUnsupervised Learning \\n                                                                                   25 \\n \\n \\n \\nScaling using the Z-score method is usually preferred because it is \\nless sensitive to extreme values, but it can make sense to use min-max \\nscaling when features have been measured on bounded scales. In our \\ndescription of the k-means algorithm in the rest of this chapter, we as-\\nsume that feature values have been scaled using one of the two methods \\nwe have described.  \\nThe usual approach is to use the training data set to define the scal-\\ning parameters (i.e., the means and standard deviations of features or \\ntheir minimums and maximums). The scaling defined by the training set \\nis then applied to the validation set and the test set as well to new data. \\n \\n \\n2.2     The k-Means Algorithm \\n \\nTo cluster observations we need a distance measure. Suppose first \\nthat there are only two features, x and y, so that we can plot the obser-\\nvations on a two-dimensional chart.  Consider the two observations, A \\nand B, in Figure 2.1.  A natural distance measure is the Euclidean dis-\\ntance. This is the length of the straight line AB. Suppose that for obser-\\nvation A, x = xA and y = yA, while for observation B, x = xB and y = yB. The \\nEuclidean distance between A and B (using Pythagoras’ theorem) is  \\n \\n√(𝑥A −𝑥B)2 + (𝑦A −𝑦B)2 \\n \\nThis distance measure can be extended to many dimensions. Sup-\\npose we have observations on m features and that the value of the jth \\nfeature for the ith observation is \\n.\\ni j\\nv\\n The distance between the pth ob-\\nservation and the qth observation is \\n \\n\\uf028\\n\\uf029\\n2\\n1\\nm\\npj\\nqj\\nj\\nv\\nv\\n\\uf03d\\n\\uf02d\\n\\uf0e5\\n \\n \\nThe extension from two features to three features is fairly easy to \\nunderstand. It involves measuring the distance in three dimensions ra-\\nther than two. Imagining distances when m > 3 is not so easy, but the \\nformula is a natural extension of that for one, two, and three dimen-\\nsions.  \\n \\n \\n26 \\n                                                                  Chapter 2 \\n \\n \\nFigure 2.1   The Euclidean distance between observations A and B, with \\nco-ordinates (xA, yA) and (xB, yB), is the length of the line AB. \\n \\n \\n \\n \\nAnother concept we need in order to understand the k-means algo-\\nrithm is the center of a cluster (sometimes referred to as the cluster’s \\ncentroid). Suppose that a certain set of observations is regarded as a \\ncluster. The center is calculated by averaging the values of each of the \\nfeatures for the observations in the cluster. Suppose there are four fea-\\ntures and the five observations in Table 2.1 are considered to be a clus-\\nter. The center of the cluster is a point that has values of 0.914, 0.990, \\n0.316, and 0.330 for features 1, 2, 3, and 4, respectively. (For example, \\n0.914 is the average of 1.00, 0.80, 0.82, 1.10, and 0.85.) The distance \\nbetween each observation and the center of the cluster (shown in the \\nfinal column of Table 2.1) is calculated in the same way as the distance \\nbetween A and B in Figure 2.1. For example, the distance of the first ob-\\nservation from the center of the cluster is \\n \\n√(1.00 −0.914)2 + (1.00 −0.990)2 + (0.40 −0.316)2 + (0.25 −0.330)2 \\n \\nwhich equals 0.145. \\n \\n \\nFeature y\\nA\\nB\\n A\\n B\\n A\\n B\\nFeature x\\nUnsupervised Learning \\n                                                                                   27 \\n \\n \\n \\nTable 2.1 Calculation of the center of a cluster of five observations on \\nfour features.  \\n \\nObserv-\\nation \\nFeature \\n1 \\nFeature \\n2 \\nFeature \\n3 \\nFeature \\n4 \\nDistance \\nto center \\n1 \\n1.00 \\n1.00 \\n0.40 \\n0.25 \\n0.145 \\n2 \\n0.80 \\n1.20 \\n0.25 \\n0.40 \\n0.258 \\n3 \\n0.82 \\n1.05 \\n0.35 \\n0.50 \\n0.206 \\n4 \\n1.10 \\n0.80 \\n0.21 \\n0.23 \\n0.303 \\n5 \\n0.85 \\n0.90 \\n0.37 \\n0.27 \\n0.137 \\nCenter \\n0.914 \\n0.990 \\n0.316 \\n0.330 \\n \\n \\nFigure 2.2 illustrates how the k-means algorithm works. The first \\nstep is to choose k, the number of clusters (more on this later). We then \\nrandomly choose k points for the centers of the clusters. The distance of \\neach observation from each cluster center is calculated as indicated \\nabove and observations are assigned to the nearest cluster center. This \\nproduces a first division of the observations into k clusters. We then \\ncompute new centers for each of the clusters, as indicated in Figure 2.2. \\nThe distances of each observation from the new cluster centers is then \\ncomputed and the observations are re-assigned to the nearest cluster \\ncenter. We then compute new centers for each of the clusters and con-\\ntinue in this fashion until the clusters do not change.  \\n \\nFigure 2.2   The k-means algorithm \\n \\n \\n \\nAssign each observation to \\nnearest  cluster center\\nChoose k random points as \\ncluster centers\\nCalculate new cluster \\ncenters\\nHave cluster \\ncenters changed?\\nNo\\nEnd\\nYes\\n28 \\n                                                                  Chapter 2 \\n \\n \\nA measure of the performance of the algorithm is the within-cluster \\nsum of squares, also known as the inertia.  Define di as the distance of \\nthe ith observation from the center of the cluster to which it belongs. \\nThen: \\n \\n             Inertia = Within-cluster sum of squares = ∑\\n𝑑𝑖\\n2\\n𝑛\\n𝑖=1\\n \\n \\nwhere n is the number of observations. For any given value of k, the ob-\\njective of the k-means algorithm should be to minimize the inertia. The \\nresults from one run of the algorithm may depend on the initial cluster \\ncenters that are chosen. It is therefore necessary to re-run the algorithm \\nmany times with different initial cluster centers. The best result across \\nall runs is the one for which the inertia is least.  \\nGenerally, the inertia decreases as k increases. In the limit when k \\nequals the number of observations there is one cluster for each obser-\\nvation and the inertia is zero. \\n \\n \\n2.3     Choosing k \\n \\nIn some cases, the choice of k may depend on the objective of the \\nclustering. For example, a company that is planning to produce small, \\nmedium, large, and extra-large sweaters for men might collect data on \\nvarious relevant features (arm length, shoulder width, chest measure-\\nment, etc.) for a random sample of men and then create four clusters to \\nhelp with product design. In other situations, the user of the algorithm \\nmay not have any preconceived ideas about k and just want to optimally \\ngroup each observation with other similar observations. \\nThe elbow method is a popular approach for determining the number \\nof clusters. The k-means algorithm is carried out for a range of values of \\nk (e.g., all values between 1 and 10). The inertia is then plotted against \\nthe number of clusters as indicated in Figure 2.3.   The slope of the line \\nin this chart indicates how the within-cluster sum of squares declines as \\nthe number of clusters increases. In this example, the decline is quite \\nlarge when we move from one to two, two to three, and three to four \\nclusters. After four clusters, the decline is much smaller. We conclude \\nthat the optimal number of clusters is four.  \\nIn addition to the within-cluster sum of squares, we are likely to be \\ninterested in how distinct the clusters are. If two clusters are very close \\ntogether we might reasonably conclude that not much is gained by \\nkeeping them separate. Analysts therefore often monitor the distance\\nUnsupervised Learning \\n                                                                                   29 \\n \\n \\n \\nbetween cluster centers. If changing the number of clusters from k to \\n𝑘+ 1 leads to two clusters with centers that are very close to each oth-\\ner, it might be considered best not to make the change. \\n \\nFigure 2.3   Application of the elbow method. The inertia (within-\\ncluster sum of squares) is plotted against the number of clusters  \\n \\n \\n \\nA less subjective way of choosing the number of clusters is the sil-\\nhouette method.  Again, we carry out the k-means algorithm for a range \\nof values of k. For each value of k, we calculate for each observation, i, \\nthe average distance between the observation and the other observa-\\ntions in the cluster to which it belongs. Define this as a(i). We also calcu-\\nlate, for each of the other clusters, the average distance between the \\nobservation and the observations in that cluster. We define b(i) as the \\nminimum value of these average distances across all the other clusters. \\nWe expect b(i) to be greater than a(i) as otherwise it probably would \\nhave made sense to allocate observation i to a different cluster. The sil-\\nhouette of an observation measures the extent to which b(i) is greater \\nthan a(i). It is1  \\n \\n                                                 \\n1 See L. Kaufman and P. Rousseeuw, Finding Groups in Data: An Introduction to Clus-\\nter Analysis, Wiley 1990.  \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nInertia\\nNumber of Clusters\\n30 \\n                                                                  Chapter 2 \\n \\n \\n( )\\n( )\\n( )\\nmax[ ( ), ( )]\\nb i\\na i\\ns i\\na i b i\\n\\uf02d\\n\\uf03d\\n \\n \\nThe silhouette, s(i), lies between −1 and +1. (As already indicated, for \\nobservations that have been allocated correctly it is likely to be posi-\\ntive.) As it becomes closer to +1, the observation more clearly belongs \\nto the group to which it has been assigned. The average of s(i) over all \\nobservations in a cluster is a measure of the tightness of the grouping of \\nthose observations. The average of s(i) over all observations in all clus-\\nters is an overall measure of the appropriateness of the clustering and is \\nreferred to as the average silhouette score.  If for a particular data set \\nthe average silhouette scores are 0.70, 0.53, 0.65, 0.52, and 0.45 for k = \\n2, 3, 4, 5, and 6, respectively, we would conclude that k = 2 and 4 are \\nbetter choices for the number of clusters than k = 3, 5, and 6.  \\nYet another approach for choosing k, known as the gap statistic, was \\nsuggested by Tibshirani et al (2001).2  In this, the within-cluster sum of \\nsquares is compared with the value we would expect under the null hy-\\npothesis that the observations are created randomly. We create N sets \\nof random points and, for each value of k that is considered, we cluster \\neach set, calculating the within-cluster sum of squares. (N=500 usually \\nworks well.) Define \\n \\nmk: \\nthe mean of the within-cluster sum of squares for randomly \\ncreated data when there are k clusters \\n sk: \\nthe standard deviation of the within-cluster sum of squares \\nfor randomly created data when there are k clusters \\nwk: \\nthe within-cluster sum of squares for the data we are con-\\nsidering when there are k clusters \\n \\nWe set \\n \\nGap(k)= mk−wk \\n \\nThis is the difference between the within-cluster sum of squares statist-\\nic for the random data and the data of interest. It is argued that the best \\nchoice for k is the smallest value such that Gap(k) is within sk+1 of \\nGap(k+1).   \\n                                                 \\n2 See R. Tibshirani, G. Walther, and T. Hastie (2001), “Estimating the number of clus-\\nters in a data set via the gap statistic,” Journal of the Royal Statistical Society, B, 63, \\nPart 2: 411-423. \\nUnsupervised Learning \\n                                                                                   31 \\n \\n \\n \\n2.4     The Curse of Dimensionality \\n \\nAs the number of features increases, the k-means algorithm becomes \\naffected by what is known as the “curse of dimensionality.” Distances \\nbetween observations increase. Consider the Euclidean distance be-\\ntween a point where all features equal 1.0 and a point where all fea-\\ntures equal 0.0. When there is one feature the distance is 1.0; when \\nthere are two features the distance is √2 or 1.4; when there are three \\nfeatures, it is √3 or 1.7; when then are 100 features it is 10; and when \\nthere are 1,000 features it is 31.6. One consequence of this is that we \\ncannot compare a within-cluster sum of squares given by data with a \\nsmall number of features to one given by data with a large number of \\nfeatures. \\nAnother problem is that, as the number of features increases, the \\ndistance measure that we have defined does not always differentiate \\nwell between observations that are close and those that are far apart. As \\na result the k-means algorithm works less well. This has led some users \\nof the algorithm to search for alternatives to the Euclidean distance \\nmeasure.  \\nThe Euclidean distance between an observation where feature j is xj \\nand another observation where feature j is yj  can be written \\n \\n√∑\\n(𝑥𝑗−𝑦𝑗)\\n2\\n𝑚\\n𝑗=1\\n \\n \\nOne alternative is \\n \\n1\\n2\\n2\\n1\\n1\\n1\\nm\\nj\\nj\\nj\\nm\\nm\\nj\\nj\\nj\\nj\\nx y\\nx\\ny\\n\\uf03d\\n\\uf03d\\n\\uf03d\\n\\uf02d\\n\\uf0e5\\n\\uf0e5\\n\\uf0e5\\n \\n \\nThis always lies between 0 and 2.  \\n  \\n \\n2.5     Country Risk \\n \\nConsider the problem of understanding the risk of countries for for-\\neign investment. Among the features that can be used for this are: \\n32 \\n                                                                  Chapter 2 \\n \\n \\n1. \\nThe real GDP growth rate (using data from the International \\nMonetary Fund) \\n2. \\nA corruption index (produced by Transparency International) \\n3. \\nA peace index (produced by Institute for Economics and Peace) \\n4. \\nA legal risk index (produced by Property Rights Association) \\n \\nValues for each of these features for 122 countries and all analyses \\ncarried out are at ANDY LANDU NGOMA  Table 2.2 provides \\nan extract from the data. The table shows the importance of feature \\nscaling (see Section 2.1). The real GDP growth rate (%) is typically a \\npositive or negative number with a magnitude less than 10. The corrup-\\ntion index is on a scale from 0 (highly corrupt) to 100 (no corruption). \\nThe peace index is on a scale from 1 (very peaceful) to 5 (not at all \\npeaceful). The legal risk index runs from 0 to 10 (with high values being \\nfavorable). Table 2.3 shows the data in Table 2.2 after it has been scaled \\nusing Z-score normalization. It shows that Australia’s real GDP growth \\nrate is slightly above average and its corruption index is 1.71 standard \\ndeviations above the average. Its peace index is 1.20 standard devia-\\ntions below average (but low peace indices are good) and the legal risk \\nindex is 1.78 standard deviations above the average. \\n \\nTable 2.2  First few observations for clustering countries according to \\ntheir risk for international investment (see csv file) \\n \\nCountry \\nReal GDP \\ngrowth rate \\n(% per yr) \\nCorruption \\nindex \\nPeace \\nindex \\nLegal risk \\nindex \\nAlbania  \\n  3.403 \\n    39 \\n1.867 \\n3.822 \\nAlgeria  \\n  4.202 \\n    34 \\n2.213 \\n4.160 \\nArgentina  \\n−2.298 \\n   36 \\n1.957 \\n4.568 \\nArmenia  \\n  0.208 \\n   33 \\n2.218 \\n4.126 \\nAustralia  \\n  2.471 \\n   79 \\n1.465 \\n8.244 \\nAustria  \\n  1.482 \\n   75 \\n1.278 \\n8.012 \\nAzerbaijan \\n−3.772 \\n   30 \\n2.450 \\n3.946 \\n \\n      Once the data has been scaled, a natural next step, given that there \\nare only four features, is to examine the features in pairs with a series of \\nscatter plots. This reveals that the corruption index and legal risk index \\nare highly correlated as shown in Figure 2.4. (This is perhaps not sur-\\nprising.  Corruption is likely to be more prevalent in countries where \\nthe legal systems are poor.) We therefore eliminate the corruption\\nUnsupervised Learning \\n                                                                                   33 \\n \\n \\n \\nindex as the information it provides is largely captured by the legal risk \\nindex. This means that we can consider our data as being points in \\nthree-dimensional space, the dimensions being: real GDP growth rate, \\npeace index, and legal risk index \\n \\nTable 2.3   Data in Table 2.2 after using Z-score scaling (see Excel file) \\n \\nCountry \\nReal GDP \\ngrowth rate \\n(% per yr) \\nCorruption \\nindex \\nPeace \\nindex \\nLegal risk \\nindex \\nAlbania  \\n   0.32 \\n−0.38 \\n−0.31 \\n−1.20 \\nAlgeria  \\n   0.56 \\n−0.64 \\n   0.47 \\n−0.97 \\nArgentina  \\n−1.44 \\n−0.54 \\n−0.10 \\n−0.69 \\nArmenia  \\n−0.67 \\n−0.69 \\n   0.48 \\n−0.99 \\nAustralia  \\n   0.03 \\n   1.71 \\n−1.20 \\n   1.78 \\nAustria  \\n−0.27 \\n   1.50 \\n−1.62 \\n   1.62 \\nAzerbaijan \\n−1.90 \\n−0.85 \\n   1.00 \\n−1.11 \\n \\n \\nFigure 2.4   Scatter plot of scaled legal risk index and corruption index \\n(see Excel file) \\n \\nFigure 2.5 shows the results of applying the k-means algorithm to \\nthe country risk data when three features (real GDP growth rate, peace \\nindex, and legal risk index) are used. As expected, the total within-\\ncluster sum of squares declines as the number of clusters, k, is in-\\ncreased. As explained earlier we can use the figure to look for an elbow, \\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nCorruption Index\\nLegal Risk \\nIndex\\n34 \\n                                                                  Chapter 2 \\n \\n \\na point where the benefit from increasing the number of clusters starts \\nto be relatively small. The elbow is not as pronounced in Figure 2.5 as it \\nis in Figure 2.3.  However, a case can be made for three clusters as the \\ndecrease in the inertia as we move from one to two and two to three \\nclusters is quite a bit greater than when we move from three to four \\nclusters. \\n \\nFigure 2.5   Variation of inertia (within-cluster sum of squares) with \\nnumber of clusters for country risk example (from Python output) \\n \\n \\n \\nThe results from the silhouette method are given in Table 2.4. It can \\nbe seen that the average silhouette score is greatest when the number \\nof clusters is three.  For this particular data set, both the elbow method \\nand the silhouette method point to the use of three clusters.3   \\nTable 2.5 shows the cluster centers after scaling. It shows that high-\\nrisk countries are on average over one standard deviation worse than \\nthe mean for all three features. (Remember, high values are bad for the \\npeace index.) Tables 2.6, 2.7, and 2.8 give the allocation of countries to \\nthree clusters.  \\n                                                 \\n3 The elbow method and the silhouette method do not always agree. \\n0\\n100\\n200\\n300\\n400\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nInertia \\nNumber of Clusters\\nUnsupervised Learning \\n                                                                                   35 \\n \\n \\n \\nTable 2.4   Variation of the average silhouette score with the number of \\nclusters (from Python output) \\n \\nNumber of  \\nclusters \\nAverage silhouette  \\nScore \\n2 \\n0.363 \\n3 \\n0.388 \\n4 \\n0.370 \\n5 \\n0.309 \\n6 \\n0.303 \\n7 \\n0.315 \\n8 \\n0.321 \\n9 \\n0.292 \\n10 \\n0.305 \\n \\n \\n \\nTable 2.5   Cluster centers after features have been scaled so that mean \\nis zero and standard deviation is one (from Python output) \\n \\n \\nPeace index \\nLegal index \\nReal GDP growth \\nrate  \\nHigh risk \\n 1.39 \\n−1.04 \\n−1.79 \\nModerate risk \\n 0.27 \\n−0.45 \\n   0.36 \\nLow risk \\n        −0.97 \\n   1.17 \\n   0.00 \\n \\n \\n \\n \\nTable 2.6   High-risk countries (from Python output) \\n \\n \\n \\n \\n \\nArgentina\\nLebanon\\nAzerbaijan\\nNigeria\\nBrazil\\nRussia\\nBurundi\\nTrinidad and Tobago\\nChad\\nUkraine\\nDemocratic Republic of Congo\\nVenezuela\\nEcuador\\nYemen\\n36 \\n                                                                  Chapter 2 \\n \\n \\nTable 2.7   Moderate-risk countries (from Python output) \\n \\n \\n \\n \\n \\nAlbania\\nMadagascar\\nAlgeria\\nMalawi\\nArmenia\\nMali\\nBahrain\\nMauritania\\nBangladesh\\nMexico\\nBenin\\nMoldova\\nBolivia\\nMontenegro\\nBosnia and Herzegovina\\nMorocco\\nBulgaria\\nMozambique\\nCameroon\\nNepal\\nChina\\nNicaragua\\nColombia\\nOman\\nCroatia\\nPakistan\\nCyprus\\nPanama\\nDominican Republic\\nParaguay\\nEgypt\\nPeru\\nEl Salvador\\nPhilippines\\nEthiopia\\nRomania\\nGabon\\nRwanda\\nGeorgia\\nSaudi Arabia\\nGhana\\nSenegal\\nGreece\\nSerbia\\nGuatemala\\nSierra Leone\\nHonduras\\nSouth Africa\\nIndia\\nSri Lanka\\nIndonesia\\nTanzania\\nIran\\nThailand\\nIsrael\\nThe FYR of Macedonia\\nJamaica\\nTunisia\\nJordan\\nTurkey\\nKazakhstan\\nUganda\\nKenya\\nVietnam\\nKuwait\\nZambia\\nLatvia\\nZimbabwe\\nLiberia\\nUnsupervised Learning \\n                                                                                   37 \\n \\n \\n \\nTable 2.8   Low-risk countries (from Python output) \\n \\n \\n \\n \\n2.6     Alternative Clustering Approaches \\n \\nThe k-means algorithm is the most popular approach to clustering, \\nbut there are alternatives.  One is agglomerative hierarchical clustering. \\nThis involves the following steps: \\n \\n1. Start with each observation in its own cluster \\n2. Combine the two closest clusters \\n3. Repeat step 2 until all observations are in single cluster  \\nThe advantage of this approach is that the clusters form a hierarchy \\nso that we can see clusters within clusters. The hierarchy can be used to \\nobtain exactly k clusters for any given value of k between one and the \\nnumber of observations. Its disadvantage is that it tends to be computa-\\ntionally very time consuming when there are a large number of obser-\\nvations.  \\nAustralia\\nMalaysia\\nAustria\\nMauritius\\nBelgium\\nNetherlands\\nBotswana\\nNew Zealand\\nCanada\\nNorway\\nChile\\nPoland\\nCosta Rica\\nPortugal\\nCzech Republic\\nQatar\\nDenmark\\nSingapore\\nEstonia\\nSlovakia\\nFinland\\nSlovenia\\nFrance\\nSpain\\nGermany\\nSweden\\nHungary\\nSwitzerland\\nIceland\\nTaiwan\\nIreland\\nUnited Arab Emirates\\nItaly\\nUnited Kingdom\\nJapan\\nUnited States\\nKorea (South)\\nUruguay\\nLithuania\\n38 \\n                                                                  Chapter 2 \\n \\n \\nA number of different measures of closeness between two clusters, A \\nand B, have been proposed for use in step 2. One is the average Euclide-\\nan distance between an observation in cluster A and an observation in \\ncluster B. Alternatively, we can use the minimum of these distances or \\nthe maximum of them. Another measure (a version of what is known as \\nWard’s method) equals the increase in inertia when two clusters are \\ncombined. Whatever the measure chosen, step 2 involves searching for \\nthe two clusters with the smallest measure and then combining them. \\nSometimes clusters are estimated from assumed statistical distribu-\\ntions. This is known as distribution-based clustering. Suppose for sim-\\nplicity that there is only one feature and that the observations exhibit \\nthe probability distribution shown in Figure 2.6. We might reasonably \\nassume that the observations come from a mixture of two normal dis-\\ntributions. This is a distribution when there is a probability p that an \\nobservation comes from a normal distribution with a particular mean \\nand standard deviation and a probability 1−p that it comes from anoth-\\ner normal distribution with a different mean and standard deviation. \\nStatistical tools can be used to distinguish between the two distribu-\\ntions and therefore identify two clusters. A similar exercise can be car-\\nried out when there are several features and more than two distribu-\\ntions. \\n \\nFigure 2.6  Probability distribution for data on a feature from which \\ntwo normally distributed clusters could be separated \\n \\n \\n \\nUnsupervised Learning \\n                                                                                   39 \\n \\n \\n \\nDensity-based clustering involves forming clusters according to the \\ncloseness of individual observations. We might initially form a cluster of \\neight observations that are close to each other, then add another obser-\\nvation to the cluster that is close to at least five of these observations, \\nthen add another observation that is close to at least five of the observa-\\ntions in the new cluster, and so on. This can lead to clusters that have \\nquite different shapes from the ones considered by the k-means algo-\\nrithm. Figure 2.7 gives two examples. The k-means algorithm would not \\nfind these clusters because of the way it uses cluster centers. The two \\nclusters in Figure 2.7a have the same centers and would therefore not \\nbe distinguished by k-means. In Figure 2.7b, k-means might identify \\nseveral clusters but not the ones that are visually obvious. \\n \\n \\n Figure 2.7   Clusters which might be identified by a density-based clus-\\ntering algorithm \\n \\n(a) \\n                                                        (b) \\n \\n \\n2.7   Principal Components Analysis \\n \\nAs an alternative to clustering, we can use principal components \\nanalysis (PCA) to understand the structure of data.4 This takes data on \\nm features and replaces it with a new set of m variables, referred to as \\nfactors or principal components, so that: \\n                                                 \\n4 PCA was proposed by Karl Pearson as early as 1901: K. Pearson (1901), “On lines \\nand planes of closest fit to systems on points in space,” Philosophical Magazine, \\n2(11): 559−572.  \\n40 \\n                                                                  Chapter 2 \\n \\n \\n\\uf0b7 Any observation on features is a linear combination of the fac-\\ntors \\n\\uf0b7 The m factors are uncorrelated \\nPCA works best on normalized data. The first factor accounts for as \\nmuch of the variability in the data as possible. Each succeeding factor \\nthen accounts for as much of the remaining variability in the data sub-\\nject to the condition that it is uncorrelated to preceding factors. The \\nquantity of a particular factor in a particular observation is the factor \\nscore. \\nA principal components analysis is often carried out for interest rate \\nmovements. (This could be relevant to a data scientist who is interested \\nin studying the effect of interest rates on consumer behavior.) Table 2.9 \\nshows the first three factors that are obtained when principal compo-\\nnents analysis is applied to daily movements in interest rates with ma-\\nturities of 1, 2, 3, 4, 5, 7, 10, and 30 years over a 12-year time period.5 \\nThe numbers in each column are referred to as factor loadings and have \\nthe property that their sum of squares is 1. All rates move in the same \\ndirection for the first factor (principal component one, PC1). If we have \\n+10 basis point units of that factor, the one-year rate increases by 2.16 \\nbasis points (or 0.0216%), the two-year rate increases by 3.31 basis \\npoints, and so on. If we have −20 basis point units of the first factor the \\none-year rate decreases by 4.32 basis points, the two-year rate decreas-\\nes by 6.62 basis points, and so on.  \\nPC2 is different from PC1 in that the first four rates move in one di-\\nrection while the next four rates move in the opposite direction. This \\nprovides a “twist” to the term structure of interest rates where the \\nslope changes. In PC3 short and long rates move in one direction while \\nintermediate rates move in the other direction. This is referred to as a \\n“bowing” of the term structure. \\nThe importance of factors is measured by the standard deviations of \\ntheir factors score across the observations. These are shown in Table \\n2.10 for the first three factors in our interest rate example. The variance \\naccounted for by all eight factor scores is 388.8 in this example. The \\nfraction of the variance accounted for by the first (most important) fac-\\ntor is therefore \\n \\n                                                 \\n5 See J. Hull, Options, Futures, and Other Derivatives, 10th edition, Pearson, page 513 \\nfor this example. The worksheet for this is under the principal components analysis \\ntab at \\nANDY LANDU NGOMA \\nUnsupervised Learning \\n                                                                                   41 \\n \\n \\n \\n2\\n17 55\\n338 8\\n.\\n.\\n \\n \\nor about 90%. The fraction accounted for by the first two factors is \\n \\n2\\n2\\n17 55\\n4 77\\n338 8\\n.\\n.\\n.\\n\\uf02b\\n \\n \\nor about 97%. This shows that replacing the eight features defining \\nterm structure movements by two new variables (PC1 and PC2) cap-\\ntures most of the variation in the data. This illustrates what we are try-\\ning to achieve with PCA. We are trying to find a small number of varia-\\nbles that capture the structure of the data. \\n \\nTable 2.9   Factor loadings defining the principal components for inter-\\nest rate movements \\n \\nMaturity \\nPC1 \\nPC2 \\nPC3 \\n1yr \\n0.216 \\n    −0.501 \\n    0.627 \\n2yr \\n0.331 \\n    −0.429 \\n    0.129 \\n3yr \\n0.372 \\n    −0.267 \\n −0.157 \\n4yr \\n0.392 \\n    −0.110 \\n −0.256 \\n5yr \\n0.404 \\n       0.019 \\n −0.355 \\n7yr \\n0.394 \\n       0.194 \\n −0.195 \\n10yr \\n0.376 \\n       0.371 \\n   0.068 \\n30yr \\n0.305 \\n       0.554 \\n   0.575 \\n \\n \\nTable 2.10   Standard deviation of factor scores for interest rates (in \\nbasis points) \\n \\nPC1 \\nPC2 \\nPC3 \\n17.55 \\n4.77 \\n2.08 \\n \\nFor a further example of PCA we return to the country risk data that \\nwe considered in Section 2.5. When all four features are used, the fac-\\ntors and the factor scores are shown in Tables 2.11 and 2.12. These re-\\nveal some interesting properties of the data. The first factor which ac-\\ncounts for 64% of the variance places roughly equal weight on corrup-\\ntion, peace, and legal risk. (Remember that a low score for peace is   \\n42 \\n                                                                  Chapter 2 \\n \\n \\ngood.) \\nThe second factor accounts for a further 24% of the variance and \\nplaces most of the weight on the real GDP growth rate. This recognizes \\nthat the real GDP growth rate is giving quite different information from \\nthe other three features.  \\nIn interpreting Table 2.11, note that we can change the signs of all \\nthe factor loadings in a column without changing the model. This is be-\\ncause the number of units of a factor that are present in an observation \\ncan be positive or negative. We should not for example read anything \\ninto the negative factor loading for the real GDP growth rate in PC2. We \\ncould change the signs of all the factor loadings for PC2 without chang-\\ning the model. \\n \\nTable 2.11   Factor loadings showing principal components for the \\ncountry risk data (see Excel PCA file) \\n \\n \\nPC1 \\nPC2 \\nPC3 \\nPC4 \\nCorruption index \\n0.594 \\n0.154 \\n−0.292 \\n−0.733 \\nPeace index \\n-0.530 \\n0.041 \\n−0.842 \\n−0.086 \\nLegal risk index \\n0.585 \\n0.136 \\n−0.431 \\n 0.674 \\nGDP growth rate \\n0.152 \\n−0.978 \\n−0.141 \\n−0.026 \\n \\n \\nTable 2.12   Standard deviation of factor scores for country risk data \\n(see Excel PCA file) \\n \\nPC1 \\nPC2 \\nPC3 \\nPC4 \\n1.600 \\n0.988 \\n0.625 \\n0.270 \\n \\nThe third factor which accounts for about 10% of the variance places \\nmost weight on the peace index and suggests that this has extra infor-\\nmation over that in corruption and legal risk. The fourth factor is rela-\\ntively unimportant, accounting for less than 2% of the variance.  The \\nPCA confirms the result in Figure 2.4 that the corruption index and legal \\nrisk index provide similar information.  \\nPCA is sometimes used in supervised learning as well as unsuper-\\nvised learning. We can use it to replace a long list of features by a much \\nsmaller list of manufactured features derived from a PCA. The manufac-\\ntured features are chosen so that they account for most of the variabil-\\nity in the data we are using for prediction and have the nice property \\nthat they are uncorrelated. \\nUnsupervised Learning \\n                                                                                   43 \\n \\n \\n \\nFinally, we emphasize that, when clustering or using PCA, we are not \\ntrying to predict anything. We are merely investigating the structure of \\nthe data. In our country risk example, an analyst might assume that the \\nfeatures are related to investment risk, but there is no guarantee that \\nthis is so. (For example, we have not tried to relate the features to losses \\nincurred on investments in different countries, as we might do in su-\\npervised learning.)   \\n \\n \\nSummary \\n \\nUnsupervised learning is concerned with understanding patterns \\nwithin data. Typically, it involves looking for clusters, i.e., groups of sim-\\nilar observations. Companies often use unsupervised learning to under-\\nstand the different types of customers they have so that they can com-\\nmunicate with them more effectively.  \\nFeature scaling is usually necessary for clustering.  Without feature \\nscaling, the impact of a feature on clustering is likely to depend on the \\nscale used to measure it. There are two main approaches to feature scal-\\ning. One is Z-score normalization where features are scaled so that they \\nhave a mean of zero and a standard deviation of one. The other is the \\nmin-max method where all features are scaled so that they have values \\nbetween zero and one.  \\nA clustering algorithm requires a distance measure. The most popu-\\nlar such measure is the Euclidean distance, which is the square root of \\nthe sum of squared differences between feature values. A cluster’s cen-\\nter is the point obtained by averaging the feature values for all observa-\\ntions in the cluster. The most popular clustering algorithm is k-means. \\nFor a particular value of k (the number of clusters), it minimizes inertia, \\nwhich is defined as the total sum of squared distances of the observa-\\ntions from their cluster centers.  \\nChoosing the best value for the number of clusters, k, is not always \\neasy. One approach is the “elbow method” which involves continuing to \\nincrease k until the improvement in the inertia is relatively small. An-\\nother approach is the silhouette method which compares for each ob-\\nservation (a) the average distance of the observation from other points \\nin its own cluster and (b) the average distance of the observation from \\npoints in the closest other cluster. A third approach involves calculating \\nthe gap statistic which compares the clustered observations to observa-\\ntions that are created randomly.  \\n44 \\n                                                                  Chapter 2 \\n \\n \\nAs the number of features increases, the Euclidean distance measure \\nincreases. This is an aspect of the curse of dimensionality and makes it \\nmore difficult to use the k-means algorithm when there are a large \\nnumber of features. As a result, it may be desirable to change the dis-\\ntance measure so that it stays within certain bounds as the number of \\nfeatures increases. \\nThere are a number of alternatives to the k-means algorithm. One is \\nhierarchical clustering. In this we start with the situation where each \\nobservation is in its own cluster. We then slowly reduce the number of \\nclusters by combining clusters that are close to each other. Distribution-\\nbased clustering involves assuming a distribution for the data that is a \\nmixture of normal (or other) distributions and estimating the parame-\\nters of those distributions. Density-based clustering involves looking for \\nregions where data is dense without reference to cluster centers. \\nPrincipal components analysis (PCA) is an important tool in machine \\nlearning. It involves replacing a large number of features by a smaller \\nnumber of manufactured features that capture most of the variability. \\nThe manufactured features are uncorrelated with each other.  \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n2.1 \\nWhy is feature scaling important in unsupervised learning? Ex-\\nplain two methods for feature scaling. What are the advantages \\nand disadvantages of each method? \\n2.2 \\nSuppose there are three features, A, B, and C. One observation has \\nvalues 2, 3, and 4 for A, B, and C, respectively. Another has values \\n6, 8, and 7 for A, B, and C respectively. What is the Euclidean dis-\\ntance between the two observations? \\n2.3 \\nWhat would be the center of a cluster consisting of the two ob-\\nservations in question 2.2? \\n2.4 \\nExplain the steps in the k-means algorithm. \\n2.5 \\nExplain (a) the elbow method and (b) the silhouette method for \\nchoosing the number of clusters, k.  \\n2.6 \\nWhy do the Euclidean distances between observations increase \\nas the number of features increases? Suppose that you start with \\nten features and then by mistake create ten more features that \\nare identical to the first ten. What effect does this have on the dis-\\ntance between two of the observations? \\n2.7 \\nHow does hierarchical clustering work? What are its advantages \\nand disadvantages relative to k-means? \\nUnsupervised Learning \\n                                                                                   45 \\n \\n \\n \\n2.8 \\nExplain what is meant by (a) distribution-based clustering and \\n(b) density-based clustering. \\n2.9 \\nUnder what circumstances is principal components analysis most \\nuseful for understanding data? \\n2.10 What is meant by (a) a factor loading and (b) a factor score? \\n \\nEXERCISES \\n \\n2.11 Use data at \\n                       ANDY LANDU NGOMA \\nto calculate cluster centers without scaling for the 14 high risk \\ncountries in Table 2.6. Scale the cluster centers. Verify that your \\nanswer agrees with the result in Table 2.5 for high risk countries.  \\n2.12 Use the principal components analysis results to determine how \\nyou would describe country risk with two factors. Give results for \\nboth the scaled data and the original non-scaled data.  \\n2.13 Python code used to produce the results in this chapter can be \\nfound at  \\nANDY LANDU NGOMA \\n(a) Carry out k-means clustering for k=3 with all four features \\n(corruption index, peace index, legal risk index, and real GDP \\ngrowth rate). Compare the countries that are in the high-risk \\ncluster with those that are in the high-risk cluster when only \\nthree features are used (see Table 2.6). \\n(b) Use hierarchical clustering to determine three clusters from \\nthe peace index, legal risk index, and real GDP growth rate.  \\nCompare the countries that are in the high-risk cluster with \\nthose that are in the high-risk cluster when the k-means al-\\ngorithm is used (see Table 2.6). A Python package, Agglom-\\nerativeClustering, for hierarchical clustering can be import-\\ned from sklearn.cluster. Try different measures of closeness                       \\n(referred to as “linkage” in the package).  \\n2.14   The country risk data used in Section 2.5 is from the years 2016 \\nand 2017.  Data for the year 2019 is at \\n ANDY LANDU NGOMA \\n     Use the 2019 data to determine clusters.  How do the clusters dif-\\nfer from those in Tables 2.6 to 2.8?  What other data would you \\nlike to collect to improve your clustering? \\n \\n\\xa0\\n \\n47 \\n \\n \\n \\n \\n \\n \\nChapter 3 \\n \\nSupervised Learning:  \\nLinear and Logistic Regression \\n \\n \\n \\n \\nLinear regression has been used by statisticians for many years. The \\nfamous mathematician Carl Friedrich Gauss is credited with first sug-\\ngesting the least squares approach that underlies linear regression in \\nabout the year 1800.  In machine learning, we do not have to assume \\nlinear relationships. (Indeed, many of the tools we will present later in \\nthis book lead to non-linear models.) In spite of this, linear regression \\nremains an important tool in machine learning. It is often one of the \\nfirst tools used by analysts in supervised learning. \\nPlain vanilla linear regression involves minimizing the mean \\nsquared error (mse) when the value of a target is being predicted from \\none or more features. This will be familiar to many readers. This chap-\\nter discusses how categorical features (i.e., features that are not numer-\\nical) can be incorporated into linear regression and therefore used for \\nprediction.  It also discusses Ridge, Lasso, and Elastic Net regression \\nwhich are particularly useful when predictions are being made from a \\nlarge number of features. It then moves on to explain logistic regres-\\nsion, which is a way of handling situations where the objective is to \\nlearn how to classify data. Finally, it covers the k-nearest-neighbor algo-\\nrithm which is a simple alternative to linear and logistic regression.  \\n \\n \\n48 \\n                                                                   Chapter 3 \\n \\n \\n \\n3.1     Linear Regression: One Feature \\n \\nWe start with the simple situation where the target Y is being pre-\\ndicted from a single feature X. In linear regression we assume a linear \\nrelationship so that the model is: \\n \\n𝑌= 𝑎+ 𝑏𝑋+ ε \\n \\nwhere a and b are constants and ɛ is the error term. Denote Xi and Yi (1 \\n≤ i ≤ n) as the values of X and Y for the ith observation in the training \\nset. The “best fit” values of a and b are defined as those that minimize \\nthe mean squared error (mse) for the observations in the training set. \\nThis means that we choose a and b so that: \\n \\n1\\n𝑛∑(𝑌𝑖−𝑎−𝑏𝑋𝑖)2\\n𝑛\\n𝑖=1\\n \\n \\nis minimized.1 We can use calculus to find the minimum. Denoting the \\naverages of the observations on X and Y by  𝑋̅ and 𝑌̅  \\n \\n𝑏= ∑\\n𝑋𝑖𝑌𝑖−𝑛𝑋̅𝑌̅\\n𝑛\\n𝑖=1\\n∑\\n𝑋𝑖\\n2 −𝑛𝑋̅2\\n𝑛\\n𝑖=1\\n \\n \\n𝑎= 𝑌̅ −𝑏𝑋̅ \\n \\nAn example of linear regression when there is only one feature is \\nprovided by the model in Figure 1.5 of Chapter 1.  This is based on the \\ntraining data set in Table 1.1, which is reproduced in Table 3.1.  In this \\ncase n = 10, 𝑋̅ = 43, and 𝑌̅ = 216,500. Also, \\n \\n        ∑\\n𝑋𝑖𝑌𝑖= 100,385,000\\n10\\n𝑖=1\\n \\n \\n∑\\n𝑋𝑖\\n2\\n10\\n𝑖=1\\n= 20,454 \\n \\nso that \\n                                                           \\n1 This is the same as minimizing the sum of squared errors as n is a constant for any \\ngiven data set. \\nLinear and Logistic Regression                                                                                             49 \\n \\n \\n \\n𝑏= 100,385,000 −10 × 43 × 216,500\\n20,454 −10 × 432\\n= 3,827.3 \\n \\n𝑎= 216,500 −3827.3 × 43 = 51,160.4 \\n \\nand the model is \\n \\n𝑌= 51,160.4 + 3,827.3𝑋 \\n \\nSometimes the parameter a is set equal to zero. In this case, \\n \\n𝑏= ∑\\n𝑋𝑖𝑌𝑖\\n𝑛\\n𝑖=1\\n∑\\n𝑋𝑖\\n2\\n𝑛\\n𝑖=1\\n \\n \\n(We use the linear model as an example here but it will be recalled that \\nin Chapter 1 we did not find a linear model to be the best model for the \\ndata in Table 3.1.)  \\n \\nTable 3.1   The training set: salaries for a random sample of ten people \\nworking in a particular profession in a certain area \\n \\nAge (years) \\nSalary ($) \\n25 \\n135,000 \\n55 \\n260,000 \\n27 \\n105,000 \\n35 \\n220,000 \\n60 \\n240,000 \\n65 \\n265,000 \\n45 \\n270,000 \\n40 \\n300,000 \\n50 \\n265,000 \\n30 \\n105,000 \\n \\n \\n3.2     Linear Regression: Multiple Features \\n \\nWhen more than one feature is used to predict a target we can write \\nthe model as \\n \\n𝑌= 𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚+ ε                      (3.1)  \\n                       \\n50 \\n                                                                   Chapter 3 \\n \\n \\n \\nwhere Y is the value of the target and the Xj (1 ≤ j ≤ m) are the values of \\nthe features that are used to predict Y. As before, the prediction error is \\ndenoted by ɛ.  The parameters a and 𝑏𝑗 (1 ≤ j ≤ m) are chosen to mini-\\nmize the mean squared error over the training data set. This means that \\nthe task in linear regression is to find values for a and the bj that mini-\\nmize \\n \\n1\\n𝑛∑(𝑌𝑖−𝑎−𝑏1𝑋𝑖1 −𝑏2𝑋𝑖2 −⋯−𝑏𝑚𝑋𝑖𝑚)2                (3.2)\\n𝑛\\n𝑖=1\\n \\n \\nwhere Yi and Xij are the values of the target and the jth feature for ob-\\nservation i. In machine learning, the parameter a is referred to as the \\nbias and the coefficients bj are referred to as the weights. As in the case \\nof a single feature, calculus can be used to determine the conditions for \\na minimum. This leads to a set of simultaneous equations for determin-\\ning the a and bj. These equations can be solved using matrix algebra.   \\nStatisticians list a number of conditions that must be satisfied for \\nlinear regression to be valid.   The relationship between the target and \\nthe features should be linear; there should be no correlation between \\nthe feature values; the errors in the prediction should be normally dis-\\ntributed with a constant standard deviation; and the observations \\nshould be independent. In practice, these conditions are at best satisfied \\nonly approximately. Identifying serious violations can lead an analyst to \\nfind a better model (e.g., by switching from a linear to a quadratic model \\nor by transforming feature values in some way). But it is worth noting \\nthat machine learning is different from traditional statistics in that we \\nare usually working with very large data sets and can handle model \\nsuitability issues by dividing the data into a training set, validation set, \\nand test set, as outlined in Chapter 1. \\n \\nGradient Descent Algorithm \\nAn alternative to matrix algebra, which has to be used for some of \\nthe analyses presented later in this chapter, is the gradient descent algo-\\nrithm. This is an iterative search routine for finding the minimum. Imag-\\nine plotting the expression in equation (3.2) as a function of a and the bj \\n(1 ≤ j ≤ m) in m + 1 dimensions. We can think of this function as a valley \\nand our task as that of finding the bottom of the valley.  Wherever we \\nare in the valley, we can use calculus to determine the path of steepest \\ndescent down the valley (i.e., the direction in which each of the a and bj \\nshould be changed to descend as quickly as possible). The gradient de-\\nscent method proceeds as follows: \\nLinear and Logistic Regression                                                                                             51 \\n \\n \\n \\n\\uf0b7 \\nchoose initial values for a and the bj \\n\\uf0b7 \\ncalculate path of steepest descent. \\n\\uf0b7 \\ntake a step down the path of steepest descent \\n\\uf0b7 \\nre-compute the path of steepest descent \\n\\uf0b7 \\ntake another step \\n\\uf0b7 \\nand so on \\n \\nWe will discuss this methodology in greater detail in Chapter 6. \\n \\nPolynomial Regressions \\nChapter 1 uses the data in Table 3.1 to carry out polynomial regres-\\nsions. These are regressions where there is a single feature X and 𝑋𝑗 is \\nset equal to 𝑋𝑗 so that the model is: \\n \\n𝑌= 𝑎+ 𝑏1𝑋+ 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚+ ε \\n \\nIn Chapter 1 we found that the model where m = 5 fits the training set \\nwell but does not generalize well to the validation set. The quadratic \\nmodel where m = 2 was chosen because it provided a better fit than the \\nlinear model while still generalizing well.  \\nSometimes products of features as well as powers of features are \\nused in a regression. An example where the target is being predicted \\nfrom two features is  \\n \\n𝑌= 𝑎+ 𝑏1𝑋1 + 𝑏2𝑋1\\n2 + 𝑏3𝑋2 + 𝑏4𝑋2\\n2 + 𝑏5𝑋1𝑋2 + ε \\n \\nRegression Statistics \\nA number of statistics can be produced from a linear regression and \\ncan be useful if the assumptions mentioned earlier are approximately \\nsatisfied. The R-squared statistic is between zero and one and measures \\nthe proportion of the variance in the target that is explained by the fea-\\ntures. It is  \\n \\n1 −\\nVariance of the errors ε\\nVariance of the observations on the target 𝑌 \\n \\nWhen there is only one feature, R-squared is the square of the coeffi-\\ncient of correlation. In the case of the data in Table 3.1, the R-squared \\nfor the linear model is 0.54 while that for the quadratic model is 0.80.   \\nThe t-statistic of the a or 𝑏𝑗 parameters estimated by a linear regres-\\nsion is the value of the parameter divided by its standard error. The P-\\n52 \\n                                                                   Chapter 3 \\n \\n \\n \\nvalue is the probability of obtaining a t-statistic as large as the one ob-\\nserved if we were in the situation where the parameter had no explana-\\ntory power at all. A P-value of 5% or less is generally considered as in-\\ndicating that the parameter is significant. If the P-value for the parame-\\nter bj in equation (3.1) is less than 5%, we are over 95% confident that \\nthe feature Xj has some effect on Y.  The critical t-value for 95% confi-\\ndence when the data set is large is 1.96 (i.e., t-statistics greater than \\n1.96 are significant in the sense that they give rise to P-values less than \\n5%).2  \\n \\n \\n3.3     Categorical Features \\n \\nThe features used for prediction can be categorical as well as numer-\\nical. As explained in Chapter 1, a categorical variable is a variable that \\ncan fall into one of a number of categories. For example, the purchasers \\nof a product might be categorized as male or female. The hair color of \\nwomen buying a particular beauty product might be categorized as \\nblonde, red, brown, or black. \\nThe standard way of dealing with categorical features is to create a \\ndummy variable for each category.  The value of this variable is one if \\nthe feature is in the category and zero otherwise. This is referred to as \\none-hot encoding. In the situation where individuals are categorized as \\nmale or female, we could create two dummy variables. For men the first \\ndummy variable would be one and the second would be zero. For wom-\\nen the first dummy variable would be zero and the second dummy vari-\\nable would be one. In the hair color example, there would be four dum-\\nmy variables and, in the case of each observation, we would assign one \\nto the relevant variable and zero to the other three. \\nThe procedure we have described is appropriate when there is no \\nnatural ordering between the feature values. When there is a natural \\nordering, we can reflect this in the numbers assigned. For example, if \\nthe size of an order is classified as small, medium, or large, we can re-\\nplace the feature by a numerical variable where small = 1, medium = 2, \\nand large = 3. Similarly, if job title is a feature and the categorization is \\nanalyst, associate, vice president, executive director, and managing di-\\n                                                           \\n2 This is for what is referred to as a “two-tailed test” where the analyst is testing for \\nthe significance of either a positive or negative relationship between the feature and \\nthe target. P-values are usually quoted for a two-tailed test.  In a one-tailed test, we \\nexpect the relationship to have a particular sign (positive or negative) and disregard \\nthe possibility of it having the other sign. The critical P-value for a one-tailed test \\nwhen the data set is large is 1.65.  \\nLinear and Logistic Regression                                                                                             53 \\n \\n \\n \\nrector, we might replace the feature by a numerical value where analyst \\n= 1, associate = 2, vice president = 3, executive director = 4, and manag-\\ning director = 5.  But, after considering salaries and responsibilities, we \\nmight choose a different set of numerical values such as analyst = 1, as-\\nsociate = 2, vice president = 4, executive director = 7, and managing di-\\nrector = 10. \\nOnce categorical features have been converted to numerical values, a \\nlinear regression can be carried out in the usual way.  Some of the \\ndummy variables created from categorical variables may have a signifi-\\ncant effect on the target while others do not.  \\n \\nThe Dummy Variable Trap \\nWhen one-hot encoding is used for one or more categorical variables \\nand there is a constant (bias) term in the regression, there is no unique \\nbest-fit linear regression equation. This is referred to as the dummy var-\\niable trap.   \\nSuppose that the following equation has been derived for predicting \\na target Y  \\n \\n𝑌= 𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚+ ε                       \\n \\nand that the first few features, X1, X2,….,Xk (k ≤ m), are dummy variables \\ncreated from the one-hot encoding of a particular categorical variable. \\nImagine what happens if we add a constant C to the bias, a, and subtract \\nC from each of the weights 𝑏1, 𝑏2, … , 𝑏𝑘.  From the nature of one-hot en-\\ncoding, it must be the case that Xj = 0 for 1 ≤ j ≤ k except for one particu-\\nlar value of j where Xj = 1.  As a result, subtracting C from each of b1, \\nb2,….,bk  reduces the estimated value of Y by exactly C. Adding C to a in-\\ncreases the estimated value of Y by C. The estimated value of Y is there-\\nfore the same when we make these two changes. This is true for any \\nvalue of C. \\nFortunately, regularization, which is designed to simplify models \\nand avoid over-fitting (see next section), has the side effect of dealing \\nwith the dummy variable trap problem by finding a single “best” regres-\\nsion equation. This is an equation where the magnitudes of the weights \\nare small. \\n \\n \\n3.4     Regularization \\n \\nIn machine learning there are often a large number of features, some \\nof which may be correlated with each other. This can lead to over-fitting \\n54 \\n                                                                   Chapter 3 \\n \\n \\n \\nand models that are unnecessarily complex. A common way of handling \\nthis is known as regularization. In the next three sections we introduce \\nthree regularization techniques and illustrate them with the data in Ta-\\nble 3.1. All calculations are in ANDY LANDU NGOMA  \\nBefore using regularization, it is important to carry out feature scal-\\ning to ensure that the numerical values of features are comparable. This \\nis described in Section 2.1. \\n \\n \\n3.5     Ridge Regression \\n \\nRidge regression (also referred to as Tikhonov regression) is a regu-\\nlarization technique where we change the function that is to be mini-\\nmized from that in equation (3.2) to:3 \\n \\n1\\n𝑛∑(𝑌𝑖−𝑎−𝑏1𝑋𝑖1 −𝑏2𝑋𝑖2 −⋯−𝑏𝑚𝑋𝑖𝑚)2\\n𝑛\\n𝑖=1\\n+  λ ∑𝑏𝑗\\n2       (3.3)\\n𝑚\\n𝑗=1\\n \\n                      \\n For each feature j, Ridge regression involves adding the term \\nλ𝑏𝑗\\n2 to the mean squared error.   (Note that we do not add a term corre-\\nsponding to the bias, a.) This change has the effect of encouraging the \\nmodel to keep the weights bj as small as possible. Ridge regression is \\nreferred to as L2 regularization. \\nConsider the situation where there are two highly correlated fea-\\ntures, X1 and X2, that have been scaled so that they have mean zero and \\nstandard deviation one. Suppose that the best fit linear model, obtained \\nby minimizing the objective function in equation (3.2), is \\n \\n𝑌= 𝑎+ 1000𝑋1 −980𝑋2 \\n \\nBecause the features are close substitutes, simpler models such as \\n \\n𝑌= 𝑎+ 𝑏𝑋1            or           𝑌= 𝑎+ 𝑏𝑋2 \\n                                                           \\n3 Alternative equivalent objective functions for Ridge regression are sometimes \\nused. The value of \\uf06c depends on the way the objective function is specified. \\nSklearn’s LinearRegression package for Python adds the sum of squared errors, \\nrather than the mean squared error, to λ ∑\\n𝑏𝑗\\n2.\\n𝑚\\n𝑗=1\\n This means that Sklearn’s \\uf06c should \\nbe n times the \\uf06c in equation (3.3). In Géron’s book Hands on machine learning with \\nScikit-Learn and TensorFlow, 1/n in equation (3.3) is replaced by 1/(2n). The value \\nof\\uf020\\uf06c used in this formulation should be half that in equation (3.3).  \\nLinear and Logistic Regression                                                                                             55 \\n \\n \\n \\nwhere b is about 20 are likely to generalize better. Ridge regression, \\nbecause it penalizes large positive or negative values of the coefficients, \\nwould find one of these models.  \\nThe Ridge regression model in equation (3.3) should only be used \\nfor determining model parameters using the training set. Once the \\nmodel parameters have been determined, equation (3.2) (i.e., the equa-\\ntion without the λ ∑\\n𝑏𝑗\\n2\\n𝑚\\n𝑗=1\\n term) should be used for prediction. A valida-\\ntion set should be used to test whether equation (3.2) generalizes well. \\nThe accuracy of the model that is finally chosen should be quantified \\nusing equation (3.2) on the test set. \\nThe parameter λ is referred to as a hyperparameter because it is \\nused to train the model, but is not part of the model that is used to pre-\\ndict Y.  The choice of a value for λ is obviously important. A large value \\nfor λ would lead to all the bj being set equal to zero. (The resulting mod-\\nel would then be uninteresting as it would always predict a value equal \\nto a for Y.) In practice, it is desirable to try several different values for λ \\nand see how well the resulting models generalize to a validation set.  \\nWe mentioned that the standard linear regression objective function \\nin equation (3.2) can be minimized analytically by using matrix algebra. \\nWe can also minimize the Ridge regression objective function in equa-\\ntion (3.3) analytically. The gradient descent method introduced in Sec-\\ntion 3.2 is an alternative.  \\nWe will use the model where we fitted a polynomial of degree five to \\nthe training data set in Table 3.1 as an illustration of regularization. We \\nknow from Chapter 1 that the model over-fits the training set. It will be \\ninstructive to see how regularization handles this.  \\nAs a first step, feature scaling is necessary. In this example we have \\nfive features. These are X, X2, X3, X4, and X5, where X is age in years. Each \\nof these must be scaled. (Note that it is not sufficient to scale only X.) We \\nwill use Z-score scaling.  Table 3.2 shows values of the features together \\nwith means and standard deviations. (The size of the numbers in the \\ntable emphasizes the importance of scaling.) Table 3.3 shows the scaled \\nfeatures.  \\nThe best fit linear regression when the features have the scaled val-\\nues in Table 3.3 and the salary Y is measured in $’000s is \\n \\n𝑌= 216.5 −32,622.6𝑋+ 135,402.7𝑋2 −215,493.1𝑋3            \\n+ 155,314.6𝑋4 −42,558.8𝑋5                                          (3.4) \\n \\n \\n \\n56 \\n                                                                   Chapter 3 \\n \\n \\n \\nTable 3.2   Feature values. X equals the age of individuals in the training \\nset \\n \\nInstance \\nX \\nX2 \\nX3 \\nX4 \\nX5 \\n1 \\n25 \\n625 \\n15,625 \\n390,625 \\n9,765,625 \\n2 \\n55 \\n3,025 \\n166,375 \\n9,150,625 \\n503,284,375 \\n3 \\n27 \\n729 \\n19,683 \\n531,441 \\n14,348,907 \\n4 \\n35 \\n1,225 \\n42,875 \\n1,500,625 \\n52,521,875 \\n5 \\n60 \\n3,600 \\n216,000 \\n12,960,000 \\n777,600,000 \\n6 \\n65 \\n4,225 \\n274,625 \\n17,850,625 \\n1,160,290,625 \\n7 \\n45 \\n2,025 \\n91,125 \\n4,100,625 \\n184,528,125 \\n8 \\n40 \\n1,600 \\n64,000 \\n2,560,000 \\n102,400,000 \\n9 \\n50 \\n2,500 \\n125,000 \\n6,250,000 \\n312,500,000 \\n10 \\n30 \\n900 \\n27,000 \\n810,000 \\n24,300,000 \\nMean \\n43.2 \\n2,045 \\n104,231 \\n5,610,457 \\n314,153,953 \\nS.D. \\n14.1 \\n1,259 \\n89,653 \\n5,975,341 \\n389,179,640 \\n \\nTable 3.3   Values of features in Table 3.2 after scaling \\n \\nInstance \\nX \\nX2 \\nX3 \\nX4 \\nX5 \\n1 \\n−1.290 \\n−1.128 \\n−0.988 \\n−0.874 \\n−0.782 \\n2 \\n  0.836 \\n   0.778 \\n   0.693    0.592 \\n   0.486 \\n3 \\n−1.148 \\n−1.046 \\n−0.943 \\n−0.850 \\n−0.770 \\n4 \\n−0.581 \\n−0.652 \\n−0.684 \\n−0.688 \\n−0.672 \\n5 \\n  1.191 \\n   1.235 \\n  1.247 \\n   1.230 \\n   1.191 \\n6 \\n  1.545 \\n   1.731 \\n   1.901    2.048 \\n   2.174 \\n7 \\n  0.128 \\n−0.016 \\n−0.146 \\n−0.253 \\n−0.333 \\n8 \\n−0.227 \\n−0.354 \\n−0.449 \\n−0.511 \\n−0.544 \\n9 \\n  0.482 \\n   0.361 \\n   0.232    0.107 \\n−0.004 \\n10 \\n−0.936 \\n−0.910 \\n−0.861 \\n−0.803 \\n−0.745 \\n \\nWe can now apply Ridge regression. Table 3.4 shows the bias, a, and \\nweights, bj, for two different values of λ.  Setting \\uf06c\\uf020= 0 would give the \\n“no-frills” regression result in equation (3.4). It can be seen that moving \\nfrom \\uf06c\\uf020= 0 to \\uf06c\\uf020= 0.02 has a dramatic effect on the weights, reducing \\nthem by several orders of magnitude. Increasing \\uf06c from 0.02 to 1.0 re-\\nduces the weights further. Figures 3.1 to 3.3 plot the forecasted salary \\nas a function of age for the predictions given by\\uf020\\uf06c = 0, 0.02, and 0.1. It \\ncan be seen that, as \\uf06c\\uf020increases, the model becomes less complex.  The \\n\\uf06c\\uf020= 0.02 model is very similar to the quadratic model (see Figure 1.4), \\nwhich we found in Chapter 1 generalizes well to new data.  \\nLinear and Logistic Regression                                                                                             57 \\n \\n \\n \\nTable 3.4   Variation of bias and weights for different values of \\uf06c\\uf020for \\nRidge regression. Salary is measured in $’000s (see Excel file for salary \\nvs. age example) \\n \\n\\uf06c\\uf020\\na \\nb1 \\nb2 \\nb3 \\nb4 \\nb5 \\n0.02 \\n216.5 \\n97.8 \\n36.6 \\n−8.5 \\n  −35.0 \\n−44.6 \\n0.10 \\n216.5 \\n56.5 \\n28.1 \\n   3.7 \\n  −15.1 \\n−28.4 \\n \\n \\nFigure 3.1   Prediction of salary ($’000s), no regularization (\\uf06c\\uf020= 0)  \\n \\n \\n \\n \\nFigure 3.2   Prediction of salary ($’000s) when Ridge regression is used \\nwith \\uf06c\\uf020= 0.02 \\n \\n \\n \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary ($)\\nAge (years)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary (\\'000s)\\nAge (years)\\n58 \\n                                                                   Chapter 3 \\n \\n \\n \\nFigure 3.3   Prediction of salary when Ridge regression is used with \\uf06c\\uf020= \\n0.1 \\n \\n \\n \\n \\n3.6     Lasso Regression \\n \\nLasso is short for “Least absolute shrinkage and selection operator.” \\nIn Ridge, we added a constant times the sum of the squared weights to \\nthe objective function. In Lasso, we add a constant times the sum of the \\nabsolute weights. This gives the following objective function: 4 \\n \\n1\\n𝑛∑(𝑌𝑖−𝑎−𝑏1𝑋𝑖1 −𝑏2𝑋𝑖2 −⋯−𝑏𝑚𝑋𝑖𝑚)2\\n𝑛\\n𝑖=1\\n+  λ ∑|𝑏𝑗|    (3.5)\\n𝑚\\n𝑗=1\\n \\n \\nThis function cannot be minimized analytically and so an approach \\nsimilar to the gradient descent algorithm explained earlier must be \\nused. Lasso regression is referred to as L1 regularization. \\nWe saw in the previous section that Ridge regression reduces the \\nweights assigned to features in order to simplify the model. The simpli-\\nfied model often generalizes better than the unregularized model. Lasso \\nregression also has the effect of simplifying the model. It does this by \\nsetting the weights of unimportant features to zero.  When there are a \\nlarge number of features, Lasso can identify a relatively small subset of \\nthem to form a good prediction model. \\n \\n                                                           \\n4 There are variations in the way the objective function is specified. Sklearn’s Line-\\narRegression package for Python replaces 1/n by 1/(2n) in equation (3.5). This \\nmeans that Sklearn’s\\uf020\\uf06c should be half as much as the \\uf06c\\uf020in equation (3.5).  \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary (\\'000s)\\nAge (years)\\nLinear and Logistic Regression                                                                                             59 \\n \\n \\n \\nThe results of using Lasso for our example where a fifth order poly-\\nnomial is used to predict the salary of an individual from the individu-\\nal’s age is shown in Table 3.5.  The table shows that Lasso does indeed \\nset the weights of some features equal to zero.  We might expect that b5, \\nb4, and possibly b3 will be zero so that the model reduces to a quadratic \\nor cubic model. In fact, this is not what happens. When \\uf06c\\uf020= 0.02, Lasso \\nonly reduces b3 to zero; when \\uf06c\\uf020= 0.1, Lasso reduces b2 and b4 to zero; \\nwhen \\uf06c\\uf020= 1, Lasso reduces b2, b3, and b5 to zero.  \\nFigures 3.4, 3.5 and 3.6 show the predictive models that are created \\nfor \\uf06c\\uf020= 0.02, 0.1, and 1.  They are simpler than the fifth-degree polyno-\\nmial model in equation (3.4) with much lower weights.  As in the case of \\nRidge regression, the models become simpler as \\uf06c is increased. The \\nmodel in Figure 3.6 is very similar to the quadratic model in Figure 1.4. \\n \\nTable 3.5   Variation of bias and weights for different values of \\uf06c\\uf020for \\nLasso regression\\uf02e\\uf020Salary is measured in $’000s (see Excel file for salary \\nvs. age example) \\n \\n\\uf06c\\uf020\\na \\nb1 \\nb2 \\nb3 \\nb4 \\nb5 \\n0.02 \\n216.5 \\n−646.4 \\n2,046.6 \\n    0 \\n−3,351.0 \\n2,007.9 \\n  0.10 \\n216.5 \\n  355.4 \\n0 \\n−494.8 \\n   0 \\n   196.5 \\n  1.00 \\n216.5 \\n  147.4 \\n0 \\n    0 \\n      −99.3 \\n0 \\n \\n \\nFigure 3.4   Prediction of salary when Lasso regression is used with \\uf06c\\uf020= \\n0.02. Salary is measured in $’000s. \\n \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary (\\'000s)\\nAge (years)\\n60 \\n                                                                   Chapter 3 \\n \\n \\n \\nFigure 3.5   Prediction of salary when Lasso regression is used with λ = \\n0.1. Salary is measured in $’000s. \\n   \\n \\n \\nFigure 3.6   Prediction of salary when Lasso regression is used with \\uf06c\\uf020= \\n1. Salary is measured in $’000s. \\n \\n \\n \\n3.7     Elastic Net Regression \\n \\nElastic Net regression is a mixture of Ridge and Lasso.  The function \\nto be minimized includes both a constant times the sum of the squared \\nweights and a different constant time the sum of the absolute values of \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary (\\'000s)\\nAge (years)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary (\\'000s)\\nAge (years)\\nLinear and Logistic Regression                                                                                             61 \\n \\n \\n \\nthe weights. It is  \\n \\n1\\n𝑛∑(𝑌𝑖−𝑎−𝑏1𝑋1𝑖−𝑏2𝑋2𝑖−⋯−𝑏𝑚𝑋𝑚𝑖)2\\n𝑛\\n𝑖=1\\n+ λ1 ∑𝑏𝑗\\n2\\n𝑚\\n𝑗=1\\n+ λ2 ∑|𝑏𝑗|\\n𝑚\\n𝑗=1\\n \\n \\nIn Lasso some weights are reduced to zero, but others may be quite \\nlarge. In Ridge, weights are small in magnitude, but they are not re-\\nduced to zero. The idea underlying Elastic Net is that we may be able to \\nget the best of both worlds by making some weights zero while reduc-\\ning the magnitude of the others. We illustrate this for our fifth order \\npolynomial example by setting λ1 = 0.02 and λ2 = 1. The resulting model \\nfor predicting salary ($’000s), Y, from age (years), X, is  \\n \\n𝑌= 216.5 + 96.7𝑋+ 21.1𝑋2 −26.0𝑋4 −45.5𝑋5 \\n \\nThis has a similar structure to the Lasso model when λ = 0.02 (see Table \\n3.5), except that the non-zero weights are much smaller. The model is \\nshown in Figure 3.7. Again, we find that the result is very similar to the \\nquadratic model which was developed in Chapter 1 for this example. \\n \\n \\nFigure 3.7   Prediction of salary when Elastic Net regression is used \\nwith \\uf06c\\uf031\\uf020= 0.2 and \\uf06c\\uf032\\uf020= 1.0 (see Excel file for salary vs. age example)  \\n \\n \\n \\n \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n20\\n30\\n40\\n50\\n60\\n70\\nSalary (\\'000s)\\nAge (years)\\n62 \\n                                                                   Chapter 3 \\n \\n \\n \\n3.8     Results for House Price Data \\n \\nSo far, we have explained regularization with a baby data set of only \\nten observations. We now show how it can be used in a more realistic \\nsituation.  \\nLocal authorities in many countries need to predict the market pric-\\nes of houses to determine property taxes. They can do this by relating \\nthe known prices of houses that have been sold to features such as the \\nnumber of bedrooms, number of bathrooms, and the neighborhood in \\nwhich the house is located.  The data set we will use consists of infor-\\nmation on houses that were sold in Iowa during a four-year period.5  \\nBefore starting we emphasize the importance of the material in \\nChapter 1 concerning the need to divide all available data into three \\nparts: a training set, a validation set, and a test set. The training set is \\nused to determine parameters for trial models. The validation set is \\nused to determine the extent to which the models created from the \\ntraining set generalize to new data. The test set is used as a final esti-\\nmate of the accuracy of the chosen model.  After data cleaning, we had \\n2,908 observations. We split this as follows: 1,800 in the training set, \\n600 in the validation set and 508 in the test set.   \\nThe full data set contains a total of about 80 features, some numeri-\\ncal and some categorical. To illustrate the regression techniques dis-\\ncussed in this chapter, we will use a total of 23 features. These are listed \\nin Table 3.6.  Twenty-one are numerical and two are categorical. One of \\nthe categorical features is concerned with the basement quality, as indi-\\ncated by the ceiling height. The categories are:  \\n \\n\\uf0b7 \\nExcellent (>100 inches) \\n\\uf0b7 \\nGood (90 to 99 inches)  \\n\\uf0b7 \\nTypical  (80 to 89 inches) \\n\\uf0b7 \\nFair (70 to 79 inches) \\n\\uf0b7 \\nPoor (< 70 inches) \\n\\uf0b7 \\nNo basement \\n \\nThis is an example of a categorical variable where there is a natural or-\\ndering. We created a new variable that had values of 5, 4, 3, 2, 1, and 0 \\nfor the six categories, respectively. \\nThe other categorical feature specifies the location of the house as in \\none of 25 neighborhoods. Given the real estate agents’ mantra “location, \\n                                                           \\n5 This data set was used in a Kaggle competition where contestants tried to predict \\nprices for test data. \\nLinear and Logistic Regression                                                                                             63 \\n \\n \\n \\nlocation, location” we felt it important to include this feature. We there-\\nfore introduced 25 dummy variables. The dummy variable equals one \\nfor an observation if the neighborhood is that in which the house is lo-\\ncated and zero otherwise.  The total number of features in the model \\nwas therefore 47 (21 numerical features, 1 for basement quality and 25 \\nfor location).  \\n \\nTable 3.6   Features for estimating house prices and weights using a \\nlinear regression model on scaled data with no regularization (from \\nPython) \\n \\nFeature \\nWeights for simple linear \\nregression \\nLot area (square feet) \\n0.08 \\nOverall quality (scale: 1 to 10) \\n0.21 \\nOverall condition (scale: 1 to 10)  \\n0.10 \\nYear built \\n0.16 \\nYear remodeled (= year built if no \\nremodeling or additions) \\n0.03 \\nFinished basement (square feet) \\n0.09 \\nUnfinished basement (square feet) \\n                    −0.03 \\nTotal basement (square feet) \\n0.14 \\nFirst floor (square feet) \\n0.15 \\nSecond floor (square feet) \\n0.13 \\nLiving area (square feet) \\n0.16 \\nNumber of full bathrooms \\n                    −0.02 \\nNumber of half bathrooms \\n0.02 \\nNumber of bedrooms \\n                    −0.08 \\nTotal rooms above grade \\n0.08 \\nNumber of fireplaces \\n0.03 \\nParking spaces in garage \\n0.04 \\nGarage area (square feet) \\n0.05 \\nWood deck (square feet) \\n0.02 \\nOpen porch (square feet) \\n0.03 \\nEnclosed porch (square feet) \\n0.01 \\nNeighborhood (25 features) \\n−0.05 to 0.12 \\nBasement quality \\n0.01 \\n \\n \\nThere are relationships between the features in Table 3.6. For exam-\\nple, the total basement area is the sum of the finished and unfinished \\nareas. Features such as living area, number of bedrooms, and number of \\n64 \\n                                                                   Chapter 3 \\n \\n \\n \\nbathrooms are related to the size of the house and are therefore likely \\nto be correlated.  These are the sort of issues that Ridge and Lasso re-\\ngression can deal with. \\nThe features and dummy variables were scaled using the Z-score \\nmethod and the training set data. We also scaled the target values (i.e., \\nthe house prices) using the Z-score method and train set observations. \\n(The latter is not necessary but will prove useful.)  \\nWhen a plain vanilla linear regression is used, the results are those \\nshown in Table 3.6.  The mean squared error for the prediction of the \\nprices of houses in the training set is 0.114.  Since observations on the \\nprice have been scaled so that they have a variance of 1, this means that \\n1−0.114 or 88.6% of the variance of house prices in the training set is \\nexplained by the regression model.  \\nFor the data we are considering, it turns out that this regression \\nmodel generalizes well. The mean squared error for the validation set \\nwas only a little higher than that for the training set at 0.117.  However, \\nlinear regression with no regularization leads to some strange results \\nbecause of the correlations between features. For example, it makes no \\nsense that the weights for number of full bathrooms and number of \\nbedrooms are negative.  \\nWe tried using Ridge regression with different values of the hyper-\\nparameter, \\uf06c. The impact of this on the prediction errors for the valida-\\ntion set is shown in Figure 3.8. As expected, the prediction error in-\\ncreases as \\uf06c increases. Values of \\uf06c in the range 0 to 0.1 might reasona-\\nbly be considered because the increase in prediction errors is small \\nwhen \\uf06c is in this range.  However, it turns out that the improvement in \\nthe model is quite small for these values of \\uf06c. The average absolute val-\\nue of the weights decreases from about 0.049 to about 0.046 as \\uf06c in-\\ncreases from 0 to 0.1.  Even when \\uf06c is increased to 0.6 the average abso-\\nlute value of the weights declines to only 0.039. \\nLasso regression leads to more interesting results. Figure 3.9 shows \\nhow the error in the validation set changes as the value of the Lasso’s \\uf06c \\nincreases from an initial value of zero. For small values of \\uf06c the error is \\nactually less than when \\uf06c\\uf020= 0, but as \\uf06c increases beyond about 0.02 the \\nerror starts to increase. A value of \\uf06c equal to 0.04 could be attractive. \\nThe loss in accuracy is quite small. The mean squared error for the vali-\\ndation set is only 12.0% of the total variance in the observations (com-\\npared with 11.7% when \\uf06c is set equal to 0 so that there is no regulariza-\\ntion). However, when \\uf06c=0.04, 25 of the weights are zero and the aver-\\nage value of the weights is reduced to 0.034. (This is much better than \\nthe corresponding results using Ridge.) \\nLinear and Logistic Regression                                                                                             65 \\n \\n \\n \\nFigure 3.8   Ridge regression results showing the mean squared error \\nas a percent of the total squared error in the observations for the vali-\\ndation set (from Excel and Python) \\n \\n \\n \\nFigure 3.9   Lasso results for validation set for different values of \\n\\uf06c\\uf020(from Python) \\n \\n \\nIf we are prepared to let the percentage mean squared error rise to \\nabout 14%, we can set \\uf06c equal to 0.1. This results in 30 of the weights \\nbecoming zero. The remaining non-zero weights are shown in Table 3.7. \\n(The weights for wood deck and open porch were less than 0.005, but \\n11.0%\\n11.5%\\n12.0%\\n12.5%\\n13.0%\\n13.5%\\n14.0%\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nVariance Unexplained\\n\\uf06c\\n11.0%\\n11.5%\\n12.0%\\n12.5%\\n13.0%\\n13.5%\\n14.0%\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\nVariance Unexplained\\n\\uf06c\\n66 \\n                                                                   Chapter 3 \\n \\n \\n \\nnot quite zero and are not shown in the table.) In Tables 3.6 and 3.7 it \\ncan be seen that overall quality and total living area are the most im-\\nportant predictors. A key point is that the negative weights in Table 3.6 \\nthat made no sense have been eliminated.  \\n \\nTable 3.7   Non-zero features in the Lasso model and their weights (af-\\nter scaling) when \\uf06c=0.1 (from Python) \\n \\nFeature \\nWeight \\nLot area (square feet) \\n0.04 \\nOverall quality (scale from 1 to 10) \\n0.30 \\nYear built \\n0.05 \\nYear remodeled  \\n0.06 \\nFinished basement (square feet) \\n0.12 \\nTotal basement (square feet) \\n0.10 \\nFirst floor (square feet) \\n0.03 \\nLiving area (square feet) \\n0.30 \\nNumber of fireplaces \\n0.02 \\nParking spaces in garage \\n0.03 \\nGarage area (square feet) \\n0.07 \\nNeighborhoods (3 out of 25 non-zero) \\n0.01, 0.02, and 0.08 \\nBasement quality \\n0.02 \\n \\nFor the data we are considering, Elastic Net did not produce an im-\\nprovement over Lasso.  It is therefore likely that an analyst would in \\nthis case opt for one of the Lasso models. Once the model has been cho-\\nsen its accuracy should be assessed using the test set. If Lasso with \\n\\uf06c=0.04 is chosen, the mean squared error for the test data set is 12.5% \\n(so that 87.5% of the variance in house prices is explained). If Lasso \\nwith \\uf06c=0.1 is chosen, the mean squared error for the test data set is \\n14.7% (so that 85.3% of the variance in house prices is explained). \\n \\n \\n3.9     Logistic Regression \\n \\nAs mentioned in Chapter 1 there are two types of supervised learn-\\ning models: those that are used to predict a numerical variable and \\nthose that are used for classification. Up to now in this chapter, we have \\nconsidered the problem of predicting a numerical variable. We now \\nmove on to the classification problem; that is, the problem of predicting \\nwhich of two categories new observations will belong to. Logistic re-\\nLinear and Logistic Regression                                                                                             67 \\n \\n \\n \\ngression is one of the tools that can be used for this. Other tools will be \\npresented in Chapters 4, 5, and 6.   \\nSuppose that there are a number of features Xj (1 ≤ j≤ m), some of \\nwhich may be dummy variables created from categorical features. Sup-\\npose further that there are two classes to which the observations can \\nbelong.  One of the classes will be referred to as a positive outcome \\n(typically this will be the thing we are trying to predict). The other class \\nwill be referred to as a negative outcome. An example of classification is \\nthe detection of junk e-mail from words included in the e-mail. A junk e-\\nmail would be classified as a positive outcome and a non-junk e-mail as \\na negative outcome. \\nLogistic regression (also called logit regression) can be used to cal-\\nculate the probability, Q, of a positive outcome. It does this by using the \\nsigmoid function: \\n \\n𝑄=\\n1\\n1 + 𝑒−𝑌                                               (3.6) \\n \\nThis is the S-shaped function shown in Figure 3.10.  It has values be-\\ntween 0 and 1. When Y is very large and negative, 𝑒−𝑌 is very large and \\nthe function Q is close to zero. When Y is very large and positive, 𝑒−𝑌 is \\nvery small and Q is close to one. \\n \\nFigure 3.10   The sigmoid function \\n \\n \\nWe set Y equal to a constant (the bias) plus a linear combination of \\nthe features: \\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n-10\\n-5\\n0\\n5\\n10\\nY\\nQ\\n68 \\n                                                                   Chapter 3 \\n \\n \\n \\n𝑌= 𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚 \\n \\nso that the probability of a positive outcome is  \\n \\n𝑄=\\n1\\n1 + exp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n)                              (3.7) \\n \\nWhen we are predicting the value of a target, as in the Iowa house \\nprice example, we can use mean square error as the objective function. \\nWhen classifying observations, a different objective function is neces-\\nsary. The maximum likelihood method in statistics is a way of choosing \\nparameters from a set of observations in a way that maximizes the \\nchance of the observations occurring.  In the present situation, it leads \\nto choosing the a and bj so that \\n \\n  \\n∑\\nln(𝑄) +\\n∑\\nln (1 −𝑄)\\nNegative \\nOutcomes\\nPositive \\nOutcomes\\n                            (3.8) \\n \\nis maximized. The first summation here is over all the observations \\nwhich led to positive outcomes and the second summation is over all \\nobservations which led to negative outcomes. This function cannot be \\nmaximized analytically and gradient ascent (similar to gradient de-\\nscent) methods must be used. \\nEarlier in this chapter we showed how regularization can simplify \\nlinear regression models and avoid overfitting. Regularization can be \\nused in a similar way in logistic regression. Ridge regression (L2 regu-\\nlarization) involves adding 𝜆∑\\n𝑏𝑗\\n2\\n𝑚\\n𝑗=1\\n to the expression for Y in equation \\n(3.6) so that the probability of a positive outcome becomes \\n \\n𝑄=\\n1\\n1 + exp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n−λ ∑\\n𝑏𝑗\\n2\\n𝑚\\n𝑗=1\\n)     \\n \\nSimilarly, Lasso regression (L1 regularization) leads to \\n \\n𝑄=\\n1\\n1 + exp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n−λ ∑\\n|𝑏𝑗|\\n𝑚\\n𝑗=1\\n)     \\n \\nand Elastic Net leads to  \\nLinear and Logistic Regression                                                                                             69 \\n \\n \\n \\n𝑄=\\n1\\n1 + exp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n−𝜆1 ∑\\n𝑏𝑗\\n2\\n𝑚\\n𝑗=1\\n−λ2 ∑\\n|𝑏𝑗|\\n𝑚\\n𝑗=1\\n)     \\n \\nWe emphasize that these formulas are used for estimating the bias \\nand weights from the training set. Once this is done, equation (3.7) is \\nused for predicting Q from the validation set, test set, or new data.   \\n \\n \\n3.10    Decision Criteria \\n \\nOnce the model for estimating the probability of a positive outcome \\nhas been developed, it is necessary to choose a criterion for deciding \\nwhether a new observation should be classified as positive.  It is tempt-\\ning to maximize a simple accuracy measure: the percentage of observa-\\ntions that are correctly classified. But this does not always work well. \\nSuppose we are trying to detect credit card fraud from features such as \\nnumber of charges per day, types of purchases, and so on. If only one \\npercent of transactions are fraudulent, we can obtain 99% accuracy \\nsimply by forecasting that all transactions are good! \\nThe problem here is that there is a class imbalance. There are two \\nclasses: \\n \\n\\uf0b7 transaction good \\n\\uf0b7 transaction fraudulent \\n \\nand the first class is much bigger than the second. If the classes were \\nequal in size (or approximately equal in size), using the accuracy meas-\\nure we have just mentioned could be appropriate. Unfortunately, most \\nof the time the classes we deal with are imbalanced.  \\nOne way of handling this problem is to create a balanced training set \\nby under-sampling observations from the majority class. For example, \\nin the situation we have just mentioned an analyst could form a training \\nset by collecting data on 100,000 fraudulent transactions and pairing it \\nwith a random sample of 100,000 good transactions. Another approach \\ninvolves over-sampling the minority class by creating synthetic obser-\\nvations. This is known as SMOTE (Synthetic Minority Over-sampling \\nTechnique).6  Balancing the training set in one of these ways is not nec-\\nessary for logistic regression but does make methods we will talk about \\nlater in this book such as SVM and neural networks, work better.   \\n                                                           \\n6 See N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthet-\\nic Minority Over-Sampling Technique,” Journal of Artificial Intelligence Research, 16 \\n(2002), 321–357. \\n70 \\n                                                                   Chapter 3 \\n \\n \\n \\nIn practice, a balanced training set is often not used in logistic re-\\ngression. It is important to keep in mind the purpose of the classifica-\\ntion. Often the cost of classifying a new observation as positive when in \\nfact it turns out to be negative is different from the cost of classifying it \\nas negative when in fact it turns out to be positive. It is then a mistake to \\nbase decisions on whether Q is greater than or less than 0.5 and a \\nthreshold for Q that is different from 0.5 is likely to be appropriate. As \\nwe will explain with the example in the next section, it can be useful to \\npresent the decision maker with a range of alternative decision criteria.  \\n \\n \\n3.11     Application to Credit Decisions \\n \\nIn this section, we consider a subset of the data provided by the \\ncompany Lending Club on the performance of its loans. (This data and \\nthe analysis are at ANDY LANDU NGOMA Lending Club is a \\npeer-to-peer lender that allows investors to lend money to borrowers \\nwithout an intermediary being involved.7  \\nLending Club uses machine learning. We will attempt the challenging \\ntask of trying to improve on Lending Club’s criteria by using machine \\nlearning ourselves. An extract from the data we use is shown in Table \\n3.8. In this example, we will be looking at only one model. It will there-\\nfore be sufficient to use a training set and a test set. (The validation set, \\nit will be recalled, is necessary when several models are considered, and \\nthe analyst must choose between them.)  \\nIn the analyses given here and elsewhere in this book, good loans are \\ndefined as those listed as “Current” and defaulting loans as those listed \\nas “Charged Off”.8 We define positive outcomes as those that lead to \\ngood loans and negative outcomes as those that lead to defaults. This is \\nsomewhat arbitrary. Some analysts would argue that we are trying to \\npredict defaults and so defaults should be the positive outcome.9  \\nThe training set consists of 8,695 observations of which 1,499 were \\nfor loans that defaulted and the remaining 7,196 were for loans that \\nproved to be good. The test set consists of 5,916 observations of which \\n1,058 were for loans that defaulted and the remaining 4,858 were for \\nloans that were good. \\n                                                           \\n7 See ANDY LANDU NGOMA \\n8 Exercise 3.16 suggests an alternative (possibly better) classification where good \\nloan as defined as “Fully Paid.” \\n9 See Exercise 3.14 for how the analysis changes when defaulting loans are labeled \\n“positive.”  \\nLinear and Logistic Regression                                                                                             71 \\n \\n \\n \\nWe use four features: \\n\\uf0b7 \\nHome ownership \\n\\uf0b7 \\nIncome per annum ($) \\n\\uf0b7 \\nDebt to income ratio (%) \\n\\uf0b7 \\nCredit score (FICO) \\n \\n(One of these, home ownership, was categorical and was handled with a \\ndummy variable that was 0 or 1.) Table 3.8 shows sample data. The \\nweights estimated for the training set are shown in Table 3.9. The bias \\nwas estimated as −6.5645. The probability of a loan not defaulting is \\ntherefore given by equation (3.6) with \\n \\n𝑌= −6.5645 + 0.1395𝑋1 + 0.0041𝑋2 −0.0011𝑋3 + 0.0113𝑋4 \\n \\nTable 3.8   Training data set used to predict loan defaults \\n \\nHome ownership,  \\n1=owns, 0=rents \\nX1 \\nIncome \\n(‘000s), \\nX2 \\nDebt to \\nincome  \\nX3 \\nCredit \\nscore, \\nX4 \\nLoan out-\\ncome  \\n1 \\n44.304 \\n18.47 \\n690 \\nDefault \\n1 \\n136.000 \\n20.63 \\n670 \\nGood \\n0 \\n38.500 \\n33.73 \\n660 \\nDefault \\n1 \\n88.000 \\n5.32 \\n660 \\nGood \\n…… \\n…… \\n…… \\n…… \\n…… \\n…… \\n…… \\n…… \\n…… \\n…… \\n \\n \\nTable 3.9    Optimal weights (see Excel or Python) \\n \\nFeature \\nSymbol \\nWeight, bi \\nHome ownership (0 or 1) \\nX1 \\n  0.1395 \\nIncome (‘000s) \\nX2 \\n  0.0041 \\nDebt to income ratio (%) \\nX3 \\n−0.0011 \\nCredit score (FICO) \\nX4 \\n  0.0113 \\n \\nThe decision for determining whether a loan is acceptable or not can \\nbe made by setting a threshold, Z, for the value of Q so that: \\n \\n\\uf0b7 If Q ≥ Z the loan is predicted to be good \\n\\uf0b7 If Q < Z the loan is predicted to be bad \\n \\n72 \\n                                                                   Chapter 3 \\n \\n \\n \\nThe results when a particular value of Z is applied to the test set can be \\nsummarized by what is referred to as a confusion matrix. This shows the \\nrelationship between predictions and outcomes.  Tables 3.10, 3.11, and \\n3.12 show the confusion matrix for three different values of Z for the \\ntest set in our model (see Excel or Python results). \\n \\n \\nTable 3.10   Confusion matrix for test set when Z = 0.75 \\n \\n \\nPredict positive \\n(no default) \\nPredict negative \\n(default) \\nOutcome positive \\n(no default) \\n77.59% \\n4.53% \\nOutcome negative \\n(default) \\n16.26% \\n1.62% \\n \\n \\nTable 3.11    Confusion matrix for test set when Z = 0.80 \\n \\n \\nPredict positive (no \\ndefault) \\nPredict negative \\n(default) \\nOutcome positive \\n(no default) \\n55.34% \\n26.77% \\nOutcome negative \\n(default) \\n9.75% \\n8.13% \\n \\n \\nTable 3.12   Confusion matrix for test set when Z = 0.85 \\n \\n \\nPredict positive \\n(no default) \\nPredict  negative \\n(default) \\nOutcome positive (no \\ndefault) \\n28.65% \\n53.47% \\nOutcome negative \\n(default) \\n3.74% \\n14.15% \\n \\nThe confusion matrix itself is not confusing but the terminology that \\naccompanies it can be.  The four elements of the confusion matrix are \\ndefined as follows: \\n \\n\\uf0b7 True Positive (TP): Both prediction and outcome are positive \\nLinear and Logistic Regression                                                                                             73 \\n \\n \\n \\n\\uf0b7 False Negative (FN): Prediction is negative, but outcome is posi-\\ntive  \\n\\uf0b7 False Positive (FP): Prediction is positive and outcome is negative \\n\\uf0b7 True Negative (TN): Prediction is negative and outcome is nega-\\ntive \\n \\nThese definitions are summarized in Table 3.13 \\n \\nTable 3.13   Summary of definitions \\n \\n \\nPredict positive out-\\ncome \\nPredict negative \\noutcome \\nOutcome positive \\nTP \\nFN \\nOutcome negative \\nFP \\nTN \\n \\nRatios that can be defined from the table are: \\n \\n  Accuracy = \\nTP + TN\\nTP + FN + FP + TN \\nTrue Positive Rate = \\nTP\\nTP + FN                                     \\n       True Negative Rate = \\nTN\\nTN + FP                                             \\n       False Positive Rate = \\nFP\\nTN + FP                                           \\nPrecision = \\nTP\\nTP + FP                 \\n \\n \\nAnother measure sometimes calculated from these ratios is known as \\nthe F-score or F1-score. This is defined as \\n \\n2 × P × TPR\\nP + TPR \\n \\nwhere P is the precision and TPR is the true positive rate. This is an ac-\\ncuracy measure sometimes used for imbalanced data sets that focuses \\non how well positives have been identified.  \\nThe measures are shown in Table 3.14 for the three different values \\nof Z that are considered in Tables 3.10 to 3.12.  \\n74 \\n                                                                   Chapter 3 \\n \\n \\n \\nTable 3.14   Ratios calculated from the confusion matrices in Tables \\n3.10 to 3.12 (see Excel or Python results) \\n \\n \\nZ = 0.75 \\nZ = 0.80 \\nZ = 0.85 \\nAccuracy \\n79.21% \\n63.47% \\n42.80% \\nTrue Positive Rate \\n94.48% \\n67.39% \\n34.89% \\nTrue Negative Rate \\n9.07% \\n45.46% \\n79.11% \\nFalse Positive Rate \\n90.93% \\n54.54% \\n20.89% \\nPrecision \\n82.67% \\n85.02% \\n88.47% \\nF-score \\n    88.18% \\n75.19% \\n50.04% \\n \\nAccuracy is the percentage of observations that are classified cor-\\nrectly. It might be thought that maximizing accuracy must be the best \\nstrategy. But, as mentioned in the previous section, this is not necessari-\\nly the case. Indeed, in our example accuracy is maximized at 82.12% by \\nsimply classifying all observations as positive (i.e. always predicting no \\ndefault and setting Z=0). \\nThe true positive rate, which is also called the sensitivity or the recall \\nis the percentage of positive outcomes that are correctly predicted.  Like \\naccuracy, this should not be the sole objective because it can be made \\none by classifying all observations as good.  \\nThe true negative rate, which is also called the specificity, is the pro-\\nportion of negative outcomes that were predicted as negative.  The false \\npositive rate is one minus the true negative rate. It is the proportion of \\nnegative outcomes that were incorrectly classified. The precision is the \\nproportion of positive predictions that prove to be correct.  \\nThere are a number of trade-offs. We can increase the true negative \\nrate (i.e., identify a greater proportion of the loans that will default) on-\\nly if we identify a lower proportion of the loans that prove to be good. \\nAlso, accuracy declines as the true negative rate increases.  \\nThe trade-offs are summarized in Figure 3.11 which plots the true \\npositive rate against the false positive rate for every possible threshold, \\nZ. This is known as the receiver operating curve (ROC).  The area under \\nthis curve (AUC) is a popular way of summarizing the predictive ability \\nof a model. If the AUC is 1.0, there is a perfect model where a 100% true \\npositive rate can be combined with a 0% false positive rate.  The dashed \\nline in Figure 3.11 corresponds to an AUC of 0.5. This corresponds to \\nmodels with no predictive ability.   A model that makes random predic-\\ntions would have an AUC of 0.5. Models with AUC < 0.5 are worse than \\nrandom.  \\n \\nLinear and Logistic Regression                                                                                             75 \\n \\n \\n \\nFigure 3.11   ROC curve showing the relationship between true positive \\nrate and false positive rate for test set (see Excel or Python files) \\n \\n \\nFor the data we have been considering, the Python implementation \\ncalculates an AUC of 0.6020 indicating that the model does has some \\nsmall predictive ability.  (Given that Lending Club has already used ma-\\nchine learning to make its lending decisions and we are using only four \\nfeatures, the AUC is quite encouraging.) \\nNote that, if we had set default as the positive outcome (perhaps be-\\ncause this is what we are trying to predict) and no-default as the nega-\\ntive outcome, the probabilities estimated for default and no-default \\nwould have been the same.  The accuracy ratio would be the same, but \\nthe other ratios in Table 3.14 would change. The AUC would be the \\nsame. See Exercise 3.14 to understand this.  \\nIn deciding on the appropriate value for Z (i.e., positioning on the \\nROC) a lender has to consider the average profit from loans that do not \\ndefault and the average loss from loans that default. Suppose for exam-\\nple that the profit from a loan that does not default is V, whereas the \\nloss from cost of a defaulting loan is 4V. The lender’s profit is greatest \\nwhen \\n \\n𝑉× TP −4𝑉× FP \\n \\nis maximized. For the alternatives considered in Tables 3.10, 3.11, and \\n3.12 this is 12.55V, 16.34V, and 13.69V, respectively. This indicates that \\nof the three alternative Z values, Z = 0.80 would be most profitable.  \\n \\n \\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nTrue Positive Rate\\nFalse Positive Rate\\n76 \\n                                                                   Chapter 3 \\n \\n \\n \\n3.12   The k-Nearest Neighbors Algorithm \\n \\nBefore closing this chapter, we mention a simple alternative to linear \\nor logistic regression known as the k-nearest neighbors algorithm. This \\ninvolves choosing a value for k and then finding the k observations \\nwhose features are most similar to the features from which we are mak-\\ning a prediction. \\nSuppose we are predicting the value of a house in a certain neigh-\\nborhood from lot size and square feet of living area. We could set k = 3.  \\nWe would then search for the three houses in our training set that are \\nmost similar to the house under consideration as far as lot size and liv-\\ning area are concerned.  We could measure similarity by scaling the fea-\\ntures and then using the Euclidean distance measure described in Chap-\\nter 2.  Suppose that the prices of the three most similar houses are \\n$230,000, $245,000, and $218,000. The estimated value of the house \\nwould be set equal to the arithmetic average of these house prices, or \\n$231,000. \\nThe algorithm can also be used for classification. Suppose that we \\nare predicting whether a loan will be good from the four features in Ta-\\nble 3.8 and set k = 10. We would search for the 10 loans in our training \\nset whose features are most similar to the loan under consideration. If \\neight of those loans proved to be good and two defaulted our prediction \\nfor the probability of no default would be 80%.   \\n \\n \\nSummary \\n \\nLinear regression is of course not a new machine learning technique. \\nIt has played a central role in empirical research for many years. Data \\nscientists have adopted it as a predictive tool. \\nMachine learning applications often have many features some of \\nwhich are highly correlated. Linear regression is then liable to produce \\na result which gives a large positive coefficient to values for one feature \\nand a large negative coefficient to values for another correlated feature. \\nWe illustrated this with the prediction-of-salary example that was con-\\nsidered in Chapter 1.  \\nOne approach to reducing the magnitude of weights in a regression \\nmodel is Ridge regression. Another is Lasso regression. The latter has \\nthe effect of reducing the weights of unimportant variables to zero. \\nElastic Net regression uses the ideas underlying both Ridge and Lasso \\nregression and can be used to achieve the advantages of both (i.e., coef-\\nLinear and Logistic Regression                                                                                             77 \\n \\n \\n \\nficients that are smaller in magnitude and the elimination of unim-\\nportant variables). \\nCategorical variables can be accommodated in linear regression by \\ncreating dummy variables, one for each category. The dummy variable \\nfor an observation is set equal to one if the observation falls into the \\ncategory and zero otherwise.  \\nLogistic regression, like regular linear regression, has been used in \\nempirical research for many years. It has become an important classifi-\\ncation tool for data scientists. Typically, there are two classes. One is \\ndesignated as “positive”; the other is designated as “negative.” The S-\\nshaped sigmoid function is used to define the probability of an observa-\\ntion falling into the positive class. An iterative search procedure is used \\nto find a linear function of the feature values that when substituted into \\nthe sigmoid function does the best job in assigning a high probability to \\npositive outcomes and a low probability to negative outcomes.  The re-\\nsults of using logistic regression on the test data set can be summarized \\nwith what is termed a confusion matrix.  \\nOnce a logistic regression has been carried out it is necessary to de-\\ncide how the results will be used. We illustrated this with a lending de-\\ncision. It is necessary for the decision maker to define a Z-value. When \\nthe probability of a positive outcome from the loan is estimated to be \\ngreater than Z, the loan is accepted. When it is less than Z, the loan is \\nrejected. There is a trade-off between success at identifying good loans \\nand success at identifying loans that will default. Improving the latter \\ntends to worsen the former, and vice versa. This trade-off can be sum-\\nmarized by a receiver operating curve (ROC) which relates the true pos-\\nitive rate (i.e., the percentage of the time a positive outcome is classified \\nas positive) and the false positive rate (the percentage of the time a \\nnegative outcome is classified as positive).  \\nA general point is we should not expect a machine learning model to \\nmake perfect predictions. The key test is whether their predictions are \\nas good as, or better than, the predictions made by a human being.  The \\npopularity of machine learning models in a variety of different fields \\nindicates that they must be passing this test. \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n3.1 \\nWhat is the objective function in a “plain vanilla” linear regres-\\nsion? \\n3.2 \\nHow is the objective function changed for (a) Ridge regression, \\n(b) Lasso regression, and (c) Elastic Net regression? \\n78 \\n                                                                   Chapter 3 \\n \\n \\n \\n3.3 \\nWhat is the main advantage of (a) Ridge regression and (b) Lasso \\nRegression? \\n3.4 \\nIn predicting house prices, how would you handle a feature which \\nis “yes” if a house has air conditioning and “no” if it does not? \\n3.5 \\nIn predicting house prices, how would you handle a feature which \\ndescribes the lot as “no slope”, “gentle slope”, “moderate slope”, \\nand “severe slope.” \\n3.6 \\nIn predicting house prices how would you handle a feature which \\nidentifies the neighborhood of the house. \\n3.7 \\nExplain the meaning of the term “regularization.” What is the dif-\\nference between L1 and L2 regularization? \\n3.8 \\nWhat is the sigmoid function? \\n3.9 \\nWhat is the objective function in a logistic regression? \\n3.10 What is the definition of (a) the true positive rate, (b) the false \\npositive rate, and (c) the precision? \\n3.11 What is plotted in an ROC? Explain the trade-offs it describes. \\n3.12  Explain what is meant by the dummy variable trap. \\n \\n \\n \\n \\nEXERCISES \\n \\n3.13  Using the validation set in Table 1.2, calculate tables similar to \\nTables 3.2 and 3.3. Using scaled data calculate biases, weights, \\nand mean squared errors for a \\n \\n(a) Plain vanilla linear regression of salary on X, X2, X3, X4, and X5, \\nwhere X is age.  \\n(b) Ridge regression of salary on X, X2, X3, X4, and X5 with \\uf06c = 0.02, \\n0.05, and 0.1. \\n(c) Lasso regression of salary on X, X2, X3, X4, and X5 with \\uf06c = 0.02, \\n0.05, and 0.1. \\n3.14 Suppose that, in the Lending Club data, we define default as the \\npositive outcome and no default as negative outcome.  \\n(a) What effect does this have on the bias and weights? Show \\nthat the probability of default and no-default are un-\\nchanged. \\n(b) Choose Z values of 0.25, 0.20 and 0.15 for predicting de-\\nfaults. Calculate confusion matrices and the ratios in Table \\n3.14. \\n(c) Use the Python implementation to confirm your answers to \\n(a) and (b) and verify that the AUC is still 0.6020. \\nLinear and Logistic Regression                                                                                             79 \\n \\n \\n \\n3.15 Extend the Iowa house price example by including additional fea-\\ntures from the Original_Data.xlsx file in \\nANDY LANDU NGOMA \\nAs in the analysis in the text, choose the first 1,800 observations \\nas the training set, the next 600 as the validation set, and the re-\\nmainder as the test set. One additional feature should be Lot \\nFrontage and you should consider alternative approaches for \\ndealing with the missing observations. Another additional feature \\nshould be the categorical feature Lot Shape. Choose a model for \\nprediction and calculate the accuracy for the test set. Repeat your \\nanalysis by randomly spitting data into training set, validation \\nset, and test set.  \\n3.16  The full data set for Lending Club is in the file Full_Data_Set.xlsx \\n(see ANDY LANDU NGOMA In the analysis in this \\nchapter “good loans” are those listed as “Current” and defaulting \\nloans are those listed as “Charged Off” (see column O in \\nFull_Data_Set.xlsx). Other loans are not considered. Repeat the \\nanalysis in this chapter assuming that “good loans” are those \\nlisted as “Fully Paid” and defaulting loans are those listed as \\n“Charged Off.” Are your results better than those in this chapter? \\nChoose additional features from the full data set and report on \\nany improvements in your logistic regression results. (Make sure \\nthe features you choose would have known values at the time the \\nloan was made.) \\n \\n\\xa0\\n \\n81 \\n \\n   \\n \\n \\nChapter 4 \\n \\nSupervised Learning:  \\nDecision Trees \\n \\n \\n \\nIn this chapter, we continue our discussion of supervised learning by \\nconsidering how decision trees can be used for prediction. Decision \\ntrees have a number of potential advantages over linear or logistic re-\\ngression. For example: \\n \\n\\uf0b7 They correspond to the way many human beings think about a \\nproblem and are easy to explain to non-specialists. \\n\\uf0b7 There is no requirement that the relationship between the target \\nand the features be linear. \\n\\uf0b7 The tree automatically selects the best features to make the pre-\\ndiction.  \\n\\uf0b7  A decision tree is less sensitive to outlying observations than a \\nregression \\n \\nThe first part of this chapter will focus on the use of decision trees \\nfor classification. We use the Lending Club data, introduced in Chapter \\n3, to illustrate the methodology. We build on the Bayes’ theorem mate-\\nrial in Chapter 1 to explain what is known as the naïve Bayesian classifi-\\ner. We then show how decision trees can be used for targets that are \\ncontinuous variables using the Iowa house price data. After that we ex-\\nplain how different machine learning algorithms can be combined to \\n82 \\n                                                                    Chapter 4 \\n \\n \\n \\nproduce composite predictions. An important example of this is a ran-\\ndom forest, which is created by generating many different decision trees \\nand combining the results.  \\n \\n4.1     Nature of Decision Trees \\nA decision tree shows a step-by-step process for making predict-\\nions. Figure 4.1 shows a simple example concerned with the classifica-\\ntion of applicants for a job into two categories: \\n \\n\\uf0b7 those who should get a job offer \\n\\uf0b7 those who should be told “thanks but no thanks” \\n \\nThe example illustrates a key feature of decision trees. The decision is \\nmade by looking at features one at a time rather than all at once. The \\nmost important feature, relevant degree in our example, is considered \\nfirst. After that experience and communication skills are used by the \\ndecision maker. \\n \\nFigure 4.1   Simple example of a decision tree \\n \\n \\n \\nAn employer might subconsciously use a decision tree such as that in \\nFigure 4.1 without ever formalizing it.  When decision trees are used as \\na machine learning tool, the tree is constructed from historical data us-\\ning an algorithm, as we will explain. \\nRelevant \\ndegree?\\nNo\\nExperience >\\n7 years?\\nExperience > \\n3 years?\\nYes\\nYes\\nNo\\nGood \\ncommunication \\nskills?\\nNo job \\noffer\\nYes\\nNo\\nJob \\noffer\\nNo job \\noffer\\nYes\\nNo\\nNo job \\noffer\\nYes\\nNo\\nJob \\noffer\\nNo job \\noffer\\nGood \\ncommunication \\nskills?\\nDecision Trees                                                                                                                           83 \\n \\n \\n \\n4.2     Information Gain Measures \\nWhat is the best feature to select at the first (root) node of a tree? \\nLet’s suppose that the purpose of our analysis is to replicate how em-\\nployment decisions have been made in the past.1  The feature to put at \\nthe root node is the one with the most information gain. Suppose we \\nhave lots of data on job applicants and find that we made a job offer to \\n20% of them. Suppose further that 50% of job applicants have a rele-\\nvant degree. If both those with a relevant degree and those without a \\nrelevant degree had a 20% chance of receiving a job offer, there would \\nbe no information gain to knowing whether an applicant has a relevant \\ndegree. Suppose instead that \\n \\n\\uf0b7 30% of those with relevant degrees received a job offer \\n\\uf0b7 10% of those without a relevant degree received a job offer \\n \\nThere is then clearly some information gain to knowing whether an ap-\\nplicant has a relevant degree.   \\nOne measure of information gain is based on entropy.  This is a \\nmeasure of uncertainty. If there are n outcomes and each outcome has a \\nprobability of 𝑝𝑖(1 ≤ i ≤ n), entropy can be defined as \\nEntropy = −∑𝑝𝑖log\\u2061(𝑝𝑖)\\n𝑛\\n𝑖=1\\n \\nHere, to be consistent with the machine learning literature, we define \\n“log” as log to the base 2.2 Initially in our example there is a 20% chance \\nof a job offer and an 80% chance of no job offer, so that: \\n \\nEntropy = −[0.2 × log(0.2) + 0.8 × log(0.8)] = 0.7219 \\n \\nIf a candidate has a relevant degree, this becomes \\n \\nEntropy = −[0.3 × log(0.3) + 0.7 × log(0.7)] = 0.8813 \\n \\nIf a candidate does not have a relevant degree, it becomes \\n \\nEntropy = −[0.1 × log(0.1) + 0.9 × log(0.9)] = 0.4690 \\n                                                           \\n1 A more sophisticated analysis might try to relate the performance of employees to \\nthe features known at the time the employment decision was made. \\n2 The base used for the logarithm does not make a difference to the results as chang-\\ning the base merely multiplies log(x) by the same constant for all x. When the base is \\n2, log(x) = y when 2𝑦= 𝑥.  \\n84 \\n                                                                    Chapter 4 \\n \\n \\n \\nBecause 50% of candidates have a relevant degree, the expected value \\nof entropy, assuming that information on whether a candidate has a \\nrelevant degree will be obtained, is \\n \\n0.5 × 0.8813 + 0.5 × 0.4690 = 0.6751 \\n \\nA measure of the information gain from finding out whether a candidate \\nhas a relevant degree is the expected uncertainty reduction. If uncer-\\ntainty is measured by entropy, this is \\n \\n0.7219 − 0.6751 = 0.0468 \\n \\nWhen constructing the decision tree, we first search for the feature \\nthat has the biggest information gain. This is put at the root of the tree. \\nFor each branch emanating from the root we then search again for the \\nfeature that has the biggest information gain. For both the “has relevant \\ndegree” and “does not have relevant degree,” the feature that maximizes \\nthe expected information gain (reduction in expected entropy) in our \\nexample is the number of years of business experience. When the can-\\ndidate has a relevant degree, the threshold for this feature that maxim-\\nizes the expected information gain is 3 years. At the second level of the \\ntree, the “has relevant degree” is therefore split into “experience > 3 \\nyears” and “experience ≤ 3 years” branches.  For the branch corre-\\nsponding to the candidate not having a relevant degree the threshold \\nthat maximizes the expected information gain is 7 years. The two sub-\\nsequent branches therefore are “experience > 7 years” and “experience \\n≤ 7 years.” We use the same procedure for building the rest of the tree.   \\nNote that numeric features can be used more than once. For example, \\nthe “Experience > 3 years” branch could lead to a further split into “Ex-\\nperience between 3 and 6 years” and “Experience greater than 6 years.”  \\nAn alternative to entropy for quantifying information gain is the Gini \\nmeasure. This is: \\nGini = 1 −∑𝑝𝑖\\n2\\n𝑛\\n𝑖=1\\n \\n \\nIt is used in the same way as entropy. In the example considered earlier, \\ninitially \\n \\nGini = 1 −0.22 −0.82 = 0.32 \\n \\nDecision Trees                                                                                                                           85 \\n \\n \\n \\nThe expected Gini measure after finding out whether the candidate has \\na relevant degree is \\n \\n0.5 × (1 −0.12 −0.92) + 0.5 × (1 −0.32 −0.72) = 0.30 \\n \\nThe information gain (reduction in the expected Gini measure) is 0.02. \\nMost of the time, the entropy and Gini measures give rise to similar \\ntrees. \\n \\n  \\n4.3     Application to Credit Decisions \\n \\nWe now apply the decision tree approach using the entropy measure \\nto the Lending Club data introduced in Chapter 3.  It will be recalled that \\nthere are 8,695 observations in the training set and 5,916 in the test set. \\nOf those in the training set, 7,196 were for good loans and 1,499 were \\nfor loans that defaulted. Without any further information, the probabil-\\nity of a good loan is therefore estimated from the training set as \\n7,196/8,695 or 82.76%. The initial entropy is therefore: \\n \\n−0.8276 × log(0.8276) −0.1724 × log(0.1724) = 0.6632 \\n \\nWe will consider the same four features as in Chapter 3: \\n \\n\\uf0b7 A home ownership variable (= 1 if home owned; = 0 if rented)  \\n\\uf0b7 The applicant’s income \\n\\uf0b7 The applicant’s debt to income ratio (dti) \\n\\uf0b7 The applicant’s credit score (FICO) \\n \\nThe first step in constructing a tree is to calculate the expected in-\\nformation gain (reduction in expected entropy) from each feature. Of \\nthe applicants, 59.14% own their own home while 40.86% rent. Loans \\nwere good for 84.44% of those who owned their own homes and \\n80.33% of those who rented. The expected entropy if home ownership \\n(but no other feature) becomes known is therefore: \\n \\n0.5914 × [−0.8444 × log(0.8444) −0.1556 × log(0.1556)] \\n+0.4086 × [−0.8033 × log(0.8033) −0.1967 × log\\u2061(0.1967)] = 0.6611 \\n \\nThe expected reduction in entropy is therefore a modest 0.6632−0.6611 \\n= 0.0020. \\n86 \\n                                                                    Chapter 4 \\n \\n \\n \\nThe calculation of the expected entropy from income requires the \\nspecification of a threshold income. Define:  \\n \\nP1:    Probability that income is greater than the threshold \\nP2:    Probability that, if income is greater than the threshold, the bor-\\nrower does not default  \\nP3:   Probability that, if income is less than the threshold, the bor-\\nrower does not default \\n \\nThe expected entropy is \\n \\n𝑃1[−𝑃2log(𝑃2) −(1 −𝑃2) log(1 −𝑃2)] \\n+(1 −𝑃1)[−𝑃3log(𝑃3) −(1 −𝑃3)log(1 −𝑃3)] \\n \\nWe carry out an iterative search to determine the threshold income that \\nminimizes this expected entropy for the training set. It turns out that \\nthis is $85,202.  For this value of the threshold, P1 = 29.93%, P2 = \\n87.82%, and P3 = 80.60% and expected entropy is 0.6573. \\nThe results of all the information gain calculations are shown in Ta-\\nble 4.1. It can be seen that the FICO score with a threshold of 717.5 has \\nthe greatest information gain. It is therefore put at the root node of the \\ntree. The initial branches of the tree correspond to FICO > 717.5 and \\nFICO ≤ 717.5. \\n \\nTable 4.1   Information gain from features to determine the root node \\n(see Excel decision tree file for Lending Club case) \\n \\nFeature \\nThreshold \\nvalue \\nExpected \\nentropy \\nInformation \\ngain \\nHome Ownership \\nN.A. \\n0.6611 \\n0.0020 \\nIncome ($’000s) \\n85.202 \\n0.6573 \\n0.0058 \\nDebt to income (%) \\n19.87 \\n0.6601 \\n0.0030 \\nCredit score (FICO) \\n717.5 \\n0.6543 \\n0.0088 \\n \\nFor the next level of the tree we repeat the process. Table 4.2 shows \\nthe calculations for FICO > 717.5. In this case, the starting expected en-\\ntropy is 0.4402. We must calculate the information gain of each of the \\nthree remaining features and consider the possibility of a further \\nbranch involving the FICO score (i.e., splitting the range of FICO scores \\nabove 717.5 into two categories) It turns out that income has the high-\\nest information gain and is therefore the feature that should be consid-\\nered next, The threshold value of income is $48,750. \\nDecision Trees                                                                                                                           87 \\n \\n \\n \\nTable 4.2   Information gain from features to determine the second lev-\\nel of the tree when FICO > 717.5 (see Excel file and Python implementa-\\ntion) \\n \\nFeature \\nThreshold \\nvalue \\nExpected \\nentropy \\nInformation \\ngain \\nHome ownership \\nN.A. \\n0.4400 \\n0.0003 \\nIncome ($’000s) \\n48.75 \\n0.4330 \\n0.0072 \\nDebt to income (%) \\n21.13 \\n0.4379 \\n0.0023 \\nFICO score \\n789 \\n0.4354 \\n0.0048 \\n \\nTable 4.3 shows the results when FICO ≤ 717.5.  In this case the \\nstarting entropy is 0.7043.  Income proves to be the feature with the \\nmost information gain and the threshold is $85,202. (Note that it is not \\nalways the case that the feature chosen is the same for both branches \\nemanating from a node. Also, when the feature chosen does happen to \\nbe the same for both branches, it will not in general have the same \\nthreshold for both branches.) \\n \\n \\nTable 4.3   Information gain of features to determine the second level of \\ntree when FICO ≤ 717.5 (see Excel file and Python implementation) \\n \\nFeature \\nThreshold \\nvalue \\nExpected \\nentropy \\nInformation \\ngain \\nHome ownership \\nN.A. \\n0.7026 \\n0.0017 \\nIncome ($’000s) \\n85.202 \\n0.6989 \\n0.0055 \\nDebt to income (%) \\n16.80 \\n0.7013 \\n0.0030 \\nFICO score \\n682 \\n0.7019 \\n0.0025 \\n \\nThe construction of the tree continues in this way. The full tree pro-\\nduced by Sklearn’s DecisionTreeClassifier is summarized in Figure 4.2. \\nThe final points reached by the tree (shown as ovals) are referred to as \\nleaves. The numbers shown in the leaves in Figure 4.2 are the predicted \\nprobabilities of default.  For example, if FICO > 717.5, Income > 48.75, \\nand dti > 21.885, the estimated probability of no-default is 0.900. This is \\nbecause there were 379 observations in the training set satisfying these \\nconditions and 341 of them were good loans (341/379 = 0.900).  \\nIn determining the tree, it is necessary to set a number of hyperpa-\\nrameters.  In this case: \\n \\n88 \\n                                                                    Chapter 4 \\n \\n \\n \\n\\uf0b7 \\nThe maximum depth of the tree was set equal to 4. This \\nmeans that there were at most four levels at which the tree \\nsplits. \\n\\uf0b7 \\nThe minimum number of observations necessary for a split \\nwas set to 1,000.  \\n \\nThe second hyperparameter explains why the tree sometimes stops \\nbefore reaching the fourth level. For example, there are only 374 obser-\\nvations where FICO > 717.5 and income ≤ 48.75; there are only 893 ob-\\nservations where FICO > 717.5, income > 48.75, and dti  ≤  21.885; and \\nthere are only 379 observations where FICO > 717.5, income > 48.75, \\nand dti > 21.885. \\n \\nFigure 4.2   Decision tree for Lending Club. Numbers at the end of the \\ntree are the probability of a good loan for training set \\n \\n \\n \\nAs with logistic regression we need a Z-value to define whether a \\nloan is acceptable. Similarly to Section 3.11, we consider Z-values of \\n0.75, 0.80, and 0.85.  An examination of Figure 4.2 shows that these \\nthree Z-values correspond to the following criteria: \\n \\n\\uf0b7 Z = 0.75: predict that all loans are good except those for which (a) \\nincome ≤ $85,202, dti  > 16.545, and FICO  ≤  687.5 and (b)  in-\\ncome  ≤  $32,802,  dti  ≤  16.545, and FICO  ≤ 717.5. \\n\\uf0b7 Z = 0.80:  Same as Z = 0.75 except that loans where FICO ≤ 717.5, \\nincome > $85,202, and dti > 25.055 are not considered good. \\nFICO ≤ 717.5\\nIncome ≤ 48.75\\ndti ≤ 21.885\\nIncome ≤ 85.202\\ndti ≤ 25.055\\ndti ≤ 16.545\\nFICO ≤ 677.5\\n0.853\\n0.936\\n0.784\\nFICO ≤ 687.5\\nIncome ≤ 32.802\\n0.900\\n0.892\\n0.852\\n0.812\\n0.745\\n0.832\\n0.725\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nno\\nno\\nno\\nno\\nno\\nno\\nno\\nno\\nno\\nDecision Trees                                                                                                                           89 \\n \\n \\n \\n\\uf0b7 Z=0.85: Predict loans are good loans when (a) FICO > 717.5 or (b) \\nFICO ≤ 717.5, income > $85,202, and dti ≤ 25.055 \\n \\nIt is interesting to note that a tree can give inconsistent predictions. \\nFor example, when FICO=700, dti=10, and Income =30, the no-default \\nprobability predicted by the tree is 0.725. But when the dti value is \\nchanged from 10 to 20 (a worse value) the no-default probability in-\\ncreases to 0.812. \\nTables 4.4 to 4.6 give confusion matrices for the test set when Z is \\n0.75, 0.80, and 0.85, respectively, and Table 4.7 provides the ratios in-\\ntroduced in Section 3.11.  \\nFigure 4.3 shows the ROC curve. The marked points correspond to \\n11 ranges within which the threshold Z can be chosen. If Z ≤ 0.725, all \\nloans are accepted; if 0.725 < Z ≤ 0.745, we accept all loans except those \\ncorresponding to the 0.725 leaf in Figure 4.2; if 0.745 < Z ≤  0.784 we \\naccept all loans except those corresponding to the 0.725 and 0.745 \\nleaves in Figure 4.2; and so on. Finally, if Z >0.936 we accept no loans.       \\nThe AUC calculated in the Python implementation is 0.5948, marginally \\nworse than the 0.6020 calculated for the logistic regression model.   \\n \\nTable 4.4   Confusion matrix for test set when Z = 0.75 (See Python or \\nExcel files) \\n \\n \\nPredict positive \\n(no default) \\nPredict negative  \\n(default) \\nOutcome positive  \\n(no default) \\n62.42% \\n19.69% \\nOutcome negative \\n(default) \\n11.07% \\n6.81% \\n \\n \\nTable 4.5   Confusion matrix for test set when Z = 0.80 (See Python or \\nExcel files) \\n \\n \\nPredict positive  \\n(no default) \\nPredict negative \\n (default) \\nOutcome positive \\n(no default) \\n59.47% \\n22.65% \\nOutcome negative \\n(default) \\n10.45% \\n7.44% \\n  \\n \\n90 \\n                                                                    Chapter 4 \\n \\n \\n \\nTable 4.6   Confusion matrix for test set when Z = 0.85 (See Python or \\nExcel files) \\n \\n \\nPredict positive \\n (no default) \\nPredict negative \\n (default) \\nOutcome positive  \\n(no default) \\n32.15% \\n49.97% \\nOutcome negative \\n(default) \\n4.73% \\n13.15% \\n \\n \\nTable 4.7   Ratios calculated from Tables 4.4 to 4.6 (See Python or Excel \\nfiles) \\n  \\n \\nZ=0.75 \\nZ=0.80 \\nZ=0.85 \\nAccuracy \\n69.24% \\n66.90% \\n45.30% \\nTrue Positive Rate \\n76.02% \\n72.42% \\n39.15% \\nTrue Negative Rate \\n38.09% \\n41.59% \\n73.53% \\nFalse Positive Rate \\n61.91% \\n58.41% \\n26.47% \\nPrecision \\n84.94% \\n85.06% \\n87.17% \\nF-score \\n80.23% \\n78.23% \\n54.03% \\n \\n \\nFigure 4.3   Trade-off between true positive rate and false positive rate \\nfor decision tree approach (see Python or Excel files) \\n \\n \\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nTrue Positive Rate \\nFalse Positive Rate\\nDecision Trees                                                                                                                           91 \\n \\n \\n \\n4.4     The Naïve Bayes Classifier \\n \\nWe introduced Bayes’ theorem in Chapter 1. Bayesian learning in-\\nvolves using Bayes’ theorem to update probabilities. For example, Chap-\\nter 1 showed how Bayes’ theorem can be used in the identification of \\nfraudulent transactions.  \\nThe tree in Figure 4.2 can be regarded as an example of Bayesian \\nlearning. The probability of a good loan in the training set with no in-\\nformation about the features is 0.8276.  The probability that the FICO \\nscore will be greater than 717.5, conditional on the loan being good, is \\n0.2079 and the unconditional probability that FICO is greater than \\n717.5 is 0.1893. From Bayes’ theorem the probability of a good loan \\nconditional on FICO > 717.5 is: \\n \\n= Prob(FICO > 717.5|good\\u2061loan) × Prob(good\\u2061loan)\\nProb(FICO > 717.5)\\n \\n \\n= 0.2079 × 0.8276\\n0.1893\\n= 0.9089 \\n \\nOther (more complicated) Bayesian calculations can be used to up-\\ndate probabilities further. For example, at the next step we can calculate \\nthe probability of a good loan conditional on both FICO > 717.5 and In-\\ncome > $48,750. \\nThe Naïve Bayes Classifier is a procedure that can be used if the val-\\nues of the features for observations classified in a particular way can be \\nassumed to be independent.  If C is a classification result and 𝑥𝑗 is the \\nvalue of the jth feature (1≤ j ≤ m), we know from Bayes theorem that \\n \\nProb(𝐶|𝑥1, 𝑥2, … , 𝑥𝑚) =\\nProb(𝑥1, 𝑥2, … , 𝑥𝑚|𝐶)\\nProb(𝑥1,𝑥2,…𝑥𝑚)\\nProb(C) \\n \\nBecause of the independence assumption this reduces to \\n \\nProb(𝐶|𝑥1, 𝑥2, … , 𝑥𝑚) =\\nProb(𝑥1|𝐶)Prob(𝑥2|𝐶)…Prob(𝑥𝑚|𝐶)\\nProb(𝑥1,𝑥2,…𝑥𝑚)\\nProb(C) \\n \\nThis shows that if we know the probability of each feature conditional \\non the classification, we can calculate probability of the classification \\nconditional on a particular mixture of features occurring.  \\nAs a simple example of this, suppose that the unconditional probabil-\\nity of a good loan is 85% and that there are three independent features\\n92 \\n                                                                    Chapter 4 \\n \\n \\n \\nwhen a loan is being assessed. These are: \\n \\n\\uf0b7 \\nWhether the applicant owns a house (denoted by H). The prob-\\nability of the applicant owning a house if the loan is good is 60% \\nwhereas the probability of the applicant owning her own house \\nif the loan defaults is 50%. \\n\\uf0b7 \\nWhether the applicant has been employed for more than one \\nyear (denoted by E).  The probability of the applicant being em-\\nployed for more than one year if the loan is good is 70% where-\\nas the probability of this if the loan defaults is 60%. \\n\\uf0b7 \\nWhether there are two applicants or only one (denoted by T). \\nThe probability of two applicants when the loan is good is 20% \\nwhereas the probability of two applicants when the loan de-\\nfaults is 10%. \\n \\nConsider an applicant that is able to check all three boxes. She owns \\na house, she has been employed for more than one year, and she is one \\nof two applicants for the same loan.  Assuming the features are inde-\\npendent across good loans and across defaulting loans: \\n \\n\\u2061Prob(Good\\u2061Loan|H, E, T) =\\n0.6 × 0.7 × 0.2\\nProb(H\\u2061and\\u2061E\\u2061and\\u2061T) × 0.85\\u2061\\u2061\\u2061 \\n \\n\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061=\\n0.0714\\nProb(H\\u2061and\\u2061E\\u2061and\\u2061T) \\n \\nProb(Defaulting\\u2061Loan|H, E, T) =\\n0.5 × 0.6 × 0.1\\nProb(H\\u2061and\\u2061E\\u2061and\\u2061T) × 0.15\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061 \\n \\n\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061=\\n0.0045\\nProb(H\\u2061and\\u2061E\\u2061and\\u2061T) \\n \\nBecause the probability of a good loan and the probability of a default-\\ning loan must add to one, we do not need to calculate the value of Prob \\n(H and E and T). The probability of a good loan is \\n \\n0.0714\\n0.0714 + 0.0045 = 0.941 \\n \\nand the probability of a defaulting loan is \\n \\nDecision Trees                                                                                                                           93 \\n \\n \\n \\n0.0045\\n0.0714 + 0.0045 = 0.059 \\n \\nFor an applicant who checks all three boxes the probability of a good \\nloan rises from 85% to just over 94%. \\nWe can also use the naïve Bayes classifier with continuous distribu-\\ntions. Suppose we want to use the data in Chapter 3 to produce a loan \\nforecast using two features: FICO score and income. We assume that \\nthese features are independent both for data on good loans and data on \\ndefaulting loans.3  Table 4.8 shows the mean and standard deviation of \\nthe FICO score and income conditional on a good loan and a defaulting \\nloan. \\n \\nTable 4.8   Statistics on FICO score and income conditional on loan re-\\nsult. Income is measured in $’000s \\n \\nLoan result \\nMean  \\nFICO \\nSD \\nFICO \\nMean \\nIncome \\nSD In-\\ncome \\nGood loan \\n696.19 \\n31.29 \\n79.83 \\n59.24 \\nDefaulting loan \\n686.65 \\n24.18 \\n68.47 \\n48.81 \\n \\nConsider an individual who has a FICO score of 720 and an income \\n(‘000s) of 100. Conditional on a loan being good the FICO score has a \\nmean of 696.19 and a standard deviation of 31.29. Assuming a normal \\ndistribution, the probability density for the individual’s FICO score con-\\nditional on the loan being good is \\n \\n1\\n√2𝜋× 31.29\\nexp (−(720 −696.19)2\\n2 × 31.292\\n) = 0.00954 \\n \\nSimilarly, assuming a normal distribution, the probability density for \\nincome conditional on the loan being good is4  \\n  \\n1\\n√2𝜋× 59.24\\nexp(−(100 −79.83)2\\n2 × 59.242\\n) = 0.00636 \\n                                                           \\n3 The independence assumption is an approximation. For loans that defaulted, the \\ncorrelation between credit score and income is about 0.07 and, for loans that are \\ngood, it is about 0.11.  \\n4 It would be better to assume a lognormal distribution for income. We have not \\ndone this to keep the example simple.  \\n94 \\n                                                                    Chapter 4 \\n \\n \\n \\nThe probability density for the credit score conditional on the loan \\ndefaulting is \\n \\n1\\n√2𝜋× 24.18\\nexp (−(720 −686.65)2\\n2 × 24.182\\n) = 0.00637 \\n \\nSimilarly, the probability density for the income conditional on the loan \\ndefaulting is \\n \\n1\\n√2𝜋× 48.81\\nexp(−(100 −68.47)2\\n2 × 48.812\\n) = 0.00663 \\n \\nThe unconditional probability of the loan being good is 0.8276 and \\nthe unconditional probability of it being bad is 0.1724.  The probability \\nof a loan being good conditional on a credit score of 720 and an income \\n($’000) of 100 is  \\n \\n0.00954 × 0.00636 × 0.8276\\n𝑄\\n= 5.020 × 10−5\\n𝑄\\n \\n \\nwhere Q is the probability density of the observation ($100,000 income \\nand credit score equal to 720). \\nThe corresponding conditional probability of a bad loan is  \\n  \\n0.00637 × 0.00663 × 0.1724\\n𝑄\\n= 0.729 × 10−5\\n𝑄\\n \\n \\nBecause the two probabilities must add up to one we know that the \\nprobability of the loan being good is 5.020/(5.020+0.729) or 0.873.  \\n(One of the attractive features of the naïve Bayes classifier is that we do \\nnot need to calculate Q to obtain this result.) \\nThe naïve Bayes classifier is easy to use when there are a large num-\\nber of features.  It makes a simple set of assumptions. These assump-\\ntions are unlikely to be completely true in practice. However, the ap-\\nproach has been found to be useful in a variety of situations. For exam-\\nple, it is quite effective in identifying spam when word frequencies are \\nused as features. (See Chapter 8 for the use of the naïve Bayes Classifier \\nin natural language processing.) \\n \\n \\n \\nDecision Trees                                                                                                                           95 \\n \\n \\n \\n \\n 4.5     Continuous Target Variables  \\n \\nSo far we have considered the use of decision trees for classification. \\nWe now describe how they can be used to predict the value of a contin-\\nuous variable.  Suppose that the feature at the root node is X and the \\nthreshold value for X is Z.  We choose X and Z to minimize the expected \\nmean squared error (mse) in the prediction of the target for the training \\nset. In other words, we minimize \\n \\nProb(𝑋≥𝑍) × (mse\\u2061if\\u2061𝑋≥𝑍) + Prob(𝑋< 𝑍) × (mse\\u2061if\\u2061𝑋< 𝑍) \\n \\nThe feature at the next node and its threshold are chosen similarly. The \\nvalue predicted at a tree leaf is the average of the values for the obser-\\nvations corresponding to the leaf. \\nWe will illustrate this procedure for the Iowa house price data con-\\nsidered in Chapter 3.  To keep the example manageable, we consider \\nonly two features: \\n \\n\\uf0b7 Overall quality (scale 1 to 10) \\n\\uf0b7 Living area (square feet) \\n \\n(These were identified as the most important features by linear regres-\\nsion in Chapter 3.) As in Chapter 3 we divide up the data (2,908 obser-\\nvations in total) so that there are 1,800 observations in the training set, \\n600 in the validation set, and 508 in the test set. The mean and standard \\ndeviation of the prices of houses in the training set (‘000s) are $180.817 \\nand $77.201.  \\nFirst, we determine the feature to put at the root node and its \\nthreshold. For each of the two features, we use an iterative search pro-\\ncedure to calculate the optimal threshold. The results are shown in Ta-\\nble 4.9. The expected mse is lowest for overall quality which has an op-\\ntimal threshold of 7.5. This feature and its threshold therefore define \\nthe root node. (Because overall quality is an integer all thresholds be-\\ntween 7 and 8 are equivalent. A similar point applies to living area.) \\nTable 4.10 considers the best feature when overall quality ≤ 7.5. It \\nturns out that, even though overall quality has been split at the root \\nnode, it is best to split it again at the second level using a threshold of \\n6.5.  Table 4.11 shows that when overall quality > 7.5, it is also best to \\nsplit overall quality again, this time with a threshold of 8.5. Following \\nthe two splits on overall quality, it is optimal to split living area at each \\nof the decision points encountered at the third level. \\n96 \\n                                                                    Chapter 4 \\n \\n \\n \\nTable 4.9   Expected mean squared error at root node. House prices are \\nmeasured in thousands of dollars for the purposes of calculating mse \\n(see Excel decision tree file for Iowa house price case). Threshold = Z. \\n \\nFeature \\nZ \\nNo. of  \\nobs < Z \\nmse of \\nobs < Z \\nNo. of \\nobs ≥ Z \\nmse of \\nobs ≥ Z \\nE(mse) \\nOverall \\nQuality \\n7.5 \\n1,512 \\n2,376 \\n288 \\n7,312 \\n3,166 \\nLiving \\n(sq. ft.) \\n1,482.5 \\n949 \\n1,451 \\n851 \\n6,824 \\n3,991 \\n \\n \\nTable 4.10   Expected mean squared error at second level when overall \\nquality ≤ 7.5. House prices are measured in thousands of dollars for the \\npurposes of calculating mse (see Excel decision tree file for Iowa house \\nprice case). Threshold = Z. \\n \\nFeature \\nZ \\nNo. of  \\nobs < Z \\nmse of \\nobs < Z \\nNo. of \\nobs ≥ Z \\nmse of \\nobs ≥ Z \\nE(mse) \\nOverall \\nQuality \\n6.5 \\n1,122 \\n1,433 \\n390 \\n1,939 \\n1,564 \\nLiving \\n(sq. ft.) \\n1,412.5 \\n814 \\n1,109 \\n698 \\n2,198 \\n1,612 \\n \\n \\nTable 4.11   Expected mean squared error at second level when overall \\nquality >7.5. House prices are measured in thousands of dollars for the \\npurposes of calculating mse (see Excel decision tree file for Iowa house \\nprice case). Threshold = Z. \\n \\nFeature \\nZ \\nNo. of  \\nobs < Z \\nmse of \\nobs < Z \\nNo. of \\nobs ≥ Z \\nmse of \\nobs ≥ Z \\nE(mse) \\nOverall \\nQuality \\n8.5 \\n214 \\n3,857 \\n74 \\n8,043 \\n4,933 \\nLiving \\n(sq. ft.) \\n1,971.5 \\n165 \\n3,012 \\n123 \\n8,426 \\n5,324 \\n \\n \\nThe tree produced by Sklearn’s DecisionTreeRegressor is shown in \\nFigure 4.4. The maximum depth of the tree was specified as three. The \\naverage house prices and root mean squared errors are shown on the \\nDecision Trees                                                                                                                           97 \\n \\n \\n \\ntree for each of the leaf nodes (see circles). The overall root mean \\nsquared error of the predictions for the training set, validation set, and \\ntest set are shown in Table 4.12. It can be seen that the model general-\\nizes quite well.  \\nThe root mean squared errors are quite large because the tree in ef-\\nfect divides all houses into only eight clusters. With more features and \\nmore depth, more clusters of houses would be considered. However, the \\nnumber of houses in some clusters would then be quite small and the \\ncalculated average house price might be unreliable.  For better results a \\nlarger data set is likely to be necessary.   \\n \\n \\nFigure 4.4   Decision tree for calculating house prices. The house price \\npredictions and the rmse’s are shown in the final (circular) leaf nodes) \\n \\n \\n \\n \\n \\nTable 4.12   Comparison of results for training set, validation set, and \\ntest set (see Python) \\n \\n \\nRoot mean squared error \\nof house price ($’000s) \\nTraining set \\n38.66 \\nValidation set \\n40.46 \\nTest set \\n39.05 \\n \\nQuality ≤ 7.5\\nQuality ≤ 6.5\\nQuality ≤ 8.5\\nLiv Area ≤ 1,378\\nLiv Area ≤ 1,822\\nLiv Area ≤ 1,969\\nLiv Area ≤ 2,229\\n125.5\\n(29.3)\\n165.6\\n(36.8)\\n191.7\\n(34.9)\\n239.6\\n(44.8)\\n248.1\\n(46.6)\\n313.0\\n(62.8)\\n335.5\\n(48.7)\\n457.4\\n(86.8)\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nno\\nno\\nno\\nno\\nno\\nno\\nno\\n98 \\n                                                                    Chapter 4 \\n \\n \\n \\nFinally, note that we can adjust our calculations so that more than \\ntwo branches are considered at a node. This is appropriate for a cate-\\ngorical feature with more than two possible values. For features that \\ncan take a range of values, we can consider N branches (N > 2) at a node \\nand maximize the information gain over N − 1 thresholds.  \\n \\n4.6     Ensemble Learning \\n \\nWhen the predictions from several different machine learning algo-\\nrithms are aggregated, the result can be better than the predictions \\nfrom any one of the algorithms.  The improvement in the predictions \\ndepends on the correlation between the estimates produced from the \\nalgorithms. If two algorithms always produce the same predictions, \\nthere is clearly nothing to be gained by using both of them. But, if this is \\nnot the case, there is potentially some value in producing a composite \\nprediction that uses the results from both algorithms.  Using two or \\nmore algorithms to make predictions is known as ensemble learning.   \\nSuppose that you have a biased coin which when tossed has a 52% \\nchance of giving one result (heads or tails, you do not know which) and \\na 48% of giving the other result. If you want to know whether it is heads \\nor tails that is more likely, you could toss the coin once, but this would \\nnot give much guidance. If you toss the coin 1,000 times there is a prob-\\nability of about 90% that, if heads has a probability of 0.52, you will see \\nmore heads than tails. Similarly, if it is tails that has a probability of \\n0.52, there is a probability of about 90% that you will see more tails \\nthan heads. This illustrates that 1,000 weak learners can be combined \\nto produce a learner where the predictions are reliable. Of course, in \\nthis example the learners are independent of each other. In machine \\nlearning different learning algorithms are unlikely to be totally inde-\\npendent and so the prediction improvement will not in general be as \\ngood as for the coin tossing example. \\nThere are a number of ways of combining predictions. Sometimes a \\n(possibly weighted) average of the predictions is appropriate. When \\neach learner recommends a particular classification, we can use majori-\\nty voting (i.e., the class that is recommended most can be chosen). \\n  \\nBagging  \\nBagging involves using the same algorithm but training it on differ-\\nent random samples of the training set or the features.  We might have \\n200,000 observations in the training set and randomly sample 100,000 \\nobservations 500 times to get 500 subsets of the training set. We then \\ntrain the model on each subset in the usual way. The sampling is nor-\\nDecision Trees                                                                                                                           99 \\n \\n \\n \\nmally done with replacement so that the same observation may appear \\nmore than once in a subset. (If the sampling is done without replace-\\nment the method is referred to as pasting.)  \\nWe can also create many new models by sampling (without re-\\nplacement) from the features. For example, if there are 50 features, we \\ncould create 100 models each with 25 features. Sometimes models are \\ncreated by random sampling from both features and observations. \\n \\nRandom Forests \\nA random forest as its name implies is an ensemble of decision trees.  \\nThe trees are often created by sampling from the features or observa-\\ntions using the bagging approach just mentioned. Each tree gives a \\nsuboptimal result but overall the prediction is usually improved.  \\nAnother approach to creating a random forest is to randomize the \\nthresholds used for features rather than search for the best possible \\nthreshold. This can be computationally efficient as finding the optimal \\nfeature threshold at each node can be time consuming.  \\nThe importance of each feature in a random forest can be calculated \\nas the weighted average information gain (as measured by entropy or \\nGini) with weights that are proportional to the number of observations \\nconsidered at a node. \\n  \\nBoosting \\nBoosting refers to an ensemble method where prediction models are \\nused sequentially, each trying to correct the errors in the previous one.  \\nConsider the loan classification problem we looked at earlier. We \\nmight create a first classification in the usual way. We then increase the \\nweight given to misclassified observations and create a new set of pre-\\ndictions; and so on. The predictions are combined in the usual way ex-\\ncept that the weight given to a prediction depends on its accuracy. This \\nprocedure is known as AdaBoost. \\nA different approach from AdaBoost is gradient boosting.  At each it-\\neration, gradient boosting tries to fit a new predictor to the errors made \\nby the previous predictor. Suppose there are three iterations. The final \\nprediction is the sum of the three predictors. This is because the second \\npredictor estimates errors in the first predictor and the third predictor \\nestimates errors in a predictor that equals the sum of the first two pre-\\ndictors.  \\n \\n \\n \\n \\n100 \\n                                                                    Chapter 4 \\n \\n \\n \\nSummary \\n \\nA decision tree is an algorithm for classification or predicting the \\nvalue of variable.  Features are considered in order of the information \\ngain they provide. For classification, two alternative measures of infor-\\nmation gain are entropy and Gini. When the value of a variable is being \\npredicted, the information gain is measured by the improvement in ex-\\npected mean squared error.  \\nIn the case of a categorical feature, the information gain arises from \\nknowledge of the feature’s label (e.g., whether a potential borrower’s \\nhome is owned or rented). In the case of numerical features, it is neces-\\nsary to determine one (or more) thresholds defining two (or more) \\nranges for the feature’s value. These thresholds are determined so that \\nthe expected information gain is maximized. \\nA decision tree algorithm first determines the optimal root node of \\nthe tree using the “maximize information gain” criterion we have just \\ndescribed. It then proceeds to do the same for subsequent nodes. The \\nends of the final branches of the tree are referred to a leaf nodes. When \\nthe tree is used for classification, the leaf nodes contain the probabili-\\nties of each category being the correct. When a numerical variable is \\nbeing predicted, the leaf nodes give the average value of the target.  The \\ngeometry of the tree is determined with the training set, but statistics \\nconcerned with its accuracy should (as always in machine learning) \\ncome from the test set. \\nSometimes more than one machine learning algorithm is used to \\nmake predictions. The results can then be combined to produce a com-\\nposite prediction. This is referred to as the use of ensemble methods. A \\nrandom forest machine learning algorithm is created by building many \\ndifferent trees and combining the results. The trees can be created by \\nsampling from the observations or from the features (or both). They can \\nalso be created by randomizing the thresholds.  \\nBagging is the term used when different subsets of the observations \\nor features in the training set are used to create multiple models. Boost-\\ning is a version of ensemble where prediction models are chosen se-\\nquentially, and each model is designed to correct errors in the previous \\nmodel.  In classification one way of doing this is to increase the weight \\nof observations that are misclassified. Another is to use one machine \\nlearning model to predict the errors given by another model. \\n \\n \\n \\nDecision Trees                                                                                                                           101 \\n \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n4.1 \\nWhat are the main differences between the decision tree ap-\\nproach to prediction and the regression approach? \\n4.2 \\nHow is entropy defined? \\n4.3 \\nHow is the Gini measure defined? \\n4.4 \\nHow is information gain measured?  \\n4.5 \\nHow do you choose the thresholds for a numerical variable in a \\ndecision tree? \\n4.6 \\nWhat is the assumption underlying the naïve Bayes classifier?  \\n4.7 \\nWhat is an ensemble method? \\n4.8 \\nWhat is a random forest? \\n4.9 \\nExplain the difference between bagging and boosting.  \\n4.10 “The decision tree algorithm has the advantage that it is trans-\\nparent.” Explain this comment. \\n \\n \\nEXERCISES \\n \\n4.11 What strategy corresponds to a Z-value of 0.9 in Figure 4.2? What \\nis the confusion matrix when this Z-value is used on the test data?  \\n4.12 For the naïve Bayes classifier data in Table 4.8, what is the prob-\\nability of a default when the credit score is 660 and the income is \\n$40,000? \\n4.13 Python exercise: Similarly, to Exercise 3.16, determine the effect \\non the decision tree analysis of defining good loans as “Fully Paid” \\nrather than “Current.”  Investigate the effect of adding more fea-\\ntures to the analysis. \\n4.14 Test the effect of changing (a) the maximum depth of the tree and \\n(b) the minimum number of samples necessary for a split on the \\ndecision tree results for Lending Club using Sklearn’s Decision-\\nTreeClassifier. \\n4.15 Choose an extra feature in addition to the two considered in Fig-\\nure 4.4 and construct a tree using all three features.  Compare the \\nresults with those in Section 4.5. Use Sklearn’s DecisionTree-\\nRegressor.   \\n\\xa0\\n \\n103 \\n \\n \\n \\n \\n \\nChapter 5 \\n \\nSupervised Learning: SVMs \\n \\n \\n \\n \\n \\n \\n \\nThis chapter considers another popular category of supervised \\nlearning models known as a support vector machines (SVMs). Like deci-\\nsion trees, SVMs can be used for either classification or for the predic-\\ntion of a continuous variable.  \\nWe first consider linear classification where a linear function of the \\nfeature values is used to separate observations into two categories. We \\nthen move on to explain how non-linear separation can be achieved.  \\nFinally, we show that, by reversing the objective, we can use SVM for \\npredicting the value of a continuous variable, rather than for classifica-\\ntion. \\n \\n \\n5.1   Linear SVM Classification \\n \\nTo describe how linear classification works, we consider a simple \\nsituation where loans are classified into good loans and defaulting loans \\nby considering only two features: credit score and income of the bor-\\nrower. A small amount of illustrative data is shown in Table 5.1.  This is \\na balanced data set in that there are five good loans and five loans that \\ndefaulted. SVM does not work well for a seriously imbalanced data set \\n104                                                                                                                                     Chapter 5 \\n \\n \\n \\nand procedures such as those mentioned in Section 3.10 sometimes \\nhave to be used to create a balanced data set. \\n \\nTable 5.1   Loan data set to illustrate SVM  \\n \\nCredit \\nscore \\nAdjusted credit \\nscore \\nIncome \\n(‘000s) \\nDefault = 0; \\ngood loan = 1 \\n660 \\n   40 \\n   30 \\n0 \\n650 \\n   30 \\n   55 \\n0 \\n650 \\n   30 \\n   63 \\n0 \\n700 \\n   80 \\n   35 \\n0 \\n720 \\n           100 \\n   28 \\n0 \\n650 \\n   30 \\n140 \\n1 \\n650 \\n   30 \\n100 \\n1 \\n710 \\n   90 \\n   95 \\n1 \\n740 \\n 120 \\n   64 \\n1 \\n770 \\n 150 \\n   63 \\n1 \\n \\nThe first step is to normalize the data so that the weight given to \\neach feature is the same. In this case, we will take a simple approach \\nand subtract 620 from the credit score. This provides approximate \\nnormalization because the adjusted credit scores range from 30 to 150 \\nwhile incomes range from 28 to 140.  \\nFigure 5.1 gives a scatter plot of the loans. Defaulting loans (repre-\\nsented by the circles) tend to be closer to the origin than the good loans \\n(represented by the squares). We can separate the observations into \\ntwo groups by drawing a straight line such as the one shown in the fig-\\nure.  Loans that are to the north-east of the line are good. Those to the \\nsouth-west of the line default. (Note that this is an idealized example. As \\nwe discuss later, the sort of perfect separation indicated in Figure 5.1 is \\nnot usually possible.) \\nThere is some uncertainty about where the line in Figure 5.1 should \\nbe positioned. We could move it sideways or change its gradient a little \\nand still achieve perfect separation.  SVM handles this uncertainty by \\nusing a pathway. The optimal pathway is the one that has maximum \\nwidth. The line separating the observations is the middle of the path-\\nway.   \\nFigure 5.2 shows the optimal pathway for the data in Table 5.1. Note \\nthat adding more observations that are correctly classified by the path-\\nway (i.e., good loans to the north-east of the pathway or defaulting loans \\nto the south-west) does not affect the optimal pathway. The critical \\npoints are those on the edge of the pathway. These are referred to as \\nSVM                                                                                                                                                 105 \\n \\n \\n \\nsupport vectors. For the data we are considering the support vectors are \\nthose defined by the third, seventh, and ninth observations. \\n \\nFigure 5.1   Data set in Table 5.1. Circles represent loans that defaulted \\nwhile squares represent good loans \\n \\n \\n \\nFigure 5.2   Optimal pathway for data in Table 5.1 \\n \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\nAdjusted Credit Score (= FICO−620)\\nIncome (\\'000s)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\nAdjusted Credit Score (= FICO−620)\\nIncome (\\'000s)\\n106                                                                                                                                     Chapter 5 \\n \\n \\n \\nThe solid line in Figure 5.2 is the center of the pathway. It is the line \\nthat would be used to separate new observations into those that are \\npredicted to be good loans and those that are expected to default.   \\nTo show how the optimal pathway can be determined in the two-\\nfeature case, we suppose that the features are 𝑥1 and 𝑥2, and the equa-\\ntions for the upper and lower edges of the pathway are:1 \\n  \\n𝑤1𝑥1 + 𝑤2𝑥2 = 𝑏𝑢                                           (5.1) \\nand  \\n𝑤1𝑥1 + 𝑤2𝑥2 = 𝑏𝑑                                           (5.2) \\n \\nwhere 𝑤1, 𝑤2, 𝑏𝑢, and 𝑏𝑑 are constants.  These definitions are illustrated \\nin Figure 5.3.  \\n \\nFigure 5.3   Calculation of path width in general situation \\n \\n \\nDefining the angle \\uf071 as indicated, we see from the lower triangle in-\\nvolving \\uf071 in Figure 5.3 that the width of the path, P, can be written as \\n                                                           \\n1 Note that this material is often presented with bu and bd having the opposite sign \\nto that here. This is just a notational issue and makes no difference to the model.  \\nx1\\nx2\\n0\\n0\\nw1x1+w2x2=bu\\nw1x1+w2x2=bd\\n𝑏𝑑\\n𝑤1\\n𝑏𝑢\\n𝑤1\\n𝑏𝑑\\n𝑤2\\n𝑏𝑢\\n𝑤2\\n\\uf071\\n\\uf071\\nSVM                                                                                                                                                 107 \\n \\n \\n \\n𝑃= (𝑏𝑢\\n𝑤1\\n−𝑏𝑑\\n𝑤1\\n) sin θ = (𝑏𝑢−𝑏𝑑\\n𝑤1\\n) sin θ \\nso that \\nsin θ =\\n𝑃𝑤1\\n𝑏𝑢−𝑏𝑑\\n \\n \\nFrom the upper triangle involving \\uf071\\uf02c\\uf020the width of the path, P, can al-\\nso be written as  \\n \\n𝑃= (𝑏𝑢\\n𝑤2\\n−𝑏𝑑\\n𝑤2\\n) cos θ = (𝑏𝑢−𝑏𝑑\\n𝑤2\\n) cos θ \\nso that \\ncos θ =\\n𝑃𝑤2\\n𝑏𝑢−𝑏𝑑\\n \\n \\nBecause sin2 θ + cos2 θ = 1, \\n \\n( 𝑃𝑤1\\n𝑏𝑢−𝑏𝑑\\n)\\n2\\n+ ( 𝑃𝑤2\\n𝑏𝑢−𝑏𝑑\\n)\\n2\\n= 1 \\nso that \\n𝑃=\\n𝑏𝑢−𝑏𝑑\\n√𝑤1\\n2 + 𝑤2\\n2 \\n \\nWe can multiply 𝑤1, 𝑤2, 𝑏𝑢, and 𝑏𝑑  by the same constant without \\nchanging the equations (5.1) and (5.2) for the upper and lower bounda-\\nry of the path. We can choose this constant so that  𝑏𝑢−𝑏𝑑 = 2. We can \\nthen set \\n \\n𝑏𝑢= 𝑏+ 1                                                  (5.3) \\n \\nand \\n \\n𝑏𝑑= 𝑏−1                                                  (5.4) \\n \\nThe equation for the line defining the middle of the pathway becomes  \\n \\n𝑤1𝑥1 + 𝑤2𝑥2 = 𝑏   \\n \\nand the width of the pathway becomes \\n \\n108                                                                                                                                     Chapter 5 \\n \\n \\n \\n𝑃=\\n2\\n√𝑤1\\n2 + 𝑤2\\n2 \\n \\nThis shows that the width of the path can be maximized by minimiz-\\ning by √𝑤1\\n2 + 𝑤2\\n2, or equivalently 𝑤1\\n2 + 𝑤2  \\n2 , subject to the constraint \\nthat the pathway separates the observations into the two classes and  \\nequations (5.1) to (5.4) are satisfied. \\nFor the example in Table 5.1, we can set 𝑥1  equal to income and 𝑥2 \\nequal to credit score.   All good loans must be to the north-east of the \\npathway while all defaulting loans must be to the south-west of the \\npathway. This means that, if a loan is good, its income and credit score \\nmust satisfy \\n \\n𝑤1𝑥1 + 𝑤2𝑥2 ≥𝑏+ 1 \\n \\nwhile if the loan defaults they must satisfy \\n \\n𝑤1𝑥1 + 𝑤2𝑥2 ≤𝑏−1 \\n \\nFrom Table 5.1, we must therefore have \\n \\n 30𝑤1 + 40𝑤2 ≤𝑏−1 \\n 55𝑤1 + 30𝑤2 ≤𝑏−1 \\n 63𝑤1 + 30𝑤2 ≤𝑏−1 \\n 35𝑤1 + 80𝑤2 ≤𝑏−1 \\n28𝑤1 + 100𝑤2 ≤𝑏−1  \\n140𝑤1 + 30𝑤2 ≥𝑏+ 1  \\n100𝑤1 + 30𝑤2 ≥𝑏+ 1  \\n  95𝑤1 + 90𝑤2 ≥𝑏+ 1  \\n64𝑤1 + 120𝑤2 ≥𝑏+ 1  \\n  63𝑤1 + 150𝑤2 ≥𝑏+ 1    \\n \\nMinimizing 𝑤1\\n2 + 𝑤2  \\n2  subject to these constraints gives b = 5.054, w1 \\n= 0.05405, w2 = 0.02162. The middle of pathway in Figure 5.2 is there-\\nfore \\n \\n0.05405𝑥1 + 0.02162𝑥2 = 5.054 \\n \\nSVM                                                                                                                                                 109 \\n \\n \\n \\nThis is the line that would be used to separate good loans from bad \\nloans. The width of the pathway, P, is 34.35.  \\n \\nThe analysis we have given can be extended to more than two fea-\\ntures. If we have m features, the objective function to be minimized is  \\n \\n∑𝑤𝑗\\n2\\n𝑚\\n𝑗=1\\n \\n \\nIf xij is the value of the jth feature for the ith observation, the constraint \\nthat must be satisfied is \\n \\n∑𝑤𝑗𝑥𝑖𝑗\\n𝑚\\n𝑗=1\\n≥𝑏+ 1 \\n \\nwhen there is a positive outcome for observation i (which in our exam-\\nple occurs when the loan does not default) and \\n \\n∑𝑤𝑗𝑥𝑖𝑗\\n𝑚\\n𝑗=1\\n≤𝑏−1 \\n \\nWhen there is a negative outcome for observation i (which in our ex-\\nample occurs when a loan defaults). \\nMinimizing the objective function subject to the constraints involves \\na standard numerical procedure known as quadratic programming. \\n \\n \\n5.2   Modification for Soft Margin \\n \\nWhat we have considered so far is referred to as hard margin classi-\\nfication because the pathway divides the observations perfectly with no \\nviolations. In practice, there are usually some violations (i.e., observa-\\ntions that are within the pathway or on the wrong side of the pathway) \\nThe problem is then referred to as soft margin classification and there is \\na trade-off between the width of the pathway and the severity of viola-\\ntions. As the pathway becomes wider the violations become greater.  \\nContinuing with the notation that 𝑥𝑖𝑗 is the value of the jth feature \\nfor the ith observation, we define: \\n \\n110                                                                                                                                     Chapter 5 \\n \\n \\n \\n𝑧𝑖= max (𝑏+ 1 −∑𝑤𝑗𝑥𝑖𝑗\\n𝑚\\n𝑗=1\\n, 0)    if positive outcome  \\n \\n𝑧𝑖= max (∑𝑤𝑗𝑥𝑖𝑗\\n𝑚\\n𝑗=1\\n−(𝑏−1), 0)   if negative outcome \\n \\nThe variable zi is a measure of the extent to which observation i violates \\nthe hard margin conditions at the end of the previous section.  \\nThe machine learning algorithm involves a hyperparameter, C, which \\ndefines the trade-off between the width of the pathway and violations. \\nThe objective function to be minimized is  \\n \\n𝐶∑𝑧𝑖\\n𝑛\\n𝑖=1\\n+ ∑𝑤𝑗\\n2\\n𝑚\\n𝑗=1\\n \\n \\nwhere n is the number of observations.  Like the hard margin case, this \\ncan be set up as a quadratic programming problem.  \\nTo illustrate the soft margin classification problem, we change the \\nexample in Table 5.1 so that (a) the adjusted credit score for the second \\nloan is 140 rather than 30 and (b) the income for the eighth loan is 60 \\nrather than 95. The new data is in Table 5.2.  \\n \\nTable 5.2   Loan data set for illustrating soft margin classification \\n \\nCredit \\nscore \\nAdjusted credit \\nscore \\nIncome \\n(‘000s) \\nDefault =0; \\ngood loan=1 \\n660 \\n 40 \\n  30 \\n0 \\n650 \\n          140 \\n  55 \\n0 \\n650 \\n 30 \\n  63 \\n0 \\n700 \\n 80 \\n  35 \\n0 \\n720 \\n          100 \\n  28 \\n0 \\n650 \\n 30 \\n         140 \\n1 \\n650 \\n 30 \\n100 \\n1 \\n710 \\n 90 \\n  60 \\n1 \\n740 \\n          120 \\n  64 \\n1 \\n770 \\n          150 \\n  63 \\n1 \\n \\nSVM                                                                                                                                                 111 \\n \\n \\n \\nFigure 5.4 shows the new data together with the optimal path when \\nC = 0.001.  It shows that one observation (the first one) is to the south- \\nwest of the pathway and another observation (the sixth one) is to the \\nnorth-east of the pathway. Three observations (the fifth, seventh and \\ntenth) are support vectors on the pathway edges and the remaining five \\nobservations are within the pathway.  \\nIt is the center solid line in Figure 5.4 that defines the way loans are \\nclassified.  Only one observation (the second one) is misclassified by the \\nSVM algorithm. \\nThe results for different values of C are shown in Table 5.3. When C = \\n0.001, as we have just seen, one loan (10% of the total) is misclassified.  \\nIn this case, w1 = 0.0397, w2 = 0.0122, and b = 3.33. When C is decreased \\nto 0.0003 so that violations are less costly, two loans (20% of the total) \\nare misclassified. When it is decreased again to 0.0002, three loans \\n(30% of the total) are misclassified.  \\nThis baby example shows that SVM is a way of generating a number \\nof plausible boundaries for a data set. As with other machine learning \\nalgorithms, it is necessary to use a training set, validation set and a test \\nset to determine the best model and its errors.   \\n \\nFigure 5.4   Optimal pathway for the data in Table 5.2 when C=0.001 \\n(See Excel or Python file) \\n \\n \\n \\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\nAdjusted Credit Score (= FICO−620)\\nIncome (\\'000s)\\n112                                                                                                                                     Chapter 5 \\n \\n \\n \\nTable 5.3   Results of applying SVM to the data in Table 5.2. See Python \\nresults. Excel results are not as accurate. \\n \\nC \\nw1 \\nw2 \\nb \\nLoans mis-\\nclassified \\nWidth of \\npathway \\n0.01 \\n0.054 \\n0.022 \\n5.05 \\n10% \\n   34.4 \\n0.001 \\n0.040 \\n0.012 \\n3.33 \\n10% \\n   48.2 \\n0.0005 \\n0.026 \\n0.010 \\n2.46 \\n10% \\n   70.6 \\n0.0003 \\n0.019 \\n0.006 \\n1.79 \\n20% \\n   102.2 \\n0.0002 \\n0.018 \\n0.003 \\n1.69 \\n30% \\n106.6 \\n \\n \\n5.3   Non-linear Separation \\n \\nSo far, we have assumed that the pathway separating observations \\ninto two classes is a linear function of the feature values. We now inves-\\ntigate how this assumption can be relaxed.  \\nFigure 5.5 provides an example of a situation where there are only \\ntwo features, 𝑥1 and 𝑥2. It appears that a non-linear boundary would \\nwork better than a linear boundary.  The general approach to finding a \\nnon-linear boundary is to transform the features so that the linear \\nmodel presented so far in this chapter can be used.  \\n \\nFigure 5.5   Example of data where a non-linear separation would be \\nappropriate (Circles and stars represent observations in different clas-\\nses.) \\n \\n \\n \\nAs a simple example of this approach, suppose that we introduce A, \\nthe age (yrs.) of the borrower, as a feature for classifying loans. We sup-\\nx2\\nx1\\nSVM                                                                                                                                                 113 \\n \\n \\n \\npose that for A < 23 and A > 63, the impact of age is negative (so that it \\nis more likely that the loan will default) while for 23 ≤ A ≤ 63 the impact \\nof age is positive. A linear pathway will not handle the age variable well.   \\nOne idea is to replace A by a new variable: \\n \\n𝑄= (43 −𝐴)2 \\n \\nIf the dependence of creditworthiness on age is closer to quadratic than \\nlinear the transformed variable Q will work better as a feature than the \\noriginal variable A.   \\nWe can extend this idea by creating several features that are powers \\nof the existing features. For example, for loan classification we could \\ncreate features that are the square of income, the cube of income, the \\nfourth power of income and so on. \\nAnother way of transforming features to achieve linearity is by using \\nwhat is known as a Gaussian radial bias function (RBF). Suppose that \\nthere are m features. We choose a number of landmarks in m-\\ndimensional space. These could (but do not need to) correspond to ob-\\nservations on the features.  For each landmark, we define a new feature \\nthat captures the distance of an observation from the landmark. Sup-\\npose that the values of the features at a landmark are ℓ1, ℓ2,…, ℓm and \\nthat the values of the features for an observation are x1, x2,…., xm.  The \\ndistance of the observation from the landmark is  \\n \\n𝐷= √∑(𝑥𝑗−ℓ𝑗)2\\n𝑚\\n𝑗=1\\n \\n \\nand the value of the new RBF feature that is created for the observation \\nis \\n \\nexp (−γ𝐷2) \\n \\nThe parameter \\uf067 determines how the value of the RBF feature for an \\nobservation declines as its distance from the landmark increases. (As \\uf067 \\nincreases, the decline becomes more rapid.)  \\nUsing many landmarks or introducing powers of the features as new \\nfeatures will usually lead to linear separation in a situation such as that \\n114                                                                                                                                     Chapter 5 \\n \\n \\n \\nin Figure 5.5. The downside is that the number of features increases, \\nand the model becomes more complex.2  \\n \\n \\n5.4   Predicting a Continuous Variable \\n \\nSVM can be used to predict the value of a continuous variable. It is \\nthen referred to as SVM regression.  Instead of trying to fit the largest \\npossible pathway between two classes while limiting violations, we try \\nto find a pathway with a pre-specified width that contains as many of \\nthe observations as possible.  \\nConsider a simple situation where a target, y, is being estimated from \\nonly one feature, x. The value of the target is on the vertical axis and the \\nvalue of the feature is on the horizontal axis.  The vertical half-width of \\nthe pathway is specified by a hyperparameter, e. We assume that the \\ncenter of the path is  \\n \\n𝑦= 𝑤𝑥+ 𝑏 \\n \\nThe situation is illustrated in Figure 5.6.   If an observation, i, lies \\nwithin the pathway, there is considered to be no error. If it lies outside \\nthe pathway, the error, zi, is calculated as the vertical distance from the \\nedge of the pathway. We could choose the pathway to minimize \\n \\n𝐶∑𝑧𝑖\\n𝑛\\n𝑖=1\\n+ 𝑤2 \\n \\nwhere C is a hyperparameter.  The w2 term is a form of regularization. It \\nis not necessary when there is only one feature but becomes more rele-\\nvant as the number of features is increases. \\nWhen there are m features with values xj (1 ≤ j ≤ m), the pathway is \\nformed by two hyperplanes separated vertically by 2e.3 The equations \\nof the hyperplanes are \\n \\n                                                           \\n2 The complexity can be reduced by what is known as the kernel trick. See, for ex-\\nample, J. H. Manton and P.-O. Amblard, “A primer on reproducing Hilbert spaces,” \\nANDY LANDU NGOMA \\n3 For example, when there are two features there is a three-dimensional relation-\\nship between the target and the features and the pathway consists of two parallel \\nplanes. When there are m (>2) features the relationship between the target and the \\nfeatures is in m+1 dimensional space.   \\nSVM                                                                                                                                                 115 \\n \\n \\n \\n𝑦= ∑𝑤𝑗𝑥𝑗+ 𝑏+ 𝑒\\n𝑚\\n𝑗=1\\n \\nand \\n𝑦= ∑𝑤𝑗𝑥𝑗+ 𝑏−𝑒\\n𝑚\\n𝑗=1\\n \\n \\nThe objective function to be minimized is \\n \\n𝐶∑𝑧𝑖\\n𝑛\\n𝑖=1\\n+ ∑𝑤𝑗\\n2\\n𝑚\\n𝑗=1\\n \\n \\nHere the regularization aspect of the second term becomes clear. Its aim \\nis to prevent large positive and negative weights.4 \\n \\n \\nFigure 5.6   Pathway for SVM regression \\n \\n \\n \\nConsider the task of estimating the price of a house from Living Area \\n(square feet). We are searching for a pathway such as that shown in \\nFigure 5.6.  Figure 5.7 shows the result for the training set when e = \\n50,000 and C = 0.01 while Figure 5.8 shows results for the training set \\n                                                           \\n4 Its effect is similar to that of the extra term included in a Ridge regression (see \\nChapter 3). \\ny=wx+b\\ny=wx+b+e\\ny=wx+b−e\\ne\\ne\\ny\\nx\\n116                                                                                                                                     Chapter 5 \\n \\n \\n \\nwhen e = 100,000 and C = 0.1. The line used for predicting house prices \\nis the solid line in the figures. \\n \\nFigure 5.7   Results for training set when house prices are predicted \\nfrom living area with e = 50,000 and C = 0.01 (See SVM regression Excel \\nfile for calculations) \\n \\n \\nFigure 5.8   Results for training set when house prices are predicted \\nfrom living area with e = 100,000 and C = 0.1 (See SVM regression Excel \\nfile for calculations) \\n \\nSVM                                                                                                                                                 117 \\n \\n \\n \\nThe two SVM regression lines have slightly different biases and \\nweights. In Figure 5.7 the b = 21,488 and w = 104.54, whereas in Figure \\n5.8 the b = 46,072 and w = 99.36. (This means that the slope of the re-\\ngression line in Figure 5.7 is slightly greater than in Figure 5.8.)  \\nWhich is the better model? As usual this must be determined out-of-\\nsample. Table 5.4 shows that the standard deviation of the prediction \\nerrors for the validation set is slightly lower for the model in Figure 5.7.  \\nIn this case, plain vanilla linear regression has better prediction errors \\nthan both models. This is not too surprising as the regularization inher-\\nent in the SVM objective function does not improve the model when \\nthere is only one feature.  (But see Exercise 5.11 for an extension of the \\nexample). \\n \\n \\nTable 5.4   Validation set results for the two SVM regression models \\nconsidered \\n \\n \\nStandard deviation of \\nprediction error ($) \\nModel in Figure 5.7 \\n58,413 \\nModel in Figure 5.8 \\n58,824 \\nLinear regression \\n57,962 \\n \\n \\nTo summarize, the SVM approach is different from a simple linear \\nregression because \\n \\n\\uf0b7 The relationship between the target and the features is repre-\\nsented by a pathway rather than a single line. \\n\\uf0b7 The prediction error is counted as zero when an observation lies \\nwithin the pathway. \\n\\uf0b7 Errors for observations outside the pathway are calculated as the \\ndifference between the target value and the closest point in the \\npathway that is consistent with the feature values. \\n\\uf0b7 There is some regularization built into the objective function as \\nexplained above. \\n \\nWe can extend SVM regression to allow the pathway to be non-linear \\nby creating new features as functions of the original features in a simi-\\nlar way to that discussed in Section 5.3 for non-linear classification. \\n \\n \\n118                                                                                                                                     Chapter 5 \\n \\n \\n \\nSummary \\n \\nSVM seeks to classify observations by deriving a pathway between \\nobservations in the training set.  The center of the pathway is the \\nboundary used to assign new observations to a class. In the simplest \\nsituation, the pathway is linear function of the features and all observa-\\ntions are correctly classified. This is referred to as a hard margin classi-\\nfication.  However, perfect separation is usually not possible and alter-\\nnative boundaries can be developed by considering alternative trade-\\noffs between the width of the pathway and the extent of violations.  \\nBy working with functions of the feature values rather than the fea-\\nture values themselves the pathway for separating the observations \\ninto two classes can be made non-linear. We have discussed the possi-\\nbility of creating new features that are squares, cubes, fourth powers, \\netc. of the feature values. Alternatively, landmarks can be created in the \\nfeature space with new features that are functions of the distances of an \\nobservation from the landmarks.  \\nSVM regression uses the ideas underlying SVM classification to pre-\\ndict the value of a continuous variable.  A pathway through the observa-\\ntions for predicting the target is created. If the value of the target for an \\nobservation is inside the pathway there is assumed to be no prediction \\nerror. If it is outside the pathway the prediction error is the difference \\nbetween the target value and what the target value would be if it were \\njust inside the pathway. The width of the pathway (measured in the di-\\nrection of the target value) is specified by the user. There is a trade-off \\nbetween the average pathway prediction error and the amount of regu-\\nlarization. \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n5.1 \\nWhat is the objective in SVM classification? \\n5.2 \\nWhat is the difference between hard and soft margin classifica-\\ntion? \\n5.3  \\nWhat are the equations for the upper and lower edges of the \\npathway in linear classification with m features in terms of the \\nweights wj and the feature values xj? \\n5.4 \\nWhat happens to the width of the path as the cost assigned to vio-\\nlations increases? \\n5.5  \\nHow is the extent of the violation measured in soft margin linear \\nclassification? \\nSVM                                                                                                                                                 119 \\n \\n \\n \\n5.6 \\nHow is the methodology for linear classification extended to non-\\nlinear classification? \\n5.7 \\nWhat is a landmark and what is a Gaussian radial bias function \\n(RBF)? \\n5.8 \\nExplain the objective in SVM regression. \\n5.9 \\nWhat are the main differences between SVM regression and sim-\\nple linear regression? \\n \\n \\nEXERCISES \\n \\n5.10 Produce a table similar to Table 5.3 for the situation where the \\ndata in Table 5.1 is changed so that the third loan is good and the \\neighth loan defaults. \\n5.11 Use Sklearn.svm.LinearSVR to extend the SVM regression analysis \\nin Section 5.4 for the Iowa house price example so that other fea-\\ntures are considered.  \\n\\xa0\\n \\n121 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nChapter 6 \\n \\nSupervised Learning:  \\nNeural Networks \\n \\n \\n \\n \\n \\nArtificial Neural Networks (ANNs) are powerful machine learning \\nalgorithms that have found many applications in business and else-\\nwhere. The algorithms learn the relationships between targets and fea-\\ntures using a network of functions. Any continuous non-linear relation-\\nship can be approximated to arbitrary accuracy using an ANN. \\nIn this chapter, we first explain what an ANN is. We then move on to \\nprovide an application and explain extensions of the basic idea to what \\nare known as autoencoders, convolutional neural networks (CNNs) and \\nrecurrent neural networks (RNNs). \\n \\n \\n6.1     Single Layer ANNs \\n \\nConsider the problem we considered in Section 4.5 of predicting the \\nvalue of a house from just two features:  \\n \\n\\uf0b7 Overall quality \\n\\uf0b7 Living area (sq. ft.) \\n \\n122                                                                                                                                      Chapter 6 \\n \\n \\n \\nA simple ANN for doing this is shown in Figure 6.1.  It has three layers: \\nthe input layer consisting of the two features, an output layer consisting \\nof the house value and a hidden layer consisting of three neurons. The \\nparameters and variables in Figure 6.1 are as follows: \\n \\n\\uf0b7 𝑤𝑗𝑘 is a model parameter. It is the weight linking the jth feature to \\nthe kth neuron. As there are two features and three neurons, \\nthere are a total of 6 of these weights in Figure 6.1. (Only two are \\nmarked on the figure.)   \\n\\uf0b7 𝑢𝑘 is a model parameter. It is the weight linking the kth neuron to \\nthe target.  \\n\\uf0b7 𝑉𝑘 is the value at neuron k.  It is calculated by the model. \\n \\nFigure 6.1   A simple single-hidden-layer ANN for predicting the value \\nof a house \\n \\n \\n \\n \\nFunctions are specified relating the 𝑉𝑘 to the 𝑥𝑗 and the value of the \\nhouse, H, to the 𝑉𝑘. The key point is that the ANN does not relate H di-\\nrectly to the 𝑥𝑗. Instead it relates H to the 𝑉𝑘 and the 𝑉𝑘 to the 𝑥𝑗.  The \\nfunctions that define these relationships are referred to as activation \\nfunctions.  A popular activation function is the sigmoid function which \\nwas introduced in Chapter 3 in connection with logistic regression (see \\nFigure 3.10).1 This is the function \\n \\n𝑓(𝑦) =\\n1\\n1 + 𝑒−𝑦 \\n                                                           \\n1 Examples of other popular activation functions are: \\n(a) the hyperbolic tangent function tanh(𝑦) = (𝑒2𝑦−1) (𝑒2𝑦+ 1)\\n⁄\\n which \\ngives values between −1 and +1; and \\nthe Relu function max(y, 0). \\nLiving area, x2\\nHouse Value, H\\nFeatures\\nNeurons\\nTarget\\nV1\\nOverall quality, x1\\nV2\\nV3\\nw11\\nw12\\nu1\\nu2\\nu3\\nNeural Networks                                                                                                                       123 \\n \\n \\n \\nFor all values of the argument y, the function lies between zero and one.   \\nWe set \\n \\n𝑉𝑘= 𝑓(𝑎𝑘+ 𝑤1𝑘𝑥1 + 𝑤2𝑘𝑥2) \\n \\nwhere the a’s are biases and the w’s are weights.  This means that \\n \\n𝑉1 =\\n1\\n1 + exp (−𝑎1 −𝑤11𝑥1 −𝑤21𝑥2) \\n \\n𝑉2 =\\n1\\n1 + exp (−𝑎2 −𝑤12𝑥1 −𝑤22𝑥2) \\n \\n𝑉3 =\\n1\\n1 + exp (−𝑎3 −𝑤13𝑥1 −𝑤23𝑥2) \\n \\nTo relate a numerical target such as H to the 𝑉𝑘, we typically use a \\nlinear activation function so that \\n \\n𝐻= 𝑐+ 𝑢1𝑉1 + 𝑢2𝑉2 + 𝑢3𝑉3 \\n \\n where c is a bias and the u’s are weights. \\nThe model in Figure 6.1 has six weights 𝑤𝑗𝑘, three biases 𝑎𝑘, three \\nweights 𝑢𝑘, and one additional bias c for a total of 13 parameters.  The \\nobjective is to choose the parameters so that the predictions given by \\nthe network are as close as possible to the target values for the training \\nset. Typically, this is done by minimizing an objective function such as \\nmean squared error (mse) or mean absolute error (mae) across all ob-\\nservations. This objective function is referred to as a cost function.2  \\nFor another simple example of an ANN consider the problem in \\nChapter 3 of classifying loans into “good” and “default” categories using \\nfour features:  \\n \\n\\uf0b7 Credit score \\n\\uf0b7 Income ($’000s) \\n\\uf0b7 Debt to income ratio  \\n\\uf0b7 Home ownership (1 = owns; 0 = rents) \\n \\n                                                           \\n2 The cost function terminology is used throughout machine learning when a nu-\\nmerical value is being predicted. For example, the mse in a linear regression is re-\\nferred to as a cost function. \\n124                                                                                                                                      Chapter 6 \\n \\n \\n \\nAn ANN with one hidden layer is shown in Figure 6.2. This works in \\nthe same way as the one in Figure 6.1 except that we do not use a linear \\nactivation function to relate the target to the 𝑉𝑘. We require the target \\nto be between zero and one because it is a probability and so it is natu-\\nral to use another sigmoid function to calculate it from the Vk.  The \\nprobability of a good loan is given by \\n \\n𝑄=\\n1\\n1 + exp (−𝑐−𝑢1𝑉1 −𝑢2𝑉2 −𝑢3𝑉3) \\n \\nIn Figure 6.2, the number of parameters is 19. \\nThe objective function for a neural network, such as that in Figure \\n6.2 where a probability is being predicted, can be the one based on max-\\nimum likelihood that we introduced in connection with logistic regres-\\nsion in equation (3.8). \\n \\nFigure 6.2   A simple single-hidden-layer ANN for classifying loans \\n \\n \\n \\nThe ANNs in Figures 6.1 and 6.2 have a single hidden layer with \\nthree neurons.  In practice, single-hidden-layer ANNs usually have \\nmany more than three neurons. There is a result known as the universal \\napproximation theorem which states that any continuous function can \\nbe approximated to arbitrary accuracy with an ANN that has a single \\nhidden layer.3 However, this may require a very large number of neu-\\nrons and it can be more computationally efficient to use multiple hidden \\nlayers. \\n                                                           \\n3 See K. Hornik, “Approximation capabilities of multilayer feedforward networks,” \\nNeural Networks, 1991, 4, 251−257.  \\ndebt to income, x3\\nProbability of \\ngood loan, Q\\nFeatures\\nNeurons\\nTarget\\nV1\\ncredit score, x1\\nincome, x2\\nowns/rents, x4\\nV2\\nV3\\nw11\\nw12\\nu1\\nu2\\nu3\\nNeural Networks                                                                                                                       125 \\n \\n \\n \\n \\n6.2     Multi-layer ANNs \\n \\nFigure 6.3 shows the general configuration of a multi-layer ANN. In \\neach of Figure 6.1 and 6.2 there is one set of intermediary variables (i.e., \\none hidden layer) between the features and the target. In Figure 6.3, \\nthere are a total of L hidden layers and therefore L + 1 sets of biases and \\nweights. We have labeled the feature values, 𝑥𝑗 (1 ≤ j ≤ m) similarly to \\nbefore. We assume K neurons per layer. The values at the neurons com-\\nprising the lth layer are labeled 𝑉𝑙𝑘 (1 ≤ k ≤ K; 1 ≤ l ≤ L). \\n As indicated in Figure 6.3 there can be multiple targets in the output \\nlayer.  The objective function can then be set equal to the sum of the \\n(possibly weighted) objective functions that would be used for each \\ntarget. (For targets that are probabilities, the maximum likelihood ob-\\njective function in equation (3.8) can have its sign changed so that it is \\nan expression to be minimized rather than maximized.)   \\n \\nFigure 6.3   A multi-layer neural network \\n \\n \\n \\nThere are weights associated with each of the lines in Figure 6.3.  Ac-\\ntivation functions are used to: \\n1. Relate values at the neurons of the first hidden layer to the fea-\\nture values, i.e., to relate the 𝑉1𝑘 to the 𝑥𝑗 \\n2. Relate the values at neurons in hidden layer l+1 to the values at \\nneurons in hidden layer l (1 ≤ l ≤ L−1) \\nv 1,1\\nv 1,2\\nv 1,3\\nv 1,5\\nv 1,4\\nx1\\nx2\\nx3\\nv 2,1\\nv 2,2\\nv 2,3\\nv 2,5\\nv 2,4\\nv 3,1\\nv 3,2\\nv 3,3\\nv 3,5\\nv 3,4\\nv L,1\\nv L,2\\nv L,3\\nv L,5\\nv L,4\\ny1\\ny 2\\ny3\\nTargets\\nFeatures\\nMultiple layers of neurons\\n……\\n……\\n……\\n……\\n……\\n……\\n……\\n……\\n……\\n126                                                                                                                                      Chapter 6 \\n \\n \\n \\n3. Relate the target values to the values in the final hidden layer (i.e., \\nto relate the y’s to the 𝑉𝐿𝑘) \\nThe sigmoid function, used in the way described in the previous sec-\\ntion, is a popular choice for the activation function for 1 and 2. In the \\ncase of 3, as explained in the previous section, linear activation func-\\ntions are usually used for numerical targets while the sigmoid function \\ncan be used for classification.   \\nThe number of hidden layers and number of neurons per hidden \\nlayer necessary to solve a particular problem is usually found by trial \\nand error.  Typically, layers and neurons are increased until it is found \\nthat further increases produce little increase in accuracy.  \\nA neural network can easily give rise to a very large number of mod-\\nel parameters. If there are F features, H hidden layers, M neurons in each \\nhidden layer, and T targets there are \\n \\n(\\n1)\\n(\\n1)(\\n1)\\n(\\n1)\\nF\\nM\\nM M\\nH\\nM\\nT\\n\\uf02b\\n\\uf02b\\n\\uf02b\\n\\uf02d\\n\\uf02b\\n\\uf02b\\n \\n \\nparameters in total.  For example, in a four-feature model with three \\nhidden layers, 80 neurons per layer and one target there are 13,441 \\nparameters. This naturally leads to over-fitting concerns, as we discuss \\nlater. \\n \\n \\n6.3     Gradient Descent Algorithm \\n \\nWhen neural networks are used, the minimization of the objective \\nfunction is accomplished using the gradient descent algorithm. We \\nbriefly outlined how this works in Chapter 3. First, an initial set of pa-\\nrameter values is chosen. An iterative procedure is then carried out to \\ngradually improve the objective function by changing these parameters. \\nTo illustrate the gradient descent algorithm, we take a simple exam-\\nple. Consider again the data introduced in Table 1.1 of Chapter 1 for sal-\\nary as a function of age. This is reproduced in Table 6.1. We assume a \\nvery simple (and not particularly good) linear model \\n \\n𝑦= 𝑏𝑥+ 𝜀 \\n \\nwhere y is salary, x is age, and  𝜀 is the error.  There is only one parame-\\nter b. The mean squared error, E, is given by \\n \\nNeural Networks                                                                                                                       127 \\n \\n \\n \\n𝐸= 1\\n10 ∑(𝑦𝑖−𝑏𝑥𝑖)2                            (6.1)\\n10\\n𝑖=1\\n \\n \\nwhere 𝑥𝑖 and 𝑦𝑖 are age and salary for the ith observation. \\n \\n \\nTable 6.1   Salaries for a random sample of ten people working in a par-\\nticular profession in a certain area \\n \\nAge (years) \\nSalary ($’000) \\n25 \\n135 \\n55 \\n260 \\n27 \\n105 \\n35 \\n220 \\n60 \\n240 \\n65 \\n265 \\n45 \\n270 \\n40 \\n300 \\n50 \\n265 \\n30 \\n105 \\n \\nThe value of the parameter b that minimizes E can of course be cal-\\nculated analytically, as we explained in Chapter 3. Here we show how \\nthe gradient descent algorithm can be used. Figure 6.4 shows the mean \\nsquared error, E, as a function of b. The aim of the algorithm is to find \\nthe value of b at the bottom of the valley in Figure 6.4. \\nWe might arbitrarily set b = 1 initially. Using calculus, it can be \\nshown that the gradient of E with respect to b is4  \\n \\n−1\\n5 ∑𝑥𝑖(𝑦𝑖−𝑏𝑥𝑖)                                         (6.2)\\n10\\n𝑖=1\\n \\n \\nSubstituting b = 1 and using the values of 𝑥𝑖 and 𝑦𝑖 in Table 6.1, this \\nformula gives −15,986.2. This means that when b increases by a small \\namount e, E increases by −15,986.2 times e.  \\n                                                           \\n4 Without knowing any calculus, we could calculate this gradient by  \\n\\uf0b7 \\nsubstituting b = 1.01 into equation (6.1) to get 𝐸+, \\n\\uf0b7 \\nsubstituting b = 0.99 into equation (6.1) to get 𝐸− \\n\\uf0b7 \\ncalculating the gradient as (𝐸+ −𝐸−) (2 × 0.01)\\n⁄\\n \\n \\n128                                                                                                                                      Chapter 6 \\n \\n \\n \\nFigure 6.4   Mean squared error as a function of the value of the pa-\\nrameter b \\n \\n \\n \\nOnce we have calculated the gradient at b = 1, we take a step down \\nthe valley. The size of the step is referred to as the learning rate. The \\nnew value of b is calculated from the old value of b as \\n \\n𝑏new = 𝑏old −learning rate × gradient               (6.3) \\n \\nIn our example, we choose a learning rate equal to 0.0002 so that we \\nchange b = 1 to \\n \\n𝑏= 1 −0.0002 × (−15,986.2) = 4.1972 \\n \\nWe then calculate the gradient from equation (6.2) when b = 4.1972. \\nThis turns out to be −2,906.9. Using equation (6.3) we calculate a new \\nvalue of b on the second iteration to be \\n \\n𝑏= 4.1972 −0.0002 × (−2906.9) = 4.7786 \\n \\nWe continue in this way, improving the value of b at each step. As in-\\ndicated in Table 6.2, the value of b quickly converges to 4.9078 which \\n(as a simple linear regression verifies) is the value that minimizes E. \\n \\n \\n0\\n5,000\\n10,000\\n15,000\\n20,000\\n25,000\\n30,000\\n35,000\\n40,000\\n0\\n2\\n4\\n6\\n8\\n10\\nb\\nMean Squared \\nError, E\\nNeural Networks                                                                                                                       129 \\n \\n \\n \\nTable 6.2   Value of b in successive iterations when learning rate is \\n0.0002 \\n \\nIteration \\nValue of b \\nGradient \\nChange in b \\n0 \\n1.0000 \\n−15,986.20 \\n+3.1972 \\n1 \\n4.1972 \\n  −2,906.93 \\n+0.5814 \\n2 \\n4.7786 \\n     −528.60 \\n+0.1057 \\n3 \\n4.8843 \\n       −96.12 \\n+0.0192 \\n4 \\n4.9036 \\n       −17.48 \\n+0.0035 \\n5 \\n4.9071 \\n         −3.18 \\n+0.0006 \\n6 \\n4.9077 \\n        −0.58 \\n+0.0001 \\n7 \\n4.9078 \\n        −0.11 \\n+0.0000 \\n8 \\n4.9078 \\n        −0.02 \\n+0.0000 \\n9 \\n4.9078 \\n          0.00 \\n+0.0000 \\n \\n \\nIn Table 6.2, we chose a learning rate of 0.0002 which proves to \\nwork well.  A learning rate that is too low will lead to very slow conver-\\ngence. A learning rate that is too high will lead to no convergence at all. \\nTables 6.3 and 6.4 illustrate this by showing what happens when learn-\\ning rates of 0.00001 and 0.0005 are used. As we explain later, methods \\nfor optimizing learning rates have been developed. \\n \\n \\nTable 6.3   Value of b in successive iterations when learning rate is \\n0.00001 \\n \\nIteration \\nValue of b \\nGradient \\nChange in b \\n0 \\n1.0000 \\n−15,986.20 \\n+0.1599 \\n1 \\n1.1599 \\n−15,332.24 \\n+0.1533 \\n2 \\n1.3132 \\n−14,705.03 \\n+0.1471 \\n3 \\n1.4602 \\n−14,103.47 \\n+0.1410 \\n4 \\n1.6013 \\n−13,526.53 \\n+0.1353 \\n5 \\n1.7365 \\n−12,973.18 \\n+0.1297 \\n6 \\n1.8663 \\n−12,442.48 \\n+0.1244 \\n7 \\n1.9907 \\n−11,933.48 \\n+0.1193 \\n8 \\n2.1100 \\n−11,445.31 \\n+0.1145 \\n9 \\n2.2245 \\n−10,977.10 \\n+0.1098 \\n \\n \\n130                                                                                                                                      Chapter 6 \\n \\n \\n \\nTable 6.4   Value of b in successive iterations when learning rate is \\n0.0005 \\n \\nIteration \\nValue of b \\nGradient \\nChange in b \\n0 \\n1.0000 \\n−15,986.20 \\n    +7.9931 \\n1 \\n8.9931 \\n  16,711.97 \\n    −8.3560 \\n2 \\n0.6371 \\n−17,470.70 \\n    +8.7353 \\n3 \\n9.3725 \\n  18,263.87 \\n    −9.1319 \\n4 \\n0.2405 \\n−19,093.05 \\n    +9.5465 \\n5 \\n9.7871 \\n  19,959.87 \\n    −9.9799 \\n6 \\n    −0.1929 \\n−20,866.05 \\n  +10.4330 \\n7 \\n10.2401 \\n  21,813.37 \\n  −10.9067 \\n8 \\n     −0.6665 \\n−22,803.69 \\n  +11.4018 \\n9 \\n10.7353 \\n  23,838.98 \\n  −11.9195 \\n \\n \\nMultiple Parameters  \\nWhen many parameters have to be estimated, all the parameter val-\\nues change on each iteration.  For the gradient descent algorithm to \\nwork efficiently, the feature values should be scaled as described in Sec-\\ntion 2.1.  The change in the value of a parameter, \\uf071, equals \\n \\n−Learning rate × gradient \\n \\nas before. In this case, the gradient used is the rate of change of the val-\\nue the objective function with respect to \\uf071. To use the language of calcu-\\nlus, the gradient is the partial derivative of E with respect to \\uf071. \\nSuppose that there are two parameters and at a particular point in \\nthe gradient descent algorithm, the gradient in the direction of one pa-\\nrameter is ten times the gradient in the direction of the other parame-\\nter. If the same learning rate is used for both parameters, the change \\nmade to the first parameter will be ten times as great as that made to \\nthe second parameter.  \\nWhen there are a large number of parameters, determining the gra-\\ndient applicable to each one is liable to be prohibitively time consuming. \\nLuckily a shortcut has been developed.5 This is known as backpropaga-\\ntion and involves working back from the end of the network to the be-\\n                                                           \\n5 See D. Rumelhart, G. Hinton, and R. Williams, “Learning internal representations \\nby error propagation,” Nature, 1986, 323, 533−536. \\nNeural Networks                                                                                                                       131 \\n \\n \\n \\nginning calculating the required partial derivatives. A technical note \\nexplaining this is on the author’s website: \\nANDY LANDU NGOMA \\nIt is also worth noting that the partial derivatives of the target with re-\\nspect to each of the features can be calculated from the neural network \\nby working forward through the network.6 \\n \\n \\n6.4    Variations on the Basic Method \\n \\nAs already indicated, neural networks learn much faster when the \\nfeatures being input are scaled (see Section 2.1). Also, some regulariza-\\ntion is usually desirable.  Similarly to linear regression, L1 regulariza-\\ntion involves adding a constant times the sum of the absolute values of \\nall the weights to the target; L2 regularization involves adding a con-\\nstant times the sum of the squares of all weights to the target. Analo-\\ngously to linear regression (see Chapter 3), L1 regularization zeroes out \\nsome weights whereas L2 regularization reduces the average magni-\\ntude of the weights. \\nThe gradient descent algorithm can lead to a local minimum.  Con-\\nsider for example the situation in Figure 6.5. If we start at point A, we \\ncould reach the local minimum at point B when the better (global) min-\\nimum is at point C. To speed up the learning process and attempt to \\navoid local minima several variations on the basic gradient descent al-\\ngorithm have been developed. For example,  \\n \\n\\uf0b7 Mini-batch stochastic gradient descent. This randomly splits the \\ntraining data set into small subsets known as mini-batches. In-\\nstead of using the whole training data to calculate gradients, it \\nupdates model parameters based on the gradients calculated \\nfrom a single mini-batch with each of the mini-batches being used \\nin turn. Because the algorithm estimates the gradient using a \\nsmall sample of the training data, it is faster. An epoch is a set of \\niterations that make one use of the whole training set. \\n\\uf0b7 Gradient descent with momentum. This calculates a gradient as an \\nexponentially decaying moving average of past gradients. This \\n                                                           \\n6 This assumes that the activation functions are differentiable everywhere. The sig-\\nmoid and hyperbolic tangent function do have this property. The relu function does \\nnot. Both backpropagation and the calculation of partial derivative involve an appli-\\ncation of the chain rule for differentiation. \\n132                                                                                                                                      Chapter 6 \\n \\n \\n \\napproach helps to speed up learning in any direction that has a \\nconsistent gradient.  \\n\\uf0b7 Gradient descent with adaptive learning rates. As Tables 6.2, 6.3, \\nand 6.4 illustrate it is important to choose a good learning rate. A \\nlearning rate that is too small will result in many epochs being \\nrequired to reach a reasonable result. A learning rate that is too \\nhigh may lead to oscillations and a poor result. Different model \\nparameters may benefit from different learning rates at different \\nstages of training.  A popular adaptive learning rate algorithm is \\nAdam which stands for Adaptive moment estimation. It uses both \\nmomentum and an exponentially decaying average of past \\nsquared gradients. \\n\\uf0b7  Learning rate decay. In addition, using adaptive learning rates it \\ngenerally makes sense to reduce the learning rate as the algo-\\nrithm progresses. This is known as learning rate decay. (The \\nshape of the curve in Figure 6.4 indicates that it would work well \\nfor that example.) To avoid local minima periodic increases in the \\nlearning rate are sometimes made. \\n\\uf0b7 Gradient descent with dropouts. Training can be faster if some \\nnodes, chosen at random from each hidden layer, are removed \\nfrom the network on each iteration. The number of iterations \\nnecessary for convergence is increased but this is more than off-\\nset by a reduction in the run time for each iteration \\n. \\nFigure 6.5   Situation where there is a local minimum at B \\n \\n \\n \\n \\nmse\\nParameter Value\\nA\\nB\\nC\\nNeural Networks                                                                                                                       133 \\n \\n \\n \\n6.5   The Stopping Rule \\n \\nIt might be thought that the algorithm should continue until the val-\\nues of the parameters can be improved no more, i.e., until we have \\nreached the bottom of the valley that defines the objective function, E, in \\nterms of the parameters. In the simple example we considered earlier, \\nthe algorithm does this. Indeed, the optimal value of b was found in Ta-\\nble 6.2 (to four places of decimals) after only seven iterations.  \\nIn practice, as we have pointed out, there can be tens of thousands of \\nparameters in a neural network. Continuing to change the parameters \\nuntil the error, E, is minimized for the training set, even if that were \\npossible, would result in a very complex model and over-fitting. As we \\npointed out in Chapter 1, a good practice is to continue fitting the ma-\\nchine learning model to the data, making it more complex, until the re-\\nsults for the validation set start to get worse.  \\nWhen implementing a neural network, we therefore calculate the \\ncost function for the both the training set and the validation set after \\neach epoch of training.  The usual practice is to stop training when the \\ncost function for the validation set starts to increase. We then choose to \\nuse the model that gives the lowest cost function for the validation set.  \\nAs training progresses, neural network software must therefore calcu-\\nlate the cost function for the validation set after each epoch and re-\\nmember all the weights and biases associated with the model that gives \\nthe lowest cost function.   \\n \\n \\n6.6   The Black−Scholes−Merton Formula \\n \\nThe Black−Scholes–Merton (or Black−Scholes) formula is one of the \\nmost famous results in finance. It gives the value of a call option on an \\nasset as \\n \\n𝑆𝑒−𝑞𝑇𝑁(𝑑1) −𝐾𝑒−𝑟𝑇𝑁(𝑑2)                         (6.4) \\n \\nwhere \\n \\n𝑑1 = ln(𝑆𝐾\\n⁄ ) + (𝑟−𝑞+ σ2 2\\n⁄ )𝑇\\n𝜎√𝑇\\n \\n \\n𝑑2 = ln(𝑆𝐾\\n⁄ ) + (𝑟−𝑞−𝜎2 2\\n⁄ )𝑇\\n𝜎√𝑇\\n \\n134                                                                                                                                      Chapter 6 \\n \\n \\n \\nThe inputs to this formula are as follows.  S is the stock price, K is the \\nstrike price, r is the risk-free rate, q is the dividend yield (i.e., income as \\na percent of the price of the stock), \\uf073 is the stock price volatility, and T \\nis the option’s time to maturity.  \\nWe will use the formula to provide an application of neural net-\\nworks.7 (See ANDY LANDU NGOMA Note that it is not nec-\\nessary to understand how call options work in order appreciate this \\napplication of neural networks.  However, interested readers will find \\nthat Chapter 10 provides a discussion of this as well as some further \\napplications of machine learning to derivative markets.  \\nWe assume q = 0 and create a data set of 10,000 observations by \\nrandomly sampling from uniform distributions for the other five inputs \\nto the Black−Scholes−Merton formula.8 The lower and upper bounds of \\nthe uniform distributions are as indicated in Table 6.5. For each set of \\nparameters sampled, we calculate the Black−Scholes−Merton price us-\\ning equation (6.4). To make the illustration more interesting, we then \\nadd a random error to each observation. The random error is normally \\ndistributed with a mean of zero and standard deviation of 0.15. The ob-\\nservations were split as follows: 60,000 to the training set, 20,000 to the \\nvalidation set, and 20,000 to the test set. Z-score scaling, based on the \\nmean and standard deviation of the observations in the training set, \\nwas used for all the features.  \\n \\nTable 6.5   Upper and lower bounds used for Black-Scholes parameters \\nto create the data set \\n \\n \\nLower bound \\nUpper bound \\nStock price, S \\n40 \\n60 \\nStrike price, K \\n0.5S \\n1.5S \\nRisk free rate, r \\n0 \\n5% \\nVolatility, \\uf073 \\n10% \\n40% \\nTime to maturity, T \\n3 months \\n2 years \\n \\n                                                           \\n7  Black−Scholes−Merton model was used to illustrate neural networks many years \\nago by J. M. Hutchinson, A. W. Lo, and T. Poggio, “A Nonparametric Approach to Pric-\\ning and Hedging Derivative Securities Via Learning Networks,” Journal of Finance, \\n(July 1994), 49(3): 851−889. A more recent similar implementation is R. Culkin and \\nS. R. Das, “Machine Learning in Finance: The Case of Deep Learning for Option Pric-\\ning,” Journal of Investment Management (2017) 15 (4): 92–100. \\n8 The q = 0 assumption means that we are using the formula corresponding to the \\noriginal Black−Scholes result. \\nNeural Networks                                                                                                                       135 \\n \\n \\n \\nWe use mean absolute error (mae) as the cost function. The neural \\nnetwork has three hidden layers and 20 neurons per layer for a total of \\nabout 1,000 parameters.   Similarly to Figure 6.1, the sigmoid function is \\nused as the activation function, except for the calculation of the option \\nprice from the values in the final hidden layer where a linear activation \\nfunction is used. Learning rates are determined using Adam.  \\nFigure 6.6 shows the mean absolute error for the training set and the \\nvalidation set as the number of epochs is increased.  The results for the \\nvalidation are more noisy than those for the training set, but it is clear \\nthat, whereas the training set results continue to improve, the valida-\\ntion set results start deteriorating after a few thousand epochs. Figure \\n6.7 shows the same results with the errors shown for each epoch being \\nthe average error over the subsequent 50 epochs. This moving-average \\ncalculation eliminates most of the noise and shows more clearly that the \\nvalidation set average errors are similar to the training set errors for \\nthe first few thousand epochs and then start to worsen. \\n \\nFigure 6.6   Training set and validation set mean absolute errors as a \\nthe number of epochs of training is increased \\n \\n \\n \\nAs mentioned earlier, once an increase in the cost function for the \\nvalidation set is observed, normal practice is to go back to the model \\nthat gave the best result for the validation set.  In this case, the best re-\\nsult for the validation set was after 2,575 epochs of training. \\n \\n136                                                                                                                                      Chapter 6 \\n \\n \\n \\nFigure 6.7   Mean absolute errors averaged over 50 epochs \\n \\n \\n \\nHow well has the model fitted the data? The mean absolute error for \\nthe test set is 0.122. This is about what one would expect. The mean \\nabsolute value of a normally distributed variable with a mean of zero \\nand a standard deviation of 0.15 is  \\n \\n√2\\n𝜋× 0.15 = 0.120 \\n \\nIt is interesting to compare three values: \\n \\n1. The true Black−Scholes−Merton price of an option \\n2. The price of the option after noise is added \\n3. The predicted price given by the neural network \\n \\nThe averages of the three prices are very close to each other. The stand-\\nard deviation of the difference between 1 and 2 and between 2 and 3 \\nwas almost exactly 0.15 (which is as expected because the standard de-\\nviation of the noise added to the Black−Scholes−Merton price was 0.15). \\nThe standard deviation of the difference between 1 and 3 was about \\n0.04. This indicates that the neural network model gets rid of the noise \\nreasonably well. If the size of the data set were increased, we would ex-\\npect 1 and 3 to become closer.  \\nNeural Networks                                                                                                                       137 \\n \\n \\n \\n6.7   Extensions \\n \\nThe example in the previous section might seem a little artificial. The \\nBlack-Scholes price can be calculated quickly and accurately, and so \\nthere is little point in using a neural network to estimate it! However, \\nthis is not true for the prices of all derivatives that are traded. Some \\nmust be valued using Monte Carlo simulation or other numerical proce-\\ndures that are computationally slow. This creates problems because \\nanalysts, for a number of reasons, have to carry out scenario analyses, \\ninvolving Monte Carlo simulations, to explore how the values of portfo-\\nlios of derivatives can change through time.  If the procedure for calcu-\\nlating the prices of some of the instruments in the portfolio involves \\nMonte Carlo simulation or another slow numerical procedure, the sce-\\nnario analyses can be impossibly slow.  \\nTo solve this problem, it is useful for analysts to replace slow numer-\\nical pricing procedures with neural networks.9 Once the neural network \\nhas been constructed the valuation is very fast. All that is involved is \\nworking forward through the neural network, starting with the inputs, \\nto get the target price. This can be several orders of magnitude faster \\nthan a slow numerical procedure. \\nThe first stage is to create a large data set relating the derivative’s \\nvalue to inputs such as the price of the underlying asset, interest rates, \\nvolatilities, and so on (as we did in the example in the previous section). \\nThis is done using the standard (slow) numerical procedure and can \\ntake a long time. But it only has to be done once.  The data set is then \\ndivided into a training set, a validation set, and a test set in the usual \\nway. A neural network is trained on the training set.  The validation set \\nis used to determine when training should stop (as in our Black−Scholes \\nexample) and the test set is used to quantify the model’s accuracy.  \\nAn interesting aspect of this type of application of neural networks is \\nthat the analyst does not have to collect and clean a large amount of da-\\nta. The analyst generates the data needed from a model. It is possible to \\nreplicate the relationship between the output and input with very little \\nerror.  One important point is that the model is only reliable for the \\nrange of values of the data in the training set. It is liable to give very \\npoor answers if extrapolated to other data.10  \\n                                                           \\n9 See R. Ferguson and A. Green, “Deeply Learning Derivatives,” October 2018, ssrn \\n3244821. \\n10 However, there have been some attempts to overcome this problem. See A. An-\\ntonov, M. Konikov, and V. Piterbarg, “Neural Networks with Asymptotic Controls,” \\nssrn 3544698. \\n138                                                                                                                                      Chapter 6 \\n \\n \\n \\n6.8   Autoencoders \\n \\nAn autoencoder is a neural network designed to reduce the dimen-\\nsionality of data. Its objective is similar to that of principal components \\nanalysis (PCA), which we discussed in Section 2.7. It is designed to ex-\\nplain most of the variation in a data set that has m features with a new \\nset of less than m manufactured features.  \\nIn an autoencoder’s neural network, the output is the same as the \\ninput.   The simplest type of autoencoder has one hidden layer and line-\\nar activation functions. The number of neurons in the hidden layer is \\nless than the number of features. The first part of the network where \\nvalues at the neurons are determined from the inputs is known as en-\\ncoding.  The second part of the network where the output is determined \\nfrom the values at neurons is known as decoding. The neural network \\ntries to minimize the total mean squared error (or other cost function) \\nbetween the predicted output values and the actual output/input val-\\nues. If the mean squared error is small, the weights used to calculate the \\npredicted output from the hidden layer provide manufactured features \\nthat are smaller in number than the original features but contain similar \\ninformation. \\nIn Section 2.7 we showed how PCA can be used to create a small \\nnumber of features describing interest rate changes. Figure 6.8 shows \\nthe design of an autoencoder to do the same thing. The aim is to deter-\\nmine two features that capture most of the variation in eight interest \\nrates.  \\nDefine 𝑟𝑖𝑗 as the jth rate for the ith observation, and 𝑟̂𝑖𝑗 as the predic-\\ntion of the jth rate made by the neural network for this observation. \\nWith the notation indicated on Figure 6.811  \\n \\n𝑉𝑖1 = ∑𝑟𝑖𝑗𝑤𝑗1\\n8\\n𝑗=1\\n \\n𝑉𝑖2 = ∑𝑟𝑖𝑗𝑤𝑗2\\n8\\n𝑗=1\\n \\n \\nwhere 𝑉𝑖1 and 𝑉𝑖2 are the values of 𝑉1 and 𝑉2 for the ith observation. Al-\\nso \\n \\n                                                           \\n11 To keep the example simple, we assume zero bias throughout the network. This \\nhas been suggested by R. Memisevic, K. Konda, and D. Krueger, “Zero-bias Autoen-\\ncoders and the Benefits of Co-adapting Features” arXiv:1402.3337. \\nNeural Networks                                                                                                                       139 \\n \\n \\n \\n𝑟̂𝑖𝑗= 𝑉𝑖1𝑢1𝑗+𝑉𝑖2𝑢2𝑗 \\n \\nThe objective is the minimize \\n \\n∑(𝑟𝑖𝑗−𝑟̂𝑖𝑗)\\n2\\n𝑖,𝑗\\n \\n \\n \\n \\nFigure 6.8   Design of autoencoder to find two features describing in-\\nterest rate changes  \\n \\n \\n \\n \\n \\n \\nThe first manufactured feature involves the jth rate moving by 𝑢1𝑗 \\nand the second one involves it moving by 𝑢2𝑗.  There are many different \\npossible sets of values for the w’s and the u’s that are equally good and \\ngive the same predictions. Results from one implementation using Excel \\nand Solver (see ANDY LANDU NGOMA are in Table 6.6.   \\nA comparison with Table 2.9 shows that the two manufactured fea-\\ntures are quite different from PC1 and PC2.  However, they are equiva-\\n1 yr. rate\\n2 yr. rate\\n3 yr. rate\\n4 yr. rate\\n5 yr. rate\\n7 yr. rate\\n10 yr. rate\\n30 yr. rate\\n1 yr. rate\\n2 yr. rate\\n3 yr. rate\\n4 yr. rate\\n5 yr. rate\\n7 yr. rate\\n10 yr. rate\\n30 yr. rate\\nV1\\nV2\\nw11\\nw12\\nu11\\nu21\\n140                                                                                                                                      Chapter 6 \\n \\n \\n \\nlent. If we transform the features in Table 6.6 so that the first manufac-\\ntured feature involves a movement in the jth rate of  \\n \\n0.518𝑢1𝑗+ 0.805𝑢2𝑗 \\n \\nand the second manufactured feature involves a movement in the jth \\nrate of  \\n \\n2.234𝑢1𝑗−2.247𝑢2𝑗 \\n \\nwe do get PC1 and PC2. \\n \\nTable 6.6   One possible output from the autoencoder in Figure 6.8 \\n \\n \\nFirst manufactured \\nfeature (the 𝑢1𝑗) \\nFirst manufactured \\nfeature (the 𝑢2𝑗) \\n1 yr. rate \\n0.0277 \\n0.2505 \\n2 yr. rate \\n0.1347 \\n0.3248 \\n3 yr. rate \\n0.2100 \\n0.3276 \\n4 yr. rate \\n0.2675 \\n0.3147 \\n5 yr. rate \\n0.3118 \\n0.3017 \\n7 yr. rate \\n0.3515 \\n0.2633 \\n10 yr. rate \\n0.3858 \\n0.2186 \\n30 yr rate \\n0.3820 \\n0.1331 \\n \\nThe advantage of PCA is that the features produced are uncorrelated \\nand their relative importance is clear from the output.  The advantage of \\nautoencoders is that they can be used with non-linear activation func-\\ntions. To use the PCA terminology, they allow data to be explained with \\nnon-linear factors.  Autoencoders have been found to be useful in image \\nrecognition and language translation.   \\n \\n \\n6.9   Convolutional Neural Networks \\n \\nIn the ANNs we have presented so far, a neuron in one layer is con-\\nnected to every neuron in the previous layer. For very large networks \\nthis is infeasible. A convolutional neural network (CNN) solves this \\nproblem by connecting the neurons in one layer to only a subset of the \\nneurons in the previous layer.  \\nNeural Networks                                                                                                                       141 \\n \\n \\n \\nCNNs are used in image recognition, voice recognition, and natural \\nlanguage processing.  Consider the task of processing an image through \\na neural network for the purposes of facial recognition.  The image is \\ndivided into many small rectangles by drawing horizontal and vertical \\nlines. The small rectangles are referred to as pixels. Each pixel has a cer-\\ntain colour and a number is associated with that colour. Even the sim-\\nplest image is liable to have 10,000 pixels (formed from 100 horizontal \\nand 100 vertical lines). This creates a large number of inputs to a neural \\nnetwork.   \\nIn Figure 6.3, the layers are columns of numbers. When a CNN is \\nused to process images, the inputs are a rectangular array of numbered \\npixels. Subsequent layers are a rectangular grid of numbers.  Figure 6.9 \\nshows how the values at grid points in the first layer depend on the val-\\nues at the pixels in the input layer. The rectangle in layer one denotes a \\nsingle grid point (or neuron) and the bolded rectangle in the input layer \\nshows what is termed the receptive field. It is all the pixels to which the \\ngrid point is related. The values at grid points in subsequent layers de-\\npend on the values at grid points in the previous layer in a similar \\nway.12 \\n \\nFigure 6.9   Relationship of value at grid point in layer one to values at \\ngrid point in the input layer in a CNN \\n \\n \\n \\n \\n \\n \\n                                                           \\n12 There may be “padding” where extra observations are added at the edges to avoid \\nsuccessive layers becoming smaller. \\nLayer 1\\nInput layer\\n142                                                                                                                                      Chapter 6 \\n \\n \\n \\nThe two-dimensional set of points in layer one in Figure 6.9 is re-\\nferred to as a feature map. A complication is that each layer comprises \\nseveral feature maps so that it must be represented in three dimen-\\nsions. However, within a feature map, all neurons share the same set of \\nweights and biases. As a neuron within a feature map and its associated \\nreceptive field are changed, the biases and weights stay the same.  This \\nreduces the number of parameters. It also has the advantage that the \\nidentification of an object in image recognition does not depend on the \\npart of the input layer where it appears.  \\nConsider a black and white image that is 100×100 so that there are \\n10,000 pixels. A regular neural network would lead about 10,0002 or \\n100 million parameters to define the neurons in Layer 1. In a CNN \\nwhere the receptive field is 10×10 and there are six feature maps, this is \\nreduced to 6 × 101 or 606  parameters.  \\n \\n \\n6.10   Recurrent Neural Networks \\n \\nIn a plain vanilla ANN each observation is considered separately \\nfrom every other observation. In a recurrent neural network (RNN) we \\npreserve the temporal sequence in which observations occur because \\nwe want to allow for changes in our prediction model through time. \\nThis can be particularly important in business where relationships be-\\ntween variables tend to change through time.  \\nIn a plain vanilla ANN, as we have explained, the value at a neuron in \\nlayer l is calculated by applying an activation function to a linear com-\\nbination of the values at the neurons of layer l−1.  When this is done for \\none observation, we proceed to do the same thing for the next observa-\\ntion. But the algorithm has no memory. When doing calculations for an \\nobservation at time t, it does not remember anything about the calcula-\\ntions it did for an observation at time t−1. \\nIn an RNN, the activation function at time t is applied to a total of:  \\n\\uf0b7 \\na linear combination of the values at the neurons of layer l−1 at \\ntime t; and \\n\\uf0b7 \\na linear combination of the values at the neurons of layer l for \\nthe observation at the previous time t−1. \\nThis gives the network memory. The values in the network at time t de-\\npend on the values at time t−1 which in turn depend on the values at \\ntime t−2, and so on.   \\nOne issue with this is that the values from several time periods earli-\\ner are liable to have very little effect because they get multiplied by rel-\\nNeural Networks                                                                                                                       143 \\n \\n \\n \\natively small numbers several times. A Long Short-Term Memory \\n(LSTM) approach has been developed to overcome this problem.13  Data \\nfrom the past has the potential to flow straight through to the current \\nnetwork. The algorithm decides which data should be used and which \\nshould be forgotten.   \\n \\n \\nSummary \\n \\nAn artificial neural network is a way of fitting a non-linear model to \\ndata.  Outputs are not related directly to inputs. There are a number of \\nintervening hidden layers which contain neurons. The values at the \\nneurons in the first hidden layer are related to the inputs; the values at \\nthe neurons in the second hidden layer are related to the values at the \\nneurons in the first hidden layer; and so on. The outputs are calculated \\nfrom values at the neurons in the final hidden layer.  \\nThe functions defining the relationships are referred to as activation \\nfunctions. The sigmoid function, which was introduced in connection \\nwith logistic regression, is often used as an activation function to relate \\n(a) the values at neurons in the first hidden layer to the input values \\nand (b) values at neurons in hidden layer l to values at neurons in hid-\\nden layer l−1. When numerical values are being estimated the activation \\nfunction relating the output to the neurons in the final hidden layer is \\nusually linear. When data is being classified, a sigmoid function is more \\nappropriate for this last step.  \\nThe gradient descent algorithm is used to minimize the objective \\nfunction in a neural network. Calculating the minimum can be thought \\nof as finding the bottom of a valley. The algorithm takes steps down the \\nvalley where at each stage it follows the line of steepest descent.  Choos-\\ning the correct size for the step, which is referred to as the learning rate, \\nis an important aspect of the gradient descent algorithm.  A number of \\nprocedures for improving the efficiency of gradient descent algorithms \\nhave been mentioned. \\nIt is not unusual for neural networks to involve tens of thousands of \\nparameters. Even if it were possible to find the values of the parameters \\nthat totally minimize the objective function for the training set, this \\nwould not be desirable as it would almost certainly result in over-\\nfitting.  In practice, a stopping rule is applied so that training using the \\n                                                           \\n13 See S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural Compu-\\ntation, 9(8): 1735−1780. \\n144                                                                                                                                      Chapter 6 \\n \\n \\n \\ngradient descent algorithm is halted when the results for the validation \\nset depart from those for the training set.  \\nAn autoencoder is a neural network where the output is the same as \\nthe input. It is designed to replace the input features with a smaller \\nnumber of almost equivalent features. A convolutional neural network \\nis a neural network where the neurons in one layer are related to a sub-\\nset of the neurons in the previous layer rather than all of them. It is par-\\nticularly useful for image recognition where the inputs are defined by \\nthe colors of tens of thousands (or even millions) of pixels. A recurrent \\nneural network is a version of an ANN that is particularly suitable for \\nsituations where the model for forecasting the output is expected to \\nevolve through time.  \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n6.1 \\nExplain what is meant by (a) a hidden layer and (b) a neuron, and \\n(c) an activation function. \\n6.2 \\nExplain how a sigmoid function relates the values at the neurons \\nin one layer to the values at neurons in the previous layer. \\n6.3 \\nWhat is the universal approximation theorem? \\n6.4 \\nWhat activation function is suggested in the chapter for relating \\nthe target to the values in the final layer when the objective is (a) \\nto predict a numerical variable and (b) to classify data? \\n6.5 \\nWhat is meant by the learning rate in a gradient descent algo-\\nrithm? \\n6.6 \\nWhat problems arise if the learning rate is too high or too low?  \\n6.7 \\nExplain how a stopping rule is chosen when an ANN is trained. \\n6.8 \\nExplain how ANNs can be used in derivatives valuation. \\n6.9 \\nExplain the key difference between a CNN and a plain vanilla \\nANN. \\n6.10 Explain the key difference between an RNN and a plain vanilla \\nANN.  \\n \\n \\nEXERCISES \\n \\n6.11 How many parameters are there when an ANN has five features, \\ntwo hidden layers and ten neurons per hidden layer, and one tar-\\nget? \\nNeural Networks                                                                                                                       145 \\n \\n \\n \\n6.12 Produce tables similar to Table 6.2 for the validation set in Table \\n1.2 of Chapter 1. Assume the simple y = bx model. Experiment \\nwith different starting points and learning rates. \\n6.13 In the model in Figure 6.1 assume that we start the gradient de-\\nscent algorithm by setting all the 𝑤𝑗𝑘 weights equal to 0, all the uk \\nweights equal to 100, and the biases equal to 0. What does the ini-\\ntial network give for the price of a house with overall quality \\nequal to 8 and living area equal to 3000 square feet?   \\n6.14 Use the Python code on ANDY LANDU NGOMA to ex-\\nplore how well the Black–Scholes–Merton application in Section \\n6.6 works as the number of observations, the number of hidden \\nlayers, and the number of neurons per layer are changed.  \\n6.15 Use neural networks for the Iowa house price data. Try different \\nnumbers of hidden layers and neurons per layer. Compare the re-\\nsults with those from linear regression. \\n6.16  Use neural networks for the Lending Club data. Try different \\nnumbers of hidden layers and neurons per layer. Compare the re-\\nsults with those from logistic regression. \\n  \\n\\xa0\\n147 \\n \\n \\n \\n \\nChapter 7 \\n \\nReinforcement Learning \\n \\n \\n \\n \\n \\n \\nSo far, we have considered situations where one decision is taken in \\nisolation from other decisions. For example, the classification of a par-\\nticular loan as “accept” or “reject” in earlier chapters was assumed to be \\nindependent of other decisions made about other loans.  The models we \\ndeveloped implicitly assumed that, whether we accept or reject a loan \\ntoday, does not in any way affect whether we will accept or reject a loan \\nthat comes across our desk tomorrow.  \\nSome situations by their nature involve a series of decisions rather \\nthan a single one.  Furthermore, as the decisions are taken, the envi-\\nronment may be changing. It is then necessary to determine the best \\naction bearing in mind that further decisions have to be taken later. \\nReinforcement learning is the branch of machine learning that deals \\nwith this type of sequential decision making.1  The algorithm receives \\nrewards when outcomes are good and incurs costs (negative rewards) \\nwhen they are bad. The objective of the algorithm is to maximize ex-\\npected future rewards possibly with discounting. \\n                                                           \\n1 A comprehensive treatment of reinforcement learning is provided by R.S. Sutton \\nand A.G. Barto, Reinforcement Learning: An Introduction, 2nd edition, 2018, The MIT \\nPress.  \\n148                                                                                                                                      Chapter 7 \\n \\n \\n \\nIn this chapter we start with a simple sequential decision-making \\nproblem where the environment does not change. This provides an il-\\nlustration of the exploitation vs. exploration trade-off which is central to \\nreinforcement learning. We then move on to consider a more compli-\\ncated situation where the environment does change. Finally, we men-\\ntion how reinforcement learning can be used in conjunction with neural \\nnetworks and discuss applications. \\n \\n \\n7.1   The Multi-armed Bandit Problem \\n \\nImagine a gambler in a casino that offers a game where one of sever-\\nal different levers can be pulled. Each lever provides a random payoff. \\nThe payoff from the kth lever is a sample from a normal distribution \\nwith mean 𝑚𝑘 and standard deviation one. The 𝑚𝑘 are liable to be dif-\\nferent and are not known. However, the casino guarantees that they will \\nnot change. (In this simple example, the environment does not there-\\nfore change). The game can be played many times. What strategy should \\nthe gambler follow to maximize the expected payoff over many trials? \\nClearly the gambler should keep careful records so that the average \\npayoff realized so far from each of the levers is known at all times. At \\neach trial of the game, the gambler has to decide between two alterna-\\ntives: \\n \\n\\uf0b7 Choose the lever that has given the best average payoff so far \\n\\uf0b7 Try a new lever \\n \\nThis is known as the exploitation vs. exploration choice.  The first alter-\\nnative is exploitation (also known as the greedy action). If, after choos-\\ning each lever a few times, the gambler only followed the first strategy, \\nshe would not find the best lever unless she was lucky. Some explora-\\ntion, where another lever is chosen at random, is therefore a good idea.  \\nExploitation maximizes the immediate expected payoff, but a little ex-\\nploration may improve the long-run payoff.  \\nA strategy for the gambler is to: \\n \\n\\uf0b7 \\nRandomly choose a lever with probability \\uf065 \\n\\uf0b7 \\nChoose the lever that has given the best average payoff so far \\nwith probability 1−\\uf065 \\n \\nReinforcement Learning                                                                                                        149 \\n \\n \\n \\nfor some \\uf065 (0 < \\uf065\\uf020≤\\uf020\\uf031\\uf029.  We can implement this strategy by sampling a \\nrandom number between 0 and 1. If it is less than \\uf065\\uf02c the lever is chosen \\nrandomly; otherwise, the lever with the best average payoff so far is \\nchosen.  We might choose a value of \\uf065\\uf020equal to one initially and then \\nslowly reduce it to zero as data on the payoffs is obtained.  \\nSuppose that the kth lever has been chosen n−1 times in the past and \\nthe total reward on the jth time it was chosen was 𝑅𝑗. The best estimate \\nof the expected reward from the kth lever (which we will refer to as the \\nold estimate) is \\n \\n𝑄𝑘\\nold =\\n1\\n𝑛−1 ∑𝑅𝑗\\n𝑛−1\\n𝑗=1\\n \\n \\nIf the kth lever is chosen for the nth time on the next trial and produces \\na reward Rn, we can update our estimate of the expected reward from \\nthe kth lever as follows: \\n \\n𝑄𝑘\\nnew = 1\\n𝑛∑𝑅𝑗= 𝑛−1\\n𝑛\\n𝑄𝑘\\nold + 1\\n𝑛𝑅𝑛\\n𝑛\\n𝑗=1\\n \\n \\nwhich can be written as \\n \\n𝑄𝑘\\nnew = 𝑄𝑘\\nold + 1\\n𝑛(𝑅𝑛−𝑄𝑘\\nold)                           (7.1) \\n                       \\nThis shows that there is a simple way of updating the expected reward. \\nWe do not have to remember every reward on every trial. \\nConsider a situation where there are four levers and \\n \\n𝑚1 = 1.2,    𝑚2 = 1.0,     𝑚3 = 0.8,   𝑚4 = 1.4 \\n \\nThe gambler would of course choose the fourth lever every time if these \\nvalues were known.  However, the mk have to be inferred from the re-\\nsults of trials. We first suppose that \\uf065\\uf020is kept constant at 0.1. This means \\nthat there is always a 90% chance that the gambler will choose the lever \\nthat has given the best result so far and a 10% chance that a lever will \\nbe chosen at random. The results from a Monte Carlo simulation run are \\nshown in Table 7.1 (see Excel file for calculations). We arbitrarily set \\nthe Q-values (i.e., average payoffs) for each lever equal to zero initially. \\nLever one is chosen on the first trial and gives a payoff of 1.293 (slightly \\n150                                                                                                                                      Chapter 7 \\n \\n \\n \\nabove average). The Q-value for the first lever therefore becomes 1.293 \\nwhile those for the others stay at zero. On the second trial there is \\ntherefore a 10% chance that gambler will explore (i.e., choose a lever at \\nrandom) and a 90% chance that she will choose the first lever. In fact, \\nshe chooses the first lever (but gets a particularly low payoff of 0.160). \\nThe first lever is still the best one with a Q-value of 0.726. The decision \\non the third trial is to exploit and so the first lever is chosen again. On \\nthe fourth trial, she explores (because a random number less than 0.1 is \\ndrawn) and randomly selects the second lever. \\n  \\nTable 7.1   Results from 5000 trials with four levers and \\uf065\\uf020= 0.1 \\n \\n \\n \\nThe “Nobs” column in Table 7.1 shows the number of times a lever \\nhas been chosen, and therefore the number of observations over which \\nthe average payoff is calculated. In the first 50 trials the best lever (Lev-\\ner 4) is not chosen at all. However, in the first 500 trials it is chosen 387 \\ntimes (i.e., on 77% of trials). In the first 5,000 trials it is chosen 4,527 \\ntimes (i.e., on just over 90% of the trials). The table shows that, after \\nwhat appears to be a rocky start, the algorithm finds the best lever \\nwithout too much difficulty. The average gain per trial over 5,000 trials \\nis 1.345, a little less than the 1.4 average payout per trial from the best \\n(fourth) lever.  \\nTables 7.2 and 7.3 show the results of keeping \\uf065 constant at 0.01 and \\n0.5. There are a number of conclusions we can draw from these tables. \\nSetting\\uf020\\uf065\\uf020= 0.01 gives very slow learning. Even after 5,000 trials the first \\nlever looks better than the fourth lever. What is more the average gain \\nper trial over 5,000 trials is worse than when \\uf065=0.1. When \\uf065\\uf020= 0.5 (so \\nthat there is always an equal chance of exploitation and exploration) the \\nalgorithm finds the best lever without too much difficulty, but the aver-\\nage gain per trial is inferior to that in Table 7.1 because there is too \\nmuch exploration.  \\nLever\\nAve Gain\\nTrial Decision Chosen Payoff\\nQ-val\\nNobs Q-val\\nNobs\\nQ-val\\nNobs\\nQ-val\\nNobs\\nper trial\\n0\\n0\\n0\\n0\\n1\\nExploit\\n1\\n1.293\\n1.293\\n1\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n1.293\\n2\\nExploit\\n1\\n0.160\\n0.726\\n2\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n0.726\\n3\\nExploit\\n1\\n0.652\\n0.701\\n3\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n0.701\\n4\\nExplore\\n2\\n0.816\\n0.701\\n3\\n0.816\\n1\\n0.000\\n0\\n0.000\\n0\\n0.730\\n50\\nExploit\\n1\\n0.113\\n1.220\\n45\\n-0.349\\n3\\n0.543\\n2\\n0.000\\n0\\n1.099\\n100\\nExploit\\n4\\n2.368\\n1.102\\n72\\n0.420\\n6\\n0.044\\n3\\n1.373\\n19\\n1.081\\n500\\nExplore\\n3\\n1.632\\n1.124\\n85\\n1.070\\n17\\n0.659\\n11\\n1.366\\n387\\n1.299\\n1000 Exploit\\n4\\n2.753\\n1.132\\n97\\n0.986\\n32\\n0.675\\n25\\n1.386\\n846\\n1.331\\n5000 Exploit\\n4\\n1.281\\n1.107\\n206\\n0.858\\n137\\n0.924\\n130\\n1.382\\n4527\\n1.345\\nLever 1 (stats)  Lever 2 (stats) Lever 3 (stats) Lever 4 (stats)\\nReinforcement Learning                                                                                                        151 \\n \\n \\n \\nTable 7.2   Results from 5000 trials with four levers and \\uf065\\uf020= 0.01 \\n \\n \\n \\nTable 7.3   Results from 5000 trials with four levers and \\uf065\\uf020= 0.5 \\n \\n \\n \\nAs indicated earlier, the best strategy is to start with \\uf065\\uf020close to 1 and \\nreduce it as data on the payoffs is accumulated. One approach is to set \\n\\uf065\\uf020equal to 1 on the first trial and then multiply it by a decay factor \\nslightly less than 1 on each subsequent trial. If the decay factor is \\uf062 the \\nprobability of exploration on trial \\uf074 is β𝜏−1.  Table 7.4 shows results for \\nthe multi-armed bandit problem we have been considering when \\uf062\\uf020= \\n0.995. It can be seen that these results are superior to those in Table 7.1 \\nwhere a constant \\uf065 equal to 0.1 is used. The algorithm quickly finds the \\nbest lever and produces a average gain per trial of 1.381 (compared \\nwith 1.345 for \\uf065 = 0.1).  \\nLike most hyperparameters, the decay factor \\uf062 must be chosen by \\ntrial and error. Reducing \\uf062 to 0.99 often works well but occasionally \\nfails to find the best lever. Increasing \\uf062 to 0.999 does find the best lever, \\nbut not as quickly as \\uf062=0.995. (See Exercise 7.10.) \\n \\nLever\\nAve Gain\\nTrial Decision Chosen Payoff\\nQ-val\\nNobs Q-val\\nNobs\\nQ-val\\nNobs\\nQ-val\\nNobs\\nper trial\\n0\\n0\\n0\\n0\\n1\\nExploit\\n1\\n1.458\\n1.458\\n1\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n1.458\\n2\\nExploit\\n1\\n0.200\\n0.829\\n2\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n0.829\\n3\\nExploit\\n1\\n2.529\\n1.396\\n3\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n1.396\\n4\\nExploit\\n1\\n-0.851\\n0.834\\n4\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n0.834\\n50\\nExploit\\n1\\n1.694\\n1.198\\n49\\n0.000\\n0\\n-0.254\\n1\\n0.000\\n0\\n1.169\\n100\\nExploit\\n1\\n0.941\\n1.132\\n99\\n0.000\\n0\\n-0.254\\n1\\n0.000\\n0\\n1.118\\n500\\nExploit\\n1\\n0.614\\n1.235\\n489\\n0.985\\n6\\n-0.182\\n2\\n0.837\\n3\\n1.224\\n1000 Exploit\\n1\\n1.623\\n1.256\\n986\\n0.902\\n7\\n-0.182\\n2\\n0.749\\n5\\n1.248\\n5000 Exploit\\n1\\n1.422\\n1.215 4952 1.022\\n18\\n0.270\\n8\\n1.148\\n22\\n1.213\\nLever 1 (stats) Lever 2 (stats)Lever 3 (stats) Lever 4 (stats)\\nLever\\nAve Gain\\nTrial Decision Chosen Payoff\\nQ-val\\nNobs Q-val\\nNobs\\nQ-val\\nNobs\\nQ-val\\nNobs\\nper trial\\n0\\n0\\n0\\n0\\n1\\nExploit\\n1\\n0.766\\n0.766\\n1\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n0.766\\n2\\nExplore\\n1\\n1.257\\n1.011\\n2\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n1.011\\n3\\nExploit\\n1\\n-0.416\\n0.536\\n3\\n0.000\\n0\\n0.000\\n0\\n0.000\\n0\\n0.536\\n4\\nExplore\\n3\\n0.634\\n0.536\\n3\\n0.000\\n0\\n0.634\\n1\\n0.000\\n0\\n0.560\\n50\\nExplore\\n4\\n0.828\\n1.642\\n17\\n1.140\\n9\\n0.831\\n9\\n1.210\\n15\\n1.276\\n100\\nExplore\\n3\\n2.168\\n1.321\\n47\\n0.968\\n15\\n0.844\\n16\\n1.497\\n22\\n1.231\\n500\\nExplore\\n1\\n0.110\\n1.250\\n86\\n0.922\\n65\\n0.636\\n72\\n1.516\\n277\\n1.266\\n1000 Explore\\n4\\n1.815\\n1.332\\n154\\n1.004\\n129\\n0.621\\n131\\n1.394\\n586\\n1.233\\n5000 Explore\\n3\\n2.061\\n1.265\\n666\\n0.953\\n623\\n0.797\\n654\\n1.400\\n3057\\n1.247\\nLever 1 (stats) Lever 2 (stats)Lever 3 (stats) Lever 4 (stats)\\n152                                                                                                                                      Chapter 7 \\n \\n \\n \\nTable 7.4   Results from 5,000 trials with four levers when \\uf065 starts at 1 \\nwith a decay factor of 0.995 \\n \\n \\n \\n \\n \\n7.2   Changing Environment \\n \\nThe multi-armed bandit problem provides a simple example of rein-\\nforcement learning.  The environment does not change and so the Q-\\nvalues are a function only of the action (i.e., the lever chosen). In a more \\ngeneral situation, there are a number of states and a number of possible \\nactions. This is illustrated in Figure 7.1. The decision maker takes an \\naction, A0, at time zero when the state S0 is known.  This results in a re-\\nward, R1, at time 1 and a new state, S1, is encountered.  The decision \\nmaker then takes another action at time 1 which results in a reward, R2, \\nat time 2 and a new state, S2; and so on. In this more general situation, \\nthe Q-value is a function of both the state and the action taken. \\nOur aim is to maximize future expected rewards. A simple objective \\nat time t is therefore to maximize the expected value of G where \\n \\n𝐺= 𝑅𝑡+1 + 𝑅𝑡+2 + 𝑅𝑡+3 + ⋯+𝑅𝑇 \\n \\nand T is a horizon date. In some cases, it is appropriate to maximize dis-\\ncounted rewards over a (possibly infinite) horizon. The reinforcement \\nlearning literature then uses the following expression for G: \\n \\n𝐺= 𝑅𝑡+1 + γ𝑅𝑡+2 + γ2𝑅𝑡+3 + ⋯                         (7.2) \\n \\nLever\\nAve Gain\\nTrial Decision Chosen Payoff\\nQ-val\\nNobs Q-val\\nNobs\\nQ-val\\nNobs\\nQ-val\\nNobs\\nper trial\\n0\\n0\\n0\\n0\\n1\\nExplore\\n2\\n1.4034\\n0\\n0\\n1.403\\n1\\n0\\n0\\n0.000\\n0\\n1.403\\n2\\nExplore\\n1\\n0.796\\n0.796\\n1\\n1.403\\n1\\n0.000\\n0\\n0.000\\n0\\n1.100\\n3\\nExplore\\n2\\n0.499\\n0.796\\n1\\n0.951\\n2\\n0.000\\n0\\n0.000\\n0\\n0.900\\n4\\nExplore\\n1\\n0.407\\n0.601\\n2\\n0.951\\n2\\n0.000\\n0\\n0.000\\n0\\n0.776\\n50\\nExplore\\n3\\n-1.253\\n0.719\\n8\\n1.640\\n18\\n0.729\\n11\\n1.698\\n13\\n1.308\\n100\\nExplore\\n1\\n0.100\\n0.852\\n19\\n1.326\\n31\\n0.681\\n20\\n1.391\\n30\\n1.126\\n500\\nExploit\\n4\\n-0.448\\n1.148\\n37\\n1.184\\n51\\n0.815\\n51\\n1.349\\n361\\n1.263\\n1000 Exploit\\n4\\n2.486\\n1.174\\n44\\n1.225\\n53\\n0.819\\n53\\n1.387\\n850\\n1.339\\n5000 Exploit\\n4\\n3.607\\n1.148\\n45\\n1.225\\n53\\n0.819\\n53\\n1.391\\n4849\\n1.381\\nLever 1 (stats) Lever 2 (stats)Lever 3 (stats) Lever 4 (stats)\\nReinforcement Learning                                                                                                        153 \\n \\n \\n \\nwhere \\uf067 (< 1) is the discount factor.2  Finance professionals are likely to \\ninterpret \\uf067\\uf020as 1/(1 + r) where r is the discount rate per period (possibly \\nadjusted for risk).  But this interpretation leads to the coefficient of 𝑅𝑡+𝑘 \\nbeing γ𝑘 rather than γ𝑘−1 . The simplest way of reconciling equation \\n(7.2) with the discount rates used in finance is to define the reward \\n𝑅𝑡+𝑘 as the cash received at time t+k discounted by one period. (i.e., if \\nthe cash received at time t + k is C then 𝑅𝑡+𝑘= γ𝐶. )  \\n \\nFigure 7.1   Reinforcement learning in a changing environment  \\n \\n \\n \\n Note that the states must include everything relevant to the action \\ntaken. For example, if we are developing a trading strategy and the past \\nhistory of stock prices is relevant, they must be included in the “cur-\\nrent” state.  \\nThe Q-values in the general situation reflect all future rewards, pos-\\nsibly discounted. Suppose that on a particular trial the total value of the \\nfuture rewards from taking action A in state S is G. Suppose further that \\nthis is the nth trial where action A has been taken in state S. Analogously \\nto equation (7.1), we might update as follows: \\n \\n𝑄new(𝑆, 𝐴) = 𝑄old(𝑆, 𝐴) + 1\\n𝑛[𝐺−𝑄old(𝑆, 𝐴)]        \\n \\nIn practice, in a changing environment it usually makes sense to give \\nmore weight to recent observations. We can do this by setting \\n \\n                                                           \\n2 See, for example, R.S. Sutton and A.G. Barto, Reinforcement Learning: An Introduc-\\ntion, 2nd edition, 2018, The MIT Press, Chapter 3.  \\n \\nS0\\nA0\\nR1\\nS1\\nA1\\nR2\\nS2\\nA2\\nR3\\nS3\\nA3\\nInitial \\nState\\nAction Taken \\ntime 0\\nReward time 1 \\nState time 1\\nAction Taken \\ntime 1\\nReward time 2\\nState time 2\\nReward time 3\\nState time 3\\nAction Taken \\ntime 2\\nAction Taken \\ntime 3\\n154                                                                                                                                      Chapter 7 \\n \\n \\n \\n𝑄new(𝑆, 𝐴) = 𝑄old(𝑆, 𝐴) + α[𝐺−𝑄old(𝑆, 𝐴)]                  (7.3) \\n \\nThe weight given to an observation then declines as it becomes older.  \\n \\n \\n7.3   The Game of Nim  \\n \\nThe game of Nim provides an illustration of the material in the Sec-\\ntion 7.2. Imagine that there is a pile of matches. You and your opponent \\ntake turns to pick up one or two or three matches. The person who is \\nforced to pick up the last match loses.  \\nA little thought indicates that as the game nears the end you will win \\nif you leave your opponent with five matches. Whatever your opponent \\npicks up you will then be able to leave her with just one match on the \\nnext round. For example, if she picks up two matches you also pick up \\ntwo matches; if she picks up three matches you pick up one match.  How \\ndo you get to the stage where you can leave your opponent with five \\nmatches?  The answer is that, if you leave her with nine matches, you \\nwill always be able to reduce the pile to five matches on your next turn. \\nContinuing in this way, it is not difficult to see that the way to win is to \\nalways leave your opponent with 4n+1 matches where n is an integer. \\nOf course, if your opponent is savvy, she will try and do the same and so \\nwho wins will depend on the initial number of matches and who goes \\nfirst.  \\nLet us consider how you would analyze Nim with reinforcement \\nlearning. We assume that your opponent behaves randomly rather than \\noptimally. The state, S, is the number of matches left and the action, A, is \\nthe number of matches picked up.  With probability 1−\\uf065 the algorithm \\npicks the best action identified so far for a particular state and with \\nprobability \\uf065 it randomly chooses an action. We start by setting all the \\nQ’s equal to 0.  This is shown in Table 7.5.  Somewhat arbitrarily we set \\nthe reward for winning the game to +1 and the reward for losing the \\ngame to −1. In this example there is no reward until the end.  We as-\\nsume that \\uf061 in equation (7.3) equals 0.05. \\n \\nTable 7.5    Initial Q-values \\n \\n \\nMatches\\npicked up\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n3\\n0\\n0\\n0\\n0\\n0\\n0\\nState (= number of matches left)\\nReinforcement Learning                                                                                                        155 \\n \\n \\n \\nTo keep the example simple, the games we consider start with only 8 \\nmatches. Suppose that on the first game you pick up 1 match, your op-\\nponent’s (random) decision is to pick up 3 matches. You then pick up 1 \\nmatch and your opponent picks up 3 matches. You win and obtain a re-\\nward of +1. Equation (7.3) gives \\n \\nQ(8, 1) = 0+0.05(1−0) = 0.05 \\n \\nbecause, when there are 8 matches and you pick up 1, you end up get-\\nting a payoff of +1. Also \\n \\nQ(4, 1) = 0+0.05(1−0) = 0.05 \\n \\nbecause, when there are 4 matches and you pick up 1, you end up get-\\nting a payoff of +1. This leads to Table 7.6. \\n \\nTable 7.6   Q-values after one game \\n \\n \\n \\nSuppose that on the next game you initially pick up 1 match and your \\nopponent picks up 2 matches. You pick up one match and your oppo-\\nnent picks up 3 matches. You have to pick up the remaining match and \\nlose for a payoff of −1. This leads to Table 7.7 with Q(8,1) and Q(5,1) \\nbeing updated as follows: \\n \\nQ(8, 1) = 0.05+0.05(−1−0.05) = −0.0025 \\n \\nQ(5,1)= 0+0.05(−1−0) = −0.05 \\n \\nTable 7.7   Q-values after two games \\n \\n \\n \\nMatches\\npicked up\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n0\\n0\\n0.05\\n0\\n0\\n0\\n0.05\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n3\\n0\\n0\\n0\\n0\\n0\\n0\\nState (= number of matches left)\\nMatches\\npicked up\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n0\\n0\\n0.05\\n−0.05\\n0\\n0\\n−0.0025\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n3\\n0\\n0\\n0\\n0\\n0\\n0\\nState (= number of matches left)\\n156                                                                                                                                      Chapter 7 \\n \\n \\n \\nTables 7.8, 7.9, and 7.10 show the situation after 1,000, 5,000, and \\n25,000 games for one simulation assuming that the initial value of \\uf065 is 1 \\nand the decay factor applied to it is 0.9995.3 The algorithm requires \\nmore data than the multi-armed bandit example, but eventually it \\nlearns the correct strategy which is: \\n \\n\\uf0b7 \\nWhen there are 8 matches, the correct strategy is to pick up 3 \\nmatches. \\n\\uf0b7 \\nWhen there are 6 matches, the correct strategy is to pick up 1 \\nmatch. \\n\\uf0b7 \\nWhen there are 5 matches there are no good strategies. \\n\\uf0b7 \\nWhen there are 4 matches, the correct strategy is to pick up 3 \\nmatches. \\n\\uf0b7 \\nWhen there are 3 matches, the correct strategy is to pick up 2 \\nmatches. \\n\\uf0b7 \\nWhen there are 2 matches, the correct strategy is to pick up one \\nmatch. \\n \\n \\nTable 7.8   Q-values as a function of state and action after 1,000 games \\n(see Excel file for calculations) \\n \\n \\n \\n \\nTable 7.9   Q-values as a function of state and action after 5,000 games \\n(see Excel file for calculations) \\n \\n \\n \\n \\n \\n                                                           \\n3 In this case there are two key hyperparameters, \\uf061 and \\uf062. Appropriate values can \\nbe determined using trial and error.   \\nMatches\\npicked up\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n0.999\\n-0.141\\n0.484\\n-0.122\\n0.155\\n0.000\\n0.272\\n2\\n-0.994\\n0.999\\n-0.108\\n-0.276\\n-0.171\\n0.000\\n0.252\\n3\\n0.000\\n-0.984\\n1.000\\n-0.070\\n-0.080\\n0.000\\n0.426\\nState (= number of matches left)\\nMatches\\npicked up\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n1.000\\n-0.127\\n0.382\\n0.069\\n0.898\\n0.000\\n0.786\\n2\\n-1.000\\n1.000\\n0.222\\n0.297\\n-0.059\\n0.000\\n0.683\\n3\\n0.000\\n-1.000\\n1.000\\n-0.106\\n0.041\\n0.000\\n0.936\\nState (= number of matches left)\\nReinforcement Learning                                                                                                        157 \\n \\n \\n \\nTable 7.10   Q-values as a function of state and action after 25,000 \\ngames (see Excel file for calculations) \\n \\n \\n \\n \\n7.4     Temporal Difference Learning \\n \\nThe method in the previous section is referred to as the Monte Carlo \\nmethod. We now present an alternative approach. \\nIn the general situation, we can define 𝑉𝑡(𝑆) as the value at time t as-\\nsuming that we are in state S and the actions taken subsequently are \\noptimal. Assuming no discounting this means that \\n \\n𝑉𝑡(𝑆) = max\\n𝐴 𝐸[𝑅𝑡+1 + 𝑉𝑡+1(𝑆′)] \\n \\nwhere 𝑆′ is the state at time t+1 assuming that action A is taken at time \\nt. A similar equation can be used to relate 𝑉𝑡+1to 𝑉𝑡+2, 𝑉𝑡+2 to 𝑉𝑡+3, and \\nso on. This allows a dynamic programming method developed by Rich-\\nard Bellman to be used in relatively simple situations. We start by con-\\nsidering all the states that could arise at the horizon time T and work \\nbackward. First, we calculate the optimal actions for states that could \\narise at time T−1. Given this we calculate the optimal actions for states \\nat time T−2, and so on.  \\nAs mentioned earlier, the strategy for winning at Nim is to leave \\nyour opponent with 4n+1 matches for some integer n. To prove this \\nformally we can show (a) that we win if we leave the opponent with five \\nmatches and (b) that if we leave our opponent with 4n+1 matches after \\none turn we can leave her with 4(n−1) +1 matches after the next turn (n \\n> 1).  This is a simple example of dynamic programming where we are \\nin effect working back from the end of the game to find the optimal cur-\\nrent decision. \\n Unfortunately, dynamic programming is not practical for many large \\nproblems, but reinforcement learning can use the ideas underlying dy-\\nnamic programming. As before, we define Q(S, A) as the current esti-\\nmate of the value of taking action A in state S.  The value of being in \\nstate S is \\n \\nMatches\\npicked up\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1\\n1.000\\n0.080\\n0.104\\n0.069\\n0.936\\n0.000\\n0.741\\n2\\n-1.000\\n1.000\\n0.103\\n0.412\\n-0.059\\n0.000\\n0.835\\n3\\n0.000\\n-1.000\\n1.000\\n-0.106\\n0.041\\n0.000\\n1.000\\nState (= number of matches left)\\n158                                                                                                                                      Chapter 7 \\n \\n \\n \\n𝑉(𝑆) = max\\n𝐴 𝑄(𝑆, 𝐴)) \\n \\nFor example, after 5,000 games we would calculate from Table 7.9 \\nV(8)=0.936,  V(6)=0.898,  V(5)=0.297,  V(4)=1.000,  V(3)=1.000,  and \\nV(2)= 1.000. \\nIn the Monte Carlo method, we update Q(S, A) by observing the total \\nsubsequent gain, G, when a particular decision is taken in a particular \\nstate.  We can use the ideas underlying dynamic programming and look \\njust one time-step ahead. Suppose that when we take action A in state S \\nwe move to state  𝑆′. We can use the current value for  𝑉(𝑆′)  to update \\nas follows:  \\n \\n𝑄new(𝑆, 𝐴) = 𝑄old(𝑆, 𝐴) + α[𝑅+ γ𝑉(𝑆′) −𝑄old(𝑆, 𝐴)] \\n \\nwhere R is the reward at the next step and \\uf067 is the discount factor.  \\nIn the Nim example, suppose that the current Q-values are those \\nshown in Table 7.9. Suppose further that the results on the next game \\nare as follows: \\n \\n\\uf0b7 \\nYou explore and choose 1match  \\n\\uf0b7 \\nYour opponent chooses 1 match \\n\\uf0b7 \\nYou exploit and choose 1 match  \\n\\uf0b7 \\nYour opponent chooses 3 matches  \\n\\uf0b7 \\nYou exploit and choose 1 match \\n\\uf0b7 \\nYour opponent chooses 1 match \\n\\uf0b7 \\nYou win \\n \\n With \\uf061 = 0.05 and \\uf067=1, Q(8,1) would be updated as follows \\n \\n𝑄new(8,1) = 𝑄old(8,1) + 0.05[𝑉(6) −𝑄old(8,1)] \\n                                      = 0.786 + 0.05 × (0.898 −0.786) \\n                                      = 0.792 \\n \\nAlso Q(6,1) would be updated as follows \\n \\n𝑄new(6,1) = 𝑄old(6,1) + 0.05[𝑉(2) −𝑄old(6,1)] \\n                                      = 0.898 + 0.05 × (1.000 −0.898) \\n                                      = 0.903 \\n \\nand Q(2,1) would be updated as  \\n \\nReinforcement Learning                                                                                                        159 \\n \\n \\n \\n𝑄new(2,1) = 𝑄old(2,1) + 0.05[1.000 −𝑄old(2,1)] \\n                                     = 1.000 + 0.05 × (1.000 −1.000) \\n                                     =  1.000 \\n \\nThis procedure is known as temporal difference learning. Here we \\nare looking only one step ahead. (A “step” is a move by you and then by \\nyour opponent.) A natural extension of temporal difference learning is \\nwhere we look n steps ahead. This is referred to as n-step bootstrapping. \\n \\n \\n7.5   Deep Q-Learning \\n \\nThe temporal difference approach we have described is referred to \\nas Q-learning. When there are many states or actions (or both), the cells \\nof the state−action table do not get filled up very quickly. It then be-\\ncomes necessary to estimate a complete Q(S, A) function from the re-\\nsults that have been obtained. As the Q(S, A) function is in general non-\\nlinear, an artificial neural network (ANN) is the natural tool for this. Us-\\ning Q-learning in conjunction with an ANN is known as deep Q-learning \\nor deep reinforcement learning.  \\n \\n \\n7.6   Applications  \\n \\nOne of the most widely publicized applications of reinforcement \\nlearning is AlphaGo. This is a computer program developed by Google to \\nplay the board game Go. In May 2017, it surprised professional Go play-\\ners by beating the world champion Go player, Ke Jie, three games to ze-\\nro. It generated data to improve its performance by playing against it-\\nself many times.   (Exercise 7.12 asks you to do something analogous to \\nthis by deriving a Nim strategy where your opponent learns how to play \\nthe game, rather than making random decisions.)    \\nReinforcement learning has found applications other than playing \\ngames. For example, reinforcement learning is used for driverless cars, \\nresource management, and the programming of traffic lights.4 \\nHealthcare is an area that has seen interesting applications of rein-\\n                                                           \\n4 See, for example, the work of  H. Mao, M. Alizadeh, I. Menache, and S. Kandula, \\n2016, entitled  “Resource Management with Deep Reinforcement Learning”: \\nANDY LANDU NGOMA and I. Arel, C. \\nLiu, T. Urbanik, and A.G.Kohls, 2010, “Reinforcement Learning-based Multi-agent \\nSystem for Network Traffic Signal Control,” IET Intell. Transp. Syst., 4, 2: 128−135.  \\n160                                                                                                                                      Chapter 7 \\n \\n \\n \\nforcement learning.5 Treating a patient is a multistage activity. The doc-\\ntor chooses one action, observes the result, chooses another action, and \\nso on. If enough data is available, algorithms should be able to deter-\\nmine the optimal action for any state. However, it is worth pointing out \\nsome of the problems that have been experienced because they are typ-\\nical of those encountered in reinforcement learning: \\n \\n\\uf0b7 Data will tend to be biased toward the treatment option that is \\ncurrently favored by physicians and so it may be difficult for the \\nalgorithm to identify treatments that are better than those cur-\\nrently used. \\n\\uf0b7 It is difficult to come up with a reward function. How, for exam-\\nple, do you trade off quality of life with the length of a patient’s \\nlife? \\n\\uf0b7 A sufficient quantity of relevant data might not exist or if it does \\nexist it might not have been collected in a way that can be used by \\na reinforcement learning algorithm. \\n \\nReinforcement learning generally requires much more data than su-\\npervised learning.  Often the data is simply not available. An analyst can \\nthen try to determine a model of the environment and use that as a way \\nof generating simulated data for input to a reinforcement learning algo-\\nrithm,  \\nThere are a number of potential applications of reinforcement learn-\\ning in finance.  Consider a trader who wants to sell a large block of \\nshares. What is the optimal strategy?  If the trader chooses to sell all the \\nshares in a single trade, she is likely to move the market and the price \\nreceived may be less than that realized if a series of small trades are \\nundertaken.  But if the share price declines sharply, a series of small \\ntrades will not work out well.6  \\nAnother application is to portfolio management.7 This is a multi-\\nstage activity. Changing the composition of a portfolio too frequently \\n                                                           \\n5 See I. Godfried, 2018, “A Review of Recent Reinforcement Learning Applications to \\nHealthcare” at ANDY LANDU NGOMA\\nlearning-applications-to-healthcare-1f8357600407. \\n6 This is considered by a number of authors, for example, Y. Nevmyvaka, Y. Feng, \\nand M. Kearns, “Reinforcement Learning for Optimized Trade Execution.” \\nANDY LANDU NGOMA \\n7 For some examples of this, see Y. Huang, “Financial Trading as a Game: A Deep \\nReinforcement Learning Approach,” arXiv:1807.02787; Z. Liang, H. Chen, J. Zhu, K. \\nJiang and Y. Li,  “Adversarial Deep Reinforcement Learning in Portfolio Manage-\\nment,” arXiv:1808.09940; Z. Jiang, D. Xu, and J. Liang, “A Deep Reinforcement Learn-\\nReinforcement Learning                                                                                                        161 \\n \\n \\n \\nwill involve transaction costs.8  The past history of stock price returns \\ncan be used to evaluate the actions that should be taken in different cir-\\ncumstances. In this case, the reward function should be chosen carefully \\nso that it penalizes risks as well as encouraging strategies that lead to \\nhigh expected returns. \\nA further application is to hedging. There is a trade-off between the \\nfrequency of hedging and the reduction in risk. Risk can be reduced by \\nincreasing the frequency of trading but this also leads to an increase in \\ntransaction costs. Traditionally, derivatives have been hedged by calcu-\\nlating their theoretical sensitivity to the price of the underlying asset, \\nthe volatility of the underlying asset, and other risk factors. Reinforce-\\nment learning provides an alternative which we discuss further in \\nChapter 10. \\n \\n \\nSummary \\n \\nReinforcement learning is concerned with sequential decision mak-\\ning. The set-up involves actions and states. The actions taken lead to \\nrewards and costs. The Q-function estimates the expected reward (net \\nof costs) from taking a particular action when the environment is de-\\nscribed by a particular state.  The best action in a particular state is one \\nfor which the Q-function is greatest. \\nAn important aspect of reinforcement learning is the exploitation vs. \\nexploration choice. When learning from simulated or historical data, it \\nis tempting to take an action that seems best based on the data that has \\nbeen seen so far. However, if the algorithm always does this, it stops \\nlearning because it never tries a new action. A reinforcement learning \\nalgorithm therefore assigns a probability \\uf065 to an action that is chosen \\nrandomly and 1−\\uf065 to the best action identified so far.  Typically, \\uf065 is \\nclose to one initially and declines as the model learns from data. \\nWe have illustrated the exploitation vs. exploration trade-off with \\ntwo examples. One involves the multi-armed bandit problem which is a \\nwell-known problem in statistics. A gambler attempts to learn which of \\na number of one-armed bandits in a casino gives the highest average \\npayout. This is a relatively simple example of reinforcement learning \\n                                                                                                                                  \\ning Framework for the Financial Portfolio Management Problem,” arXiv: \\n1706.10059.pdf \\n8 One source of transaction costs is the bid−ask spread. A portfolio manager typical-\\nly has to buy at a market maker’s ask price and sell at the market maker’s bid price \\nwith the ask price being greater than the bid price.  \\n162                                                                                                                                      Chapter 7 \\n \\n \\n \\nbecause the environment (i.e., the state) never changes. The other ex-\\nample involves the game of Nim where the state is defined by the num-\\nber of matches left and the action is the number of matches picked up.  \\nIn both cases, we have shown that reinforcement learning provides a \\nway of learning the best strategy.  \\nThe value of taking a particular action in a particular state is referred \\nto as the Q-value. The best value for a state is the maximum of the Q-\\nvalues over all possible actions. There are a number of different ways of \\nupdating Q-values.  One is to base the updating on the total net reward \\n(possibly discounted) between the current time and the horizon date.  \\nAnother is to look only one action ahead and base the updating on the \\nbest value calculated so far for being in the state that exists at the time \\nof the next action.  Other updating procedures are between these two \\nextremes where we look several actions ahead in calculating the conse-\\nquences of an action. \\nIn real-world applications of reinforcement learning there are usual-\\nly a large number of states and actions. One way of coping with this is to \\nuse reinforcement learning in conjunction with artificial neural net-\\nworks (ANNs). Reinforcement learning generates the Q-values for some \\nstate−action combinations and an ANN is used to estimate a more com-\\nplete function. \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n7.1 \\nHow does reinforcement learning differ from supervised learn-\\ning? \\n7.2 \\nExplain why a reinforcement learning algorithm needs to involve \\nboth exploration and exploitation. \\n7.3 \\nExplain how dynamic programming works. \\n7.4 \\nWhat is the optimal strategy for playing Nim? To what extent has \\nthe Monte Carlo simulation found the best action after 1,000, \\n5,000, and 25,000 games in Tables 7.8 to 7.10? \\n7.5 \\nExplain the Monte Carlo approach to reinforcement learning. \\n7.6 \\nWhat is meant by temporal difference learning? \\n7.7 \\nWhy is it sometimes necessary to use an artificial neural network \\nin conjunction with reinforcement learning? \\n7.8 \\nWhat is meant by deep Q-learning? \\n \\n \\n \\n \\nReinforcement Learning                                                                                                        163 \\n \\n \\n \\nEXERCISES \\n \\n7.9 \\nSuppose that Table 7.8 shows the current Q-values for Nim. In the \\nnext game you win because one match is always picked up by \\nboth you and your opponent. How would the table be updated for \\n(a) the Monte Carlo approach and (b) the temporal difference \\nlearning approach?  \\n7.10 Use the worksheet in ANDY LANDU NGOMA for the \\nmulti-armed bandit problem to investigate the impact of using \\ndifferent values for the decay factor, \\uf062\\uf02e \\n7.11 Change the Nim Visual Basic program available at  \\nANDY LANDU NGOMA \\n \\nso that it uses temporal difference learning rather than the Monte \\nCarlo approach. Compare how quickly the two approaches find \\nthe best move when there are eight matches.  \\n7.12 Change the Nim Visual Basic program available at  \\nANDY LANDU NGOMA \\n \\nso that your opponent learns the best strategy rather than behav-\\ning randomly. \\n \\n    \\n\\xa0\\n165 \\n \\n \\n \\n \\n \\n \\nChapter 8  \\n \\nNatural Language Processing \\n \\n \\n \\n \\n \\nUp to now we have talked about applying machine learning algo-\\nrithms to numerical or categorical data. We now move on to consider \\nthe way machine learning can handle language.  This is known as natu-\\nral language processing (NLP) or computational linguistics. It is becom-\\ning increasingly important because much of the data generated in the \\nworld is in the form of written or spoken words.  \\nThe development of NLP applications is challenging because the \\nrules of language are difficult to communicate to a machine and words \\ncan have several meanings. Also, a human being can pick up nuances in \\nlanguage which are almost impossible for a machine to recognize.  For \\nexample, it is difficult for a machine to recognize sarcasm or irony.   In \\nspite of this, a great deal of progress has already been made and we can \\nexpect to see exciting developments in NLP in the future.  \\nIn Chapter 1, we mentioned Google Neural Machine Translation \\nwhich has been very successful in translating text from one language to \\nanother.  Applications such as Siri on iPhones and Amazon’s Alexa can \\nrecognize human speech to perform a variety of simple tasks.  Programs \\nfor accurately converting speech to text work well.  It seems likely that \\nmachines will soon take over the role of professional translators. Two \\nindividuals who speak different languages will then be able to com-\\nmunicate seamlessly.  \\n166                                                                                                                                     Chapter 8 \\n \\n \\n \\nThere are many different natural language processing applications. \\nThis chapter will focus for the most part on what is referred to as senti-\\nment analysis. This involves the processing of data from such sources as \\nsurveys and social media to determine whether it is positive, negative, \\nor neutral about a particular product, company, person, event, etc.  The \\nsheer volume of the data available today often makes manual pro-\\ncessing unrealistic. \\nNLP allows a company to monitor customer responses to its prod-\\nucts and its actions. This can be done in real time and provides im-\\nportant inputs to the company’s decision making. For example, when a \\ncompany markets a new product, the comments from customers can be \\nlead to timely decisions. In 1985, Coca-Cola changed its drink’s formula \\nfor the first time in 99 years.  The new product was not popular and the \\nolder formula was eventually reintroduced as Coca-Cola Classic. Today, \\nNLP and the huge amount of data that is available on social media and \\nelsewhere would enable the company to determine the market’s re-\\nsponse to the new formula very quickly. \\nWhen a company uses a new advertisement, it can use NLP to assess \\nhow well it is received by consumers. If the reaction of consumers is \\nnegative, the advertisement can be pulled quickly. This could have been \\nuseful for Gillette’s “the best men can be” advertisement in January \\n2019, which was not at all well received. NLP can also be used to avoid \\npublic relations disasters. When United Airlines forcibly removed a pas-\\nsenger who was an Asian-American doctor from one of its planes in \\nApril 2017, the initial statements from the company only served to in-\\nflame the situation. NLP could have been used to assess the public’s re-\\nsponse very quickly. The company could then have issued an almost \\nimmediate unconditional apology to mitigate the event’s negative im-\\npact (particularly on its Asian customers).  \\nNLP has many applications for stock market investors. If news re-\\nports about a company or quarterly earnings calls to analysts are posi-\\ntive (negative), we might expect the stock price to increase (decrease). \\nZhang and Skiena were among the first researchers to investigate this.1 \\nThey ranked companies by a sentiment measure derived from news \\nreports each day. They then constructed a portfolio which was long \\nstocks where the sentiment measure was positive and short stocks \\nwhere the sentiment measure was negative. The value of the stocks in \\nthe long portfolio equaled the value of the stocks in the short portfolio \\n                                                             \\n1 See W. Zhang and S. Skiena, “Trading strategies to exploit blog and news senti-\\nment,” 2010, Proceedings of the 4th international AAAI Conference on Weblogs and \\nSocial Media.  \\nANDY LANDU NGOMA \\nNatural Language Processing                                                                                             167 \\n \\n \\n \\nso that the portfolio was market neutral (i.e., its return was not affected \\nby the performance of the stock market as a whole).  Their results \\nshowed that trading relatively few stocks and holding them for short \\nperiods produced impressive returns. \\nBefore readers rush out to develop their own trading strategies \\nbased on media reports, a word of caution is in order. An important \\ntheory concerning the way prices are formed in financial markets is the \\nefficient markets hypothesis. This argues that financial markets reflect \\nall known information. As NLP research findings, such as those of Zhang \\nand Skiena just mentioned, get well known we can expect more traders \\nto use NLP to guide their trading and as a result market prices will ad-\\njust almost immediately to news reports. It will then not be possible to \\ngenerate the large returns realized by researchers in their experiments. \\nDoes this mean that it is too late to profit from using NLP for invest-\\ning? That is not necessarily the case. New data sources are becoming \\navailable all the time. One approach is to try and be one step ahead of \\nmost others in exploiting these new data sources. Another is to develop \\nbetter models than those being used by others and then be very secre-\\ntive about it. Renaissance Technologies, a hedge fund, provides an ex-\\nample of the second approach. It has been amazingly successful at using \\nsophisticated models to understand stock price patterns. Other hedge \\nfunds have been unable to replicate its success. The average return of \\nits flagship Medallion fund between 1988 and 2018 was 66% per year \\nbefore fees. This included a return of close to 100% in 2008 when the \\nS&P 500 lost 38.5%. Two senior executives, Robert Mercer and Peter \\nBrown, are NLP experts and have been running the company following \\nthe retirement of the founder, Jim Simons, in 2009.2   \\nPython contains a number of tools for NLP.  Downloading data from \\nthe web is referred to as web scraping (and also as screen scraping, \\nweb data extraction, and web harvesting). Most data on the web is in \\nthe form of *.html files and Beautiful Soup is a useful resource for con-\\nverting these to files than are better for analysis.  Natural Language \\nToolkit (NLTK) is a platform for building Python programs. It contains \\nmany different tools to help out with the analyses that will be described \\nin this chapter. \\nIn the next few sections, we will assume that opinions about some \\nactivity of a company are to be classified. But the approaches suggested \\ncan be used in many other situations. For example, another popular ap-\\n                                                             \\n2 For more information about Renaissance Technologies, see G. Zuckerman, The \\nman who solved the market: How Jim Simons launched the quant revolution, 2019, \\nPenguin Random House.  \\n168                                                                                                                                     Chapter 8 \\n \\n \\n \\nplication is to distinguish emails that are spam from those that are non-\\nspam (sometimes referred to as ham). \\n \\n \\n8.1   Sources of Data  \\n \\nIn sentiment analysis, an analyst wants to use data that has been col-\\nlected to predict the sentiment of new opinions. A conclusion from the \\nanalysis might be “the opinions being expressed at the moment about \\nour product are 82% positive and 18% negative.”  If opinions are being \\nclassified as “positive”, “negative”, or “neutral”, the conclusion might \\ntake the form “the opinions currently being expressed about our prod-\\nuct are 60% positive, 20% negative and 20% neutral.” Sometimes a \\nnumerical scale is involved, e.g., 1=very negative, 2=somewhat negative, \\n3= neutral, 4=somewhat positive, 5=very positive. An output from NLP \\ncould then be, “the average current sentiment is 3.9.” \\nThe general approach to sentiment analysis is similar to that for oth-\\ner machine learning applications that have been discussed in this book.  \\nWe collect data that has been labeled in one of the ways just discussed \\n(e.g., positive or negative). We divide the data into a training set and a \\ntest set. (If several different models are being compared a validation set \\nis also a good idea, as discussed in Chapter 1.) We use the training set to \\ndevelop the required model(s).  A validation set can be used to choose \\nbetween models. The test data evaluates the accuracy of the chosen \\nmodel. The model is then used as a classification tool for new opinions.  \\nWhere do the labeled opinions come from? We have to base the la-\\nbels on past opinions. There are publicly available data sets where opin-\\nions have been labeled and these are sometimes used to train and test a \\nmodel. However, data sets that have been used for one situation may \\nnot be appropriate for another. For example, opinions about movies \\ntogether with positive/negative labels may not be appropriate for as-\\nsessing opinions about a consumer product used in the kitchen.3  \\nThe best (and most expensive) approach involves a company collect-\\ning a large number of opinions that customers have given for its prod-\\nucts or actions in the past and asking one or more human beings to label \\nthem in one of the ways discussed above. \\nIt is worth noting that that human beings agree on how an opinion \\nshould be labeled only about 80% of the time and so a model that \\nagrees with human judgement 100% of the time is unrealistic.  Typical-\\n                                                             \\n3 Movie reviews are a convenient source of data for sentiment analysis because they \\nare usually labeled with between one and five stars.  \\nNatural Language Processing                                                                                             169 \\n \\n \\n \\nly a good model will agree with human judgement perhaps 70% of the \\ntime.  \\n \\n \\n8.2   Pre-Processing \\n \\nSuppose we have managed to obtain a large number of labeled opin-\\nions with which to construct a model.  A first stage is often pre-\\nprocessing, which is a type of data cleaning.  The main objectives of pre-\\nprocessing is to identify a vocabulary of words that will be considered.  \\nThe first stage in pre-processing is known as word tokenization and \\ninvolves splitting text into a set of words. It involves looking for spaces \\nand punctuation. For example,  \\n \\n“The product works well! I would recommend the product to some-\\none else.” \\n \\nbecomes \\n \\n“The”, “product”, “works”, “well”, “!”, “I”, “would”, “recommend”, \\n“the”, “product”, “to”, “someone”, “else”, “.”  \\n \\nPunctuation can be removed as it usually does not add much infor-\\nmation. Also, we do not want an algorithm to consider “The” and “the” \\nto be different words. It therefore makes sense to convert all upper case \\ncharacters to lower case. \\nIt is common to remove what are termed stop words. These are \\nwords like “the”, “a”, “in” and “an” which are very common but add very \\nlittle to the meaning of text. NLTK has a list of stop words in different \\nlanguages and provides a procedure for removing stop words from text.  \\nAn analyst can change the list. This can be appropriate in some circum-\\nstances and the way it is done can depend on the nature of the text be-\\ning considered.  For example, words removed in legal documents might \\nbe different from those removed in a news article.  One approach that \\ncan be used to identify stop words is to list the 10 or 20 most commonly \\noccurring words in the opinions and then decide whether they should \\nbe retained.  \\nAnother type of pre-processing that is sometimes used involves \\nwhat is referred to as stemming. This is the removal of suffices such as \\n“s”, “ing”, “like”, and “ly”. Doing stemming for English text is not trivial. \\nFor example, we would like to replace “drinking” by “drink” but “sitting” \\nby “sit” and lying” by “lie”.  A key objective of stemming is that related \\n170                                                                                                                                     Chapter 8 \\n \\n \\n \\nwords map to the same stem. The stem itself does not have to be a \\nword. Thus “arguable”, “argues”, “argued”, “arguing” might all be \\nmapped to “argu”. A related procedure is lemmatization. This uses a \\nlook up table to determine a mapping of a word into its root word.  For \\nexample, “worse” would be mapped to “bad”. Also, when a word has \\nmore than one meaning, lemmatization may attempt to use the context \\n(i.e., the surrounding words) to find an appropriate root word.    \\nCorrecting spelling mistakes is desirable because it avoids duplicate \\nwords such as “machine” and “machne” or “learning” and “learnig”. Fair-\\nly sophisticated spell checkers are now available, but they are not per-\\nfect and occasionally change the intended meaning of a sentence. It is \\nalso important to recognize abbreviations. For example, “u” in a text \\nmessage should be changed to “you” and “approx” should be changed to \\n“approximate”.  \\nIt may also be appropriate to remove rare words that appear only \\nonce or twice in the training set.  Suppose we are trying to predict \\nwhether a certain news report has a positive or negative effect on a \\nstock price. If the news report includes the word “myopic”, and that is \\nthe only time the word is used in the training set, it is unlikely that a \\nmodel would be able to conclude with any confidence the word has in-\\nformation content.   \\nThe key point here is that the words we finally choose for the vocab-\\nulary will be used to classify opinions. Words which appear very occa-\\nsionally are of little use as are words that appear in virtually all opin-\\nions. One approach to help identify the words that should be used is to \\nretain only those that appear in between, say, 20% and 80% of opin-\\nions. (Some experimentation can be used to determine the right high \\nand low percentages here.)  \\n \\n \\n8.3   Bag-of-Words Model \\n \\nAssume we have done pre-processing and formed a vocabulary that \\nwill be used for classification. Although we have done our best to reduce \\nthe number of different words that are considered, we may still have \\n10,000 or more words in the vocabulary. In the bag-of-words model \\neach opinion is characterized by the number of times each word ap-\\npears. To illustrate this with a simple example, suppose that our vocab-\\nulary consists of the following list of 10 words  \\n \\n“bad”, “good”, “great”, “much”, “never”, “product”, “recommend”, “some-\\none”, “terrible”, “well”  \\nNatural Language Processing                                                                                             171 \\n \\n \\n \\nSuppose further that we want to classify the opinion mentioned earlier: \\n \\n“The product works well!  I would recommend the product to some-\\none else” \\n \\nA bag-of-words would convert this opinion to  \\n \\n(0, 0, 0, 0, 0, 2, 1, 1, 0, 1) \\n \\nThis indicates that the first word in the list, “bad”, does not appear; the \\nnext four words in the vocabulary also do not appear; the word “prod-\\nuct” appears twice; and so on. Note that words such as “the” and \\n“works” appear in the opinion, but not in the vocabulary, and are there-\\nfore ignored.  \\nHow do we decide whether the opinion is positive or negative?  A \\nsimple approach, which does not involve machine learning, would be to \\nwrite down a list of positive and negative words and determine wheth-\\ner the opinion contains more positive words than negative words, or \\nvice versa. The positive words could include “good”, “great”, “recom-\\nmend”, and “well”. The negative words could include “bad”, “never”, and \\n“terrible”.  If this sort of list making is too much work, there are a num-\\nber of sentiment lexicons that can be used.  \\nIn the case of the opinion in our example, there are two positive \\nwords (“recommend” and “well”) and no negative words and so the \\nopinion would be classified as positive. Labeled data can be used to es-\\ntimate the accuracy of the model.  \\nThere is no learning going on in this simple approach as the positive \\nand negative words are provided externally.  A more sophisticated ap-\\nproach is to use the training set to determine the words that most \\ncommonly occur in positive and negative opinions. The words that oc-\\ncur much more frequently in positive opinions than negative opinions \\nwould be categorized as “positive words” while those that occur much \\nmore frequently in negative opinions than positive opinions would be \\ncategorized as “negative words”.  We would have to decide what we \\nmean here by “much more frequently.” How much more frequently do \\nwords have to be in positive opinions than negative opinions in order to \\nbe “positive words”. Similarly, how much more frequently do words \\nhave to be in negative opinions rather than positive opinions to be \\n“negative words”. The required difference between the frequencies are \\nhyperparameters and it is likely to be desirable to use a validation set to \\nchoose good values for them.  \\n172                                                                                                                                     Chapter 8 \\n \\n \\n \\nOne decision that has to be made in all approaches is whether posi-\\ntive or negative words that occur two or more times in an opinion \\nshould be given more weight. If in our example the word “well” ap-\\npeared twice instead of once, should it be considered as being equiva-\\nlent to two positive words or one positive word? There is some re-\\nsearch indicating that the repetition of a word provides little additional \\ninformation.4   \\nThe bag-of-words model takes no account of the order of words. It \\njust counts the words. Unfortunately, simple word counts can give in-\\ncorrect answers. One reason is the presence of negatives. Consider for \\nexample the opinion: \\n \\n“I would not recommend this product”  \\n \\nClassifying this as a positive opinion because it includes the word “rec-\\nommend” would give an incorrect result. One possible improvement is \\nto consider word pairs as positive or negative. In the opinion just given \\nthe word pairs would be “I would”, “would not” “not recommend”, “rec-\\nommend this”, and “this product”. The word pair “not recommend” \\nwould almost certainly be on the negative list. \\nA set of n consecutive words is referred to as an n-gram.  One word is \\na unigram, two consecutive words is a bigram, three consecutive words \\nis a trigram, and so on. We have just shown that bigrams can identify \\nthe negation of a word and avoid some incorrect signals.  Trigrams can, \\nin principle, work even better than bigrams. For example, the trigram \\n“not so bad” could be a positive trigram.  Of course, the problem here is \\nthat the number of possible bigrams is much greater than the number of \\npossible unigrams, and the number of possible trigrams is greater again. \\nIn what follows we will assume that the analysis is based on unigrams \\n(single words). But the approaches can be extended to cover bigrams \\nand trigrams.  \\n \\n \\n8.4   Application of Naïve Bayes Classifier \\n \\nWe introduced the naïve Bayes classifier in Section 4.4. It is a popu-\\nlar approach for sentiment analysis. Here we introduce an application \\nof the classifier based on whether particular words occur in opinions.  \\n                                                             \\n4 See for example, B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up? Sentiment \\nclassification using machine learning techniques” in Proceedings of Empirical Meth-\\nods for Natural Language Processing, 2002. \\nNatural Language Processing                                                                                             173 \\n \\n \\n \\nThe application can be extended so that it is based on a count of the \\nnumber of times different words appear.5 The naïve Bayes classifier \\nassumes that the occurrence of word X in an opinion is uncorrelated \\nwith the occurrence of word Y for all the X and Y that are in the vocabu-\\nlary being used for classification.   \\nSuppose that there are m words in the vocabulary and our objective \\nis to classify opinions two ways: positive or negative.  If the jth word in \\nthe vocabulary is in a particular opinion, we define pj as the probability \\nthat word j appears in positive opinions and qj as the probability it ap-\\npears in negative opinions. If the jth word in the vocabulary is not in a \\nparticular opinion, we define pj as the probability that word j does not \\nappear in positive opinions and qj as the probability it does not appear \\nin negative opinions. The naïve Bayes classifier gives the probability \\nthat the opinion is positive as \\n \\nProb(Positive|words) =\\n𝑝1𝑝2 … 𝑝𝑚\\nProb(words) Prob (Positive) \\n \\nand the probability that it is negative as \\n \\nProb(Negative|words) =\\n𝑞1𝑞2 … 𝑞𝑚\\nProb(words) Prob (Negative) \\n \\nwhere “words” refers to the list defining whether words are in an opin-\\nion or not, and Prob (Positive), Prob (Negative), and Prob (words) are \\nunconditional probabilities. Because the two probabilities must add up \\nto one, Prob(Positive|words) is \\n \\n𝑝1𝑝2 … 𝑝𝑚× Prob (Positive)\\n𝑝1𝑝2 … 𝑝𝑚× Prob (Positive) + 𝑞1𝑞2 … 𝑞𝑚× Prob (Negative) \\n \\nwhile Prob(Negative|words) is \\n \\n𝑞1𝑞2 … 𝑞𝑚× Prob (Negative)\\n𝑝1𝑝2 … 𝑝𝑚× Prob (Positive) + 𝑞1𝑞2 … 𝑞𝑚× Prob (Negative) \\n \\nThe naïve Bayes classifier can easily be extended to the situation \\nwhere there are more than two classifications. For example, if we are \\nclassifying an opinion as positive, negative, or neutral, we can define rj \\n                                                             \\n5 As indicated in the previous section, multiple occurrences of a word in an opinion \\nmay not have more information than a single occurrence.  \\n174                                                                                                                                     Chapter 8 \\n \\n \\n \\nas the probability that word j appears in a neutral opinion if it is in the \\nopinion under consideration and as the probability that word j does not \\nappear in a neutral opinion if it is not in the opinion under considera-\\ntion. Define P, Q, and R as the unconditional probability of positive, neg-\\native, and neutral observations. Then \\n \\nProb(Positive|words) =\\n𝑝1𝑝2 … 𝑝𝑚𝑃\\n𝑝1𝑝2 … 𝑝𝑚𝑃+ 𝑞1𝑞2 … 𝑞𝑚𝑄+𝑟1𝑟2 … 𝑟𝑚𝑅 \\n  \\nProb(Negative|words) =\\n𝑞1𝑞2 … 𝑞𝑚𝑄\\n𝑝1𝑝2 … 𝑝𝑚𝑃+ 𝑞1𝑞2 … 𝑞𝑚𝑄+𝑟1𝑟2 … 𝑟𝑚𝑅 \\n \\nProb(Neutral|words) =\\n  𝑟1𝑟2 … 𝑟𝑚𝑅\\n𝑝1𝑝2 … 𝑝𝑚𝑃+ 𝑞1𝑞2 … 𝑞𝑚𝑄+𝑟1𝑟2 … 𝑟𝑚𝑅 \\n \\nTo provide a simple example of these equations, suppose that there \\nare 10 observations in the training set and only two words. Table 8.1 \\nshows whether a particular word is in an observation (1 indicates that \\nit is in the observation and 0 indicates that it is not. \\n \\nTable 8.1   Example of a situation where there are 10 opinions in the \\ntraining set and two words. 1 indicates that a word is present while 0 \\nindicates that it is not present.  \\n \\nOpinion \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\nWord 1 \\n1 \\n1 \\n1 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n1 \\nWord 2 \\n0 \\n0 \\n1 \\n1 \\n1 \\n1 \\n0 \\n1 \\n0 \\n0 \\nLabel \\nPos \\nPos \\nPos \\nPos \\nNeg \\nNeg \\nNeg \\nNeut \\nNeut \\nNeut \\n \\nConsider an opinion that contains word 1 but not word 2. What is \\nthe probability that it is favorable? In this case, p1 = 3/4 = 0.75 (because \\nthree of the four positive observations in the training set contain word \\n1) and p2 = 2/4 = 0.5 (because two of the four positive observations do \\nnot contain word 2). Similarly, q1 = 0, q2 = 0.33, r1 = 0.33, and r2 = 0.67. \\nThe unconditional probability of a positive, negative, and neutral opin-\\nion are 0.4, 0.3, and 0.3, respectively. The equations given above show \\nthat the conditional probability that the opinion under consideration is \\npositive is: \\n \\n0.75 × 0.50 × 0.4\\n0.75 × 0.50 × 0.4 + 0 × 0.33 × 0.3 + 0.33 × 0.67 × 0.3 = 0.69 \\n  \\nNatural Language Processing                                                                                             175 \\n \\n \\n \\nThe conditional probability that it is negative is \\n0 × 0.33 × 0.3\\n0.75 × 0.50 × 0.4 + 0 × 0.33 × 0.3 + 0.33 × 0.67 × 0.3 = 0 \\n \\nThe conditional probability that it is neutral is  \\n \\n0.33 × 0.67 × 0.3\\n0.75 × 0.50 × 0.4 + 0 × 0.33 × 0.3 + 0.33 × 0.67 × 0.3 = 0.31 \\n \\nThe performance of the model can be assessed with a test set.  \\nOur baby example illustrates one problem with the naïve Bayes clas-\\nsifier. There is a chance that a particular word, j, appears in the opinion \\nunder consideration but does not appear in any of the training set ob-\\nservations that have a particular label. The probability of the opinion \\nhaving that label is then zero. In our example, word 1 appears in the \\nopinion under consideration but does not appear at all in the observa-\\ntions of the training set that are labeled negative. Hence q1 =0 and the \\nprobability of the opinion being negative is bound to be calculated as \\nzero. This would be the case even if the opinion contained many other \\nwords that were present in observations of the training set that were \\nlabeled negative.  All other negative words have no weight if one partic-\\nular word is not found in the negative training set observations.  \\nAssigning a zero conditional probability may be too extreme. A way \\nof making the zero probabilities slightly positive, so that a more reason-\\nable result is obtained, is known as Laplace smoothing. In this case we \\ncan imagine adding two new observations for each of the three classes \\nin such a way that each word is present in one of the two observations \\nand not present in the other. This involves adding a total of six observa-\\ntions of which two are positive, two are negative and two are neutral. \\nThe unconditional of positive, negative and neutral opinions become \\n6/16, 5/16, and 5/16 instead of 4/10, 3/10, and 3/10. Furthermore, p1 \\nand p2 become 4/6 and 3/6 instead of 3/4 and 2/4, respectively; q1 and \\nq2 become 1/5 and 2/5 instead of 0/3 and 1/3, respectively; r1 and r2 \\nbecome 2/5 and 3/5 instead of 1/3 and 2/3, respectively. \\nThe new probability of that the opinion is positive is \\n \\n0.667 × 0.5 × 0.375\\n0.667 × 0.5 × 0.375 + 0.2 × 0.4 × 0.3125 + 0.4 × 0.6 × 0.3125 = 0.556 \\n  \\nThe probability that it is negative is  \\n \\n176                                                                                                                                     Chapter 8 \\n \\n \\n \\n0.2 × 0.4 × 0.3125\\n0.667 × 0.5 × 0.375 + 0.2 × 0.4 × 0.3125 + 0.4 × 0.6 × 0.3125 = 0.111 \\n \\nThe probability that it is neutral is  \\n \\n0.4 × 0.6 × 0.3125\\n0.667 × 0.5 × 0.375 + 0.2 × 0.4 × 0.3125 + 0.4 × 0.6 × 0.3125 = 0.333 \\n \\nIn this simple example, the impact of Laplace smoothing on the proba-\\nbilities is quite large. As the data set become bigger, its impact declines.  \\n \\n \\n8.5   Application of Other Algorithms \\n \\nThe naïve Bayes classifier has the advantage that it is easy to imple-\\nment and very fast. However, the independence assumption is imper-\\nfect because some words often occur together. For example, a common-\\nly occurring phrase in opinions might be “easy to use”. The words \\n“easy” and “use” would then have a joint probability of occurring in a \\npositive opinion that is much greater than the product of “easy” occur-\\nring and “use” occurring.  \\nOther classification algorithms that have been described in this book \\nare logistic regression, decision trees, SVM, and neural networks.  When \\nthey are used for sentiment analysis, the set up is similar to that for the \\nexamples given earlier in the book.  The features (i.e., the words in the \\nvocabulary used for classification) have values of either 0 or 1 with 1 \\nindicating that the word is present and 0 indicating that it is not pre-\\nsent. We divide the available data into a training set and a test set. (If \\nseveral different models are being compared a validation set is also a \\ngood idea as discussed in Chapter 1.) We use the training set (and pos-\\nsibly a validation set) to develop a classification model.  The test data \\nevaluates the accuracy of the model.  \\nConsider first SVM. Unlike the other algorithms, this merely classi-\\nfies opinions. It does not provide probabilities. In this respect, it is simi-\\nlar to the approaches suggested in Section 8.3. However, it is potentially \\nsuperior to those approaches because it can detect that some words are \\nmore positive (negative) than others.  \\nThe SVM method described in Chapter 5 can be extended so that \\nthere are more than two classes. This involves the estimation of the po-\\nsitions of multiple hyperplanes rather than just one. Unlike logistic re-\\ngression, the SVM algorithm has the advantage that it can be used even \\nNatural Language Processing                                                                                             177 \\n \\n \\n \\nwhen the number of features (i.e., words in the vocabulary) is greater \\nthan the number of observations in the training set. \\nLogistic regression is designed to accommodate two classes but \\nthere are ways of extending it to accommodate more than two classes.  \\nIt has the advantage over SVM that it provides probabilities for the clas-\\nses and has the advantage over naïve Bayes that it does not make the \\nindependence assumption.   \\nDecision trees and neural networks can be used to handle multiple \\nclasses. In Section 3.9 we introduced a maximum likelihood objective \\nfunction when there are only two classes. This can be generalized to \\nmore than two classes. If Qi is the probability that we predict that ob-\\nservation i from a labeled set will fall into its correct class, we want to \\nmaximize \\n  \\n∑ln (𝑄𝑖)\\n𝑖\\n \\n \\nThe performance of a chosen algorithm can be assessed using measures \\nsimilar to those presented in Section 3.11. \\n \\n \\n8.6   Information Retrieval \\n   \\nWe now leave sentiment analysis to consider the application of NLP \\nto information retrieval. This is important for search engines and it can \\nalso be important for businesses that want quick access to a library of \\ndocuments that are stored electronically.  \\nWhen words are input to a search engine by a user, how does it de-\\ncide the most relevant documents to present to the user? Two measures \\nthat are commonly used are term frequency (TF) and inverse document \\nfrequency (IDF). The TF for a document and a word is defined as  \\n \\nNumber of times the word appears in the document \\nNumber of words in the document\\n \\n \\nThe IDF for a word is defined as  \\n \\nlog (𝑁\\n𝑛) \\n \\n178                                                                                                                                     Chapter 8 \\n \\n \\n \\nwhere N is the total number of documents and n is the number of doc-\\numents containing the word.6  The TF-IDF is formed by multiplying TF \\nand IDF and is a score for a particular word in a particular document.    \\nConsider a corporate search engine where there are 10,000 docu-\\nments. To ensure fast information retrieval, the documents are pre-\\nprocessed and word counts are calculated. Suppose we input “the travel \\npolicy” into the search engine. The engine will calculate the TF-IDF for \\neach of the words: “the”, “travel”, and “policy”. The word “the” will have \\na relatively high TF for each document but its IDF will be zero because it \\nwill appear in every document so that N = n.  Suppose that the word \\n“travel” appears in 10% of the documents. Its IDF is \\n \\nlog (10) = 3.32 \\n \\n(assuming that the logarithm is calculated using base 2). The TF-IDF of \\nthe word “travel” is calculated for each document by multiplying this by \\nthe proportion of words that are “travel” in the document. (For 90% of \\nthe documents this percentage is zero and so the TF-IDF is zero.) The \\nword “policy” is handled similarly.  \\nFor each document, we can calculate a score as \\n \\nTF-IDF(“the”) + TF-IDF (“travel”) + TF-IDF (“policy”) \\n \\nThe documents are ranked by their score. The one with the highest \\nscore will be presented to the user first; the one will the next highest \\nscore will be presented second; and so on. Note that the word “the” \\nplays no role in determining which documents are returned. Also, if a \\ndocument contains neither “travel” nor “policy” it will have a score of \\nzero. Interestingly, with this simple information retrieval algorithm, the \\norder of the words input by the user makes no difference. \\n \\n \\n8.7   Other NLP Applications \\n \\nThere are many different NLP applications that have not been dis-\\ncussed in this chapter. In this section we briefly review a few of them. \\nIf we give a machine two different words such as “many” and “nu-\\nmerous”, how can it tell that they have a similar meaning? One way is by \\n                                                             \\n6 Logarithms are usually calculated using the base 2 in computer science. If another \\nbase is used all the IDFs are just multiplied by a constant and results are not affect-\\ned. \\nNatural Language Processing                                                                                             179 \\n \\n \\n \\nlooking at what other words they tend to be used with.7 We could for \\nexample, look at a large number of documents and ask the question: if \\nword X is in the document, what is the probability of word Y being \\nclose? By “close” we might mean that word Y has to be within five \\nwords of word X.  \\nIf there are 10,000 words in the vocabulary being used, this could \\nlead to a 10,000 by 10,000 table showing the probability of any one \\nword being close to any other word in a document.  We would expect \\nthe rows for two words that have similar meanings to be similar to each \\nother. It turns out that the information in such a table can be summa-\\nrized by a rather smaller table that is 10,000 by 300.8 (The procedure \\ndetermining the smaller table is similar to autoencoding which was dis-\\ncussed in Chapter 6.) The meaning of words can therefore be repre-\\nsented by what are termed word vectors with 300 entries. This is known \\nas word embedding. It turns out that these word vectors have nice addi-\\ntion and subtraction properties. For example, it is approximately true \\nthat the following relationship holds between vector representations:  \\n \\nking – man + woman = queen \\n \\nAnother application of NLP is to word sequences. It asks the ques-\\ntion: What is the probability of a particular word sequence such as “I \\nwill give you the answer to the question later today” occurring in text?  \\nClearly it is more likely than a sequence where the words have been \\njumbled such as “I you give will the answer later to the question today.” \\nThis has important applications in \\n  \\n\\uf0b7 \\ntranslating from one language to another,  \\n\\uf0b7 \\nspeech recognition \\n\\uf0b7 \\nusing NLP to summarize texts \\n\\uf0b7 \\nthe conversion of speech to text.  \\n \\nOne idea for estimating the probability of a word sequence is to see \\nhow often the text appears in a large number of documents. However, \\nthis is infeasible. The chance of a sequence of words such as the one just \\nmentioned appearing in even millions of pages of text is virtually zero. \\nWhat we have to do in practice is break the sentence down into subse-\\n                                                             \\n7 This was suggested by J.R. Firth in 1957, who is known for his famous quotation: \\n“You shall know a word by the company it keeps.” \\n8 Tables which have 50, 100, or 200 columns instead of 300 are sometimes used. \\n180                                                                                                                                     Chapter 8 \\n \\n \\n \\nquences of words. For example, we might consider the probability of \\noccurrence of “I will”, “will give”, “give you”, “you the”, etc.   \\nTranslating from one language top another is a very challenging NLP \\napplication. There are a number of approaches. Google’s GNMT system, \\nwhich was mentioned in Chapter 1, uses a long short-term memory re-\\ncurrent neural network (see Section 6.8). This proved to be a big im-\\nprovement over its previous system which involved translating on a \\nphrase-by-phrase basis.  The phrase-by-phrase system was in turn an \\nimprovement over a previous word-by-word translation approach.  \\n \\n \\nSummary \\n \\nNatural language processing (NLP) involves translating words into \\nnumbers so that they can be analyzed. One important application of \\nNLP is to sentiment analysis. This is concerned with determining the \\nnature of opinions such as those in reviews and tweets. The opinions \\ncan be classified as positive or negative, with neutral being a possible \\nthird category. Alternatively, they can be expressed on a numerical \\nscale (e.g. 1 to 5).  \\nOne of the most challenging aspects of sentiment analysis is obtain-\\ning relevant labeled opinions that can be used for training a model and \\ntesting it.  Sometimes publicly available data (e.g., from movie reviews) \\ncan be used. When this is not appropriate it is necessary to collect opin-\\nions have that have been made in the past and undertake the laborious \\ntask of classifying them manually.   \\nOpinions must be processed before they can be used in a model. This \\ninvolves such tasks as separating out the words, eliminating punctua-\\ntion, changing upper case letters to lower case, removing commonly \\noccurring words, and removing words that are very rare.  The result is a \\nvocabulary of words that will be used for classification.  \\nBag-of-words models are commonly used for sentiment analysis. \\nThe models classifying an opinion depend on whether or not each word \\nin the vocabulary is present in the opinion. Among the machine learning \\nmodels that can be used are the naïve Bayes classifier, SVM, logistic re-\\ngression, decision trees, and neural networks.   \\nSearch engines are an interesting application of NLP. The task is to \\nchoose the most relevant documents from a large number of possibili-\\nties from key words input by the user. Important statistics are the fre-\\nquency with which a particular key word appears in each document and \\nthe proportion of all documents in which the word appears.  \\n \\nNatural Language Processing                                                                                             181 \\n \\n \\n \\nSHORT CONCEPT QUESTIONS   \\n \\n8.1     What is meant by “sentiment analysis”? \\n8.2     What are the alternative ways of creating labels for text in a \\nsentiment analysis? \\n8.3 \\nList five ways in which text can be pre-processed for an NLP \\napplication. \\n8.4 \\nWhat is the difference between stemming and lemmatization? \\n8.5 \\nWhat is a bag-of-words model? \\n8.6 \\nWhy do negative words such as “not” cause a problem in a \\nbag-of-words model? \\n8.7 \\nWhat assumption is made when the naïve Bayes classifier is \\nused in sentiment analysis?  \\n8.8  \\nExplain what is meant by a “trigram.” \\n8.9  \\nGive one advantage of logistic regression over (a) SVM and (b) \\nthe naïve Bayes classifier.  \\n8.10 What problem is Laplace smoothing designed to deal with? \\n8.11 Explain how TF and IDF are used in information retrieval.  \\n8.12  What is a word vector? \\n \\n \\nEXERCISES \\n \\n8.13 Suppose that there are three words in a vocabulary and we \\nwish to classify an opinion that contains the first two words, \\nbut not the third, as positive or negative using the naïve Bayes \\nclassifier. The training set is as follows (1 indicates that the \\nopinion contains the word, 0 indicates that it does not): \\n \\nOpinion \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\nWord 1 \\n1 \\n1 \\n1 \\n0 \\n0 \\n0 \\n0 \\n1 \\n1 \\n1 \\nWord 2 \\n0 \\n0 \\n1 \\n1 \\n1 \\n0 \\n0 \\n1 \\n0 \\n0 \\nWord 3 \\n0 \\n0 \\n1 \\n1 \\n0 \\n0 \\n0 \\n0 \\n1 \\n0 \\nLabel \\nPos \\nPos \\nPos \\nPos \\nNeg \\nNeg \\nPos \\nPos \\nNeg \\nNeg \\n \\n \\nEstimate the probability that the opinion under consideration \\nis (a) positive and (b) negative.  \\n8.14 Download 1000 positive and 1000 negative reviews from \\nANDY LANDU NGOMA \\n182                                                                                                                                     Chapter 8 \\n \\n \\n \\n \\nand polarity data set v2.0.  Using the naïve Bayes classifier and \\nlogistic regression to develop a classification model for the \\nmovies.  \\n \\n \\n   \\n183 \\n \\n \\n \\n \\n \\nChapter 9 \\n \\nModel Interpretability \\n \\n \\n \\nSupervised learning models produce predictions, but they do not ex-\\nplain their predictions to users. In some cases, this is not important. For \\nexample, when a search engine predicts that a particular document will \\nbe what the user wants, understanding the underlying model is not im-\\nportant and there is a low cost to the user when the model makes a mis-\\ntake. But, in many other situations, understanding how predictions are \\nmade is desirable because mistakes are costly and confidence in the \\nmodel is important. As machine learning has become more widely used, \\nresearchers have started to devote a great deal of effort to model inter-\\npretability issues.1 \\nConsider the situation in Section 3.11 where machine learning is be-\\ning used to accept or reject loan applications. If an applicant for a loan is \\nrejected and asks the reason, it is not likely to be advisable for a bank \\nrepresentative to say: “The algorithm rejected you. Sorry I do not have \\nany more information.” The decision made by the algorithm has a po-\\ntentially high cost to the would-be borrower and the bank’s reputation \\n                                                             \\n1 A book providing an excellent discussion of model interpretability is C. Molnar, \\nInterpretable Machine Learning, 2020, ANDY LANDU NGOMA\\nml-book/. \\n \\n184                                                                                                                                      Chapter 9 \\n \\n \\n \\nis likely to suffer if the representative blames an algorithm for lending \\ndecisions.  \\nAs another example, we can consider the predictions for the price of \\na house in Iowa made in Section 3.8.  The person making use of a pre-\\ndiction might be a seller of the house, a buyer of the house, or a real es-\\ntate agent. In all cases, mistakes are liable to be costly and an individual \\nrelying on the prediction is likely to want to know how it was obtained.  \\nModel interpretability has been defined as the degree to which a \\nhuman being can understand a decision made by a model.2 One model is \\nmore interpretable than another if a human being can more easily un-\\nderstand its output. \\nHuman beings are naturally curious about the predictions made by \\nmachine learning models and want to learn from them. In some cases, \\nthere may be biases in the model (e.g., involving race or gender) that \\nare unacceptable.  Understanding these biases can lead to the model \\nbeing changed or used in a different way. Sometimes understanding an \\nunfavorable prediction can lead to a decision to take some action that \\nchanges a feature so that the outcome being predicted is improved. \\nIt is interesting to note that legislation can require model interpret-\\nability. The General Data Protection Regulation in the European Union, \\nwhich is discussed in Chapter 11, includes a “right to explanation” with \\nregard to machine learning algorithms applied to the data of citizens of \\nthe European Union.  Specifically, individuals have the right to “mean-\\ningful information about the logic involved in, as well as the significance \\nand the envisaged consequences of, such processing for the data sub-\\nject.” \\nSometimes, understanding a model can lead one to understand the \\nmodel’s limitations. As a simple example of this, consider image recog-\\nnition software that distinguishes between polar bears and dogs. It \\nmight be found that the model is making predictions by looking at the \\nbackground (ice vs. grass/trees) rather than the characteristics of the \\nanimals. Understanding this clearly indicates a limitation of the model.  \\nThere is an amusing story concerning a German horse, named Hans, \\nwho in the early 20th century appeared to be intelligent and able to \\nsolve mathematical problems (for example, the horse could add, sub-\\ntract, multiply, divide, and answer questions such as: “if the ninth day of \\nthe month is a Wednesday what day of the month is the following Fri-\\nday?”). Hans indicated answers by stomping his hoof a number of times \\n                                                             \\n2 See T. Miller “Explanation in artificial intelligence: Insights from the social scienc-\\nes.” (2017) arXiv: 1706.07269.  \\nModel Interpretability                                                                                                            185 \\n \\n \\n \\nand received a reward when the answer was correct. For some time, the \\nhorse was assumed to be intelligent and researchers studied the inter-\\nesting phenomenon of a horse that could hear mathematical questions \\nand correctly answer them.  Eventually, it was found that the horse’s \\nreal expertise was in reading the expressions on the face of the person \\nasking the questions. Subtle changes in expression led the horse to \\nknow when to stop stomping.  He did not actually have any mathemati-\\ncal intelligence. The horse can be considered analogous to a machine \\nlearning algorithm. Humans at first incorrectly interpreted why Hans \\ngave the answer he did.   \\nThe task of understanding a model can be distinguished from the \\ntask of understanding a particular prediction made by the model.  It is \\nimportant for companies to have some understanding of a machine \\nlearning model so that they can have confidence in the results and know \\nwhen the environment is such that the model is not applicable. It is also \\nimportant that particular predictions are explainable. \\nIn this chapter, we will distinguish between models that are intrinsi-\\ncally interpretable (white boxes) and models whose structures do not \\npermit easy interpretation (black boxes). The k-nearest neighbors algo-\\nrithm (see Section 3.12) is clearly in the first category. It is not difficult \\nfor someone to understand how the model works and any particular \\nprediction is easily explained.  It is essentially a “prediction-by-analogy” \\nmodel and corresponds to how many human beings make predictions. \\n(For example, a real estate agent in providing advice on the value of a \\nhouse is likely to use the prices obtained for similar houses that have \\nsold recently.)  \\nAs mentioned in Chapter 4, decision trees are fairly easy to explain \\nbecause they also correspond to the way humans sometimes make pre-\\ndictions. (It is easier for a human to consider one feature at a time ra-\\nther than all together.) Linear regression is also in the white-box cate-\\ngory because the weights that are derived have a simple interpretation.  \\nModels such as neural networks, SVM, and ensemble models such as \\nrandom forests are in the black-box category. There is no easy way for \\nus to understand how the models work or why the models make a par-\\nticular prediction.  \\n \\n \\n9.1   Linear Regression \\n \\n Linear regression is fairly easy to interpret, which in part explains \\nwhy it is so popular. The model is: \\n \\n186                                                                                                                                      Chapter 9 \\n \\n \\n \\n𝑌= 𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚 \\nwhere Y is the prediction of the target and Xj  (1 ≤ j ≤ m) are the features.  \\nThe weight, bj, can be interpreted as the sensitivity of the prediction to \\nthe value of the feature j. If the value of feature j increases by an amount \\nu with all other features remaining the same, the value of the target in-\\ncreases by bju. In the case of categorical features that are 1 or 0, the \\nweight gives the impact on a prediction of the target of changing the \\ncategory of the feature when all other features are kept the same.  \\nThe bias, a, is a little more difficult to interpret. It is the value of the \\ntarget if all the feature values are zero. However, feature values of zero \\nmake no sense in many situations. (For example, no houses have a lot \\nsize of zero or a first-floor square footage of zero.)  For interpretation \\npurposes, it is a good idea to redefine features so that the mean is sub-\\ntracted. The value of the jth feature then becomes  𝑋𝑗−𝑋̅𝑗 rather than \\n𝑋𝑗, where 𝑋̅𝑗 is the mean of the feature value calculated from all items in \\nthe training set.3 The regression model becomes: \\n \\n𝑌= 𝑎∗+ 𝑏1(𝑋1 −𝑋̅1) + 𝑏2(𝑋2 −𝑋̅2) + ⋯+ 𝑏𝑚(𝑋𝑚−𝑋̅𝑚)     (9.1) \\nThe new bias a* is \\n𝑎∗= 𝑎+ 𝑏1𝑋̅1 + 𝑏2𝑋̅2 + ⋯+ 𝑏𝑚𝑋̅𝑚 \\nThis bias does have an easy interpretation. It is the value of the tar-\\nget when all features have their average values.  In the case of linear \\nregression, it is also the average value of the target. \\nFor a more sophistical interpretation of a linear regression model we \\ncan use the statistics mentioned in Section 3.2 that are calculated when \\nlinear regression models are implemented. R-squared is an indication of \\nthe overall performance of the model and therefore how much it can be \\nrelied upon. The t-statistics for the weights can be used to provide con-\\nfidence limits for the sensitivities. Suppose bj is 10 with a t-statistic of 5. \\nBecause the t-statistic is the weight divided by its standard error, the \\nstandard error is 2. We know that the best estimate of the effect of a \\nchange u in the value of the jth feature on the value of the target is 10u \\nbut it could be as low as 6u or as high as 14u.4   \\n                                                             \\n3 We could use Z-score scaling for the features so that, in addition to subtracting the \\nmean, we divide by the standard deviation. However, this may make the weights \\nmore difficult to interpret. \\n4 The value is within two standard deviations of the mean about 95% of the time \\nwhen the data set is large.  \\nModel Interpretability                                                                                                            187 \\n \\n \\n \\nSo far we have focused on how the model can be understood. We \\nnow suppose that we want to explain a particular prediction.  A natural \\napproach is to use equation (9.1). The impact of feature j on a predic-\\ntion, when the average value of the feature is used as a benchmark is \\n𝑏𝑗(𝑋𝑗−𝑋̅𝑗) \\nAs the feature’s value, 𝑋𝑗, moves further away from its average value of \\n𝑋̅𝑗, its impact on the prediction made for the target increases. An analyst \\ncan take account of the standard error of bj to produce a range of possi-\\nble values for the impact of feature j on a particular prediction. \\nModel interpretability is an important reason for using regulariza-\\ntion methods such as Ridge and Lasso. Consider the Iowa house price \\nexample in Chapter 3. The regression with 47 features in Table 3.6 does \\nnot include any regularization, but it happens to generalize quite well to \\nthe validation set with the result that the predictions it produces are \\nquite reasonable.5  However, the negative weights in the model (e.g., for \\nnumber of bedrooms), which arise from correlations between features, \\nwould be difficult to explain. (Indeed, the model might be dismissed by \\nsome users as being ridiculous!) The regularized model in Table 3.7, \\nwhich is produced using Lasso, is much easier to explain.6  \\nTable 9.1 shows how results might be tabulated when the model in \\nTable 3.7 is used to price a house. For completeness, two features ap-\\npear in Table 9.1 that were not in Table 3.7. These are wood deck (sq. \\nft.) and open porch (sq. ft.). They had very small (but non-zero) weights \\nof 0.001 and 0.002, respectively in the Python Lasso implementation on \\nscaled data when \\uf06c=0.1.   \\nThe house of interest is assumed to have a lot area of 15,000 square \\nfeet, an overall quality of 6, and so on. To improve interpretability, the \\nZ-score scaling used for Table 3.7 has been reversed in Table 9.1. (It will \\nbe recalled that scaling was necessary for Lasso.) This means that the \\nfeature weights in Table 3.7 are divided by the standard deviation of the \\nfeature and multiplied by the standard deviation of the house price to \\nget the feature weights in Table 9.1. \\nThe table shows that the average lot area for the houses in the train-\\ning set is 10,249 square feet, compared with 15,000 square feet for the \\n                                                             \\n5 Note that it is not always the case that a model without any regularization general-\\nizes well. \\n6 Lasso regularization is also particularly useful from the perspective of model in-\\nterpretability because it reduces the number of features making it easier for a user \\nto understand the model. \\n188                                                                                                                                      Chapter 9 \\n \\n \\n \\nhouse of interest. The weight for lot area is 0.3795 ($ per square foot). \\nThis means that the contribution of lot area to the value of the house is \\n$0.3795 × (15,000 −10,249) or $1,803. \\n \\nTable 9.1   Impact of different features on the value of a particular \\nhouse in Iowa when the model in Table 3.7 is used.  See Excel file. \\nFeature \\nHouse \\nvalue \\nAverage \\nvalue \\nFeature \\nweight \\nContrib-\\nution ($) \\nLot area (sq. ft.) \\n15,000 \\n10,249 \\n0.3795 \\n+1,803     \\nOverall quality (1 to 10) \\n6.0 \\n6.1 \\n16,695 \\n−1,669 \\nYear built  \\n1990 \\n1972 \\n134.4 \\n+2,432 \\nYear remodeled \\n1990 \\n1985 \\n241.2 \\n+1,225 \\nFinished base (sq. ft.) \\n1,200 \\n444.0 \\n20.42   +15,437 \\nTotal basement (sq. ft.) \\n1,300 \\n1,057 \\n19.08 \\n+4,630 \\nFirst floor (sq. ft.) \\n1,400 \\n1,159 \\n6.666 \\n+1,609 \\nLiving area (sq. ft.) \\n2,000 \\n1,503 \\n46.79   +23,273 \\nNumber of fireplaces \\n0 \\n0.6089 \\n2,452 \\n −1,493 \\nParking spaces  \\n2 \\n1.774 \\n2,857 \\n      +646 \\nSize of garage (sq. ft.) \\n600 \\n474.8 \\n24.42 \\n  +3,055 \\nWood deck (sq. ft.) \\n0 \\n93.62 \\n0.6364         −60 \\nOpen porch (sq. ft.) \\n0 \\n46.59 \\n2.562 \\n      −119 \\nNeighborhood 1 \\n0 \\n0.02611 \\n6,395 \\n      −167 \\nNeighborhood 2 \\n0 \\n0.05944 \\n27,523    −1,636 \\nNeighborhood 3 \\n0 \\n0.01611 \\n10,311        −166 \\nBasement quality \\n5 \\n3.497 \\n1,820 \\n  +2,734 \\nTotal \\n \\n \\n \\n  +51,532 \\n \\nThe model predicts that the price of the house of interest (with the \\nfeature values in the second column of Table 9.1) is $232,349. The value \\nof an “average house” with the feature values in the third column of Ta-\\nble 9.1 is $180,817. This house is therefore worth $51,532 more than \\nthe average house. The final column of Table 19.1 shows the contribu-\\ntion of each feature to the difference between the value of the house and \\nthe value of an average house. (The calculation is shown for the first \\nfeature, lot area, above.) A key point is that for a linear model the sum of \\nthese contributions equals the difference. (As we discuss later non-\\nlinear models do not have this property.) \\nThe table shows that, for the particular house being considered, the \\nfinished basement and living area add most to the value. Many other \\nresults can be deduced from the table. For example, the table shows \\nthat, if the house had been in neighborhood 2 (the most expensive \\nModel Interpretability                                                                                                            189 \\n \\n \\n \\nneighborhood), the house would have been worth an extra $27,523. \\n(This is because the feature value for neighborhood 2 would be 1 rather \\nthan 0.) \\nIt should be noted that the model in Table 9.1, although much better \\nthan the original model in Table 3.6, is by no means perfect. It still has \\nsome features that are not independent of each other. For example, total \\nbasement (sq. ft.) and first floor (sq. ft.) are correlated. When one is high \\n(low), the other is likely to be high (low).7  In considering the effect of \\ndifferent features, it does not really make sense to consider the effect of \\nthe total basement (sq. ft.) increasing without first floor (sq. ft.) also \\nincreasing. There is no simple answer to this problem. We could consid-\\ner grouping two or more features together when considering changes. \\nSometimes a principal components analysis or an autoencoder can sug-\\ngest a way to redefine the features so that they are independent (or \\nnearly independent). \\n \\n9.2   Logistic Regression \\n \\n \\nThe prediction from logistic regression is the probability of a posi-\\ntive outcome, not a value. As explained in Section 3.9, \\n \\nProb (Positive Outcome) = \\n1\\n1 + exp[− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)] \\nwhere Xj is the value of feature j, bj is its weight and a is the bias. It fol-\\nlows from this that  \\nProb (Negative Outcome) = \\nexp[− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)]\\n1 + exp[− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)] \\nWe can calculate the sensitivities of these probabilities to the value \\nof a feature analytically. For example, using a little calculus, it can be \\nshown that when feature j increases by a small amount u, the probabil-\\nity of a positive outcome increases by \\nexp[− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)]\\n{1 + exp [− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)]}2 𝑏𝑗𝑢 \\nThis is  \\n                                                             \\n7 The correlation between the feature values is about 0.79. \\n190                                                                                                                                      Chapter 9 \\n \\n \\n \\nProb (Positive Outcome) ×  Prob (Negative Outcome) × 𝑏𝑗𝑢   (9.2) \\nUnfortunately, this is only accurate for small values of u because the \\nrelationship between probability and feature values is non-linear. \\nAn approach to overcoming the non-linearity problem is to work in \\nterms of odds. If the probability of an event is p, the odds against it hap-\\npening are (1 − p)/p to 1. “Odds against” tend to be used when the event \\nhas a probability less than 0.5. For example, the odds against the throw \\nof a dice providing a six is 5 to 1. This would be stated as “5 to 1 against” \\nand means that a fair bet of $1 should (a) provide a payoff of $5 and re-\\nturn the $1 wager if a six is thrown and (b) lead to a loss of the $1 wager \\nin other circumstances.    \\nThe odds in favor of an event (often used when the event has a prob-\\nability greater than 0.5) are p/(1 − p) to 1. Thus, the odds in favor of a \\ndice giving 1, 2, 3, or 4 is 2 to 1.  This would be stated as “2 to 1 on” and \\nindicates that a fair bet of $2 will return $1 plus the $2 wager or noth-\\ning.  \\nThe logistic regression equations show that the odds of a positive re-\\nsult are \\nexp[− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)] to 1 against \\nor \\nexp(𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚) to 1 on \\nThis shows that the natural logarithm of odds are linear in the features: \\nln(odds against) = − (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚) \\nln(odd on) = 𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚 \\nAs in the case of linear regression, it makes sense to redefine fea-\\ntures so that the value of the jth feature is  𝑋𝑗−𝑋̅𝑗 rather than 𝑋𝑗 where \\n𝑋̅𝑗 is the mean of the feature value calculated from all items in the train-\\ning set. We can analyze the logarithm of odds in the same way that pre-\\ndictions are analyzed in linear regression. When the analog of Table 9.1 \\nis produced, we will be able to see the contribution of each feature’s \\ndeviation from its average to the logarithm of the odds.  \\nWorking with the logarithm of odds is a little artificial. Instead, we \\ncan note that the percentage effect on “odds against” of increasing the \\nvalue of a feature by u is exp(−𝑏𝑗𝑢) −1.  Similarly, the percentage effect \\non “odds on” of increasing the value of a feature by u is exp(𝑏𝑗𝑢) −1. \\nWhen the percentage effects from a number of different features are \\nmultiplied together we get the total percentage effect. \\nModel Interpretability                                                                                                            191 \\n \\n \\n \\n Odds can be converted to probabilities: \\nProbability = \\n1\\n1 + odds against \\nand \\nProbability = \\nodds on\\n1 + odds on \\nWe can illustrate these results with the model in Section 3.11. The \\nprobability of a loan not defaulting (which we defined as the positive \\nresult) is  \\n1\\n1 + exp[−(−6.5645 + 0.1395X1 + 0.004107X2 −0.001123X3 + 0.01125𝑋4)]   \\n(9.3) \\nwhere X1 is a categorical variable indicating home ownership, X2 is in-\\ncome in ($’000s), X3 is debt to income ratio, and X4 is credit score. Con-\\nsider someone who does not own a home (X1 = 0) with an income of \\n$60,000, a debt to income ratio of 5, and a credit score of 670. Substitut-\\ning these values into the formula, the probability that the loan will not \\ndefault is estimated as 0.7711.  Consider what happens if the credit \\nscore increases by 10 from 670 to 680. From equation (9.2), the in-\\ncrease in this probability can be estimated as \\n0.7711 × 0.2289 × 0.01125 × 10 = 0.01986 \\nThis is an approximate answer because probability is a non-linear func-\\ntion of the features. Substituting into equation (9.3) shows that increas-\\ning X4 to 680 changes the probability to 0.7903 so the exact increase in \\nprobability given by the model is 0.7903 −0.7711 or 0.01925.  \\nThe odds of a positive outcome are 3.369 to 1 on because 3.369 = \\n0.7711/(1 − 0.7711). We know that, when the credit score increases by \\n10, ln(odds on) increases by 0.01125 × 10 = 0.1125 (from 1.2145 to \\n1.3270). Alternatively, the percentage increase in the odds on could be \\ncalculated as exp(0.01125 × 10) −1 or 11.91%. Either of these results \\ncan be used to calculate the new odds on and if desired this can be con-\\nverted to a probability.  \\nThe same analysis can be applied to categorical features. Suppose \\nthat, in our example, we wonder how much better the lender’s position \\nwould be if the borrower owned a house.  The percentage increase in \\nthe odds on that the loan will not default is, from the coefficient of X1 in \\n192                                                                                                                                      Chapter 9 \\n \\n \\n \\nequation (9.3), exp(0.1395) −1 or 14.97%. This means that the odds \\nincrease from 3.369 to 1 on to 3.873 to 1 on. The new probability is \\n3.873 (1 + 3.873)\\n⁄\\n or 0.7948. \\n \\n \\n9.3   Black-box Models \\n \\nFor a black-box model, the measures needed to understand the \\nmodel must usually be calculated numerically.8 The effect on a predic-\\ntion of making a change to a feature value in a particular situation can \\nbe calculated by re-running the model.  For the linear model in Section \\n9.1, this is independent of the values of the features, but for non-linear \\nmodels it depends on the feature values. \\nAnalogously to what we did in Section 9.1, we can provide a measure \\nof the contribution of each feature in a particular situation by calculat-\\ning the change in the prediction when the feature is changed from its \\naverage value to its actual value with all other features remaining at \\ntheir average values.  In Table 9.1, the sum of the contributions of dif-\\nferent features equals the difference between the actual prediction and \\nthe average prediction. As a result, the difference between the predic-\\ntion and one based on average feature values is neatly divided into a \\nnumber of components. For a non-linear model, calculating contribu-\\ntions using the approach in Table 9.1 does not have this simple property \\nand we must use a more complicated calculation, which will be ex-\\nplained in Section 9.4.  \\nA key point about black-box models is that the relationship between \\nthe prediction and a feature’s value may not be linear. It may not even \\nbe monotonic. For example, a store could find that one of the features \\naffecting its sales is temperature and that the average effect of tempera-\\nture has the form shown in Figure 9.1. This shows that, when the tem-\\nperature is very low or very high, the volume of sales declines.    \\nIn a particular situation, the impact of feature j can be determined \\nchanging feature j while keeping all features except feature j fixed. An \\nextension of this idea is where two features are considered simultane-\\nously so that a three-dimensional plot is obtained.   \\nTo provide an overall understanding of the role of feature j in a \\nmodel (as opposed to its role when other features have particular val-\\nues), we can calculate many predictions where \\n                                                             \\n8 But note that in the case of neural networks, partial derivatives can be calculated \\nby working forward through the network and using the chain rule. \\nModel Interpretability                                                                                                            193 \\n \\n \\n \\n \\n \\n\\uf0b7 Feature j has a particular value xj. \\n\\uf0b7 The values of the other feature are chosen randomly.  \\n \\nBy averaging across all these predictions, we obtain the expected pre-\\ndiction conditional on feature j equaling xj. By considering a number of \\ndifferent values for xj we are then able to plot the expected prediction as \\na function of xj. This is known as a partial dependence plot. (In the case \\nof linear regression, it is a straight line.) \\n \\nFigure 9.1   Possible effect of temperature on a store’s sales  \\n \\n \\n9.4   Shapley Values \\n \\nLinear regression models have the property, illustrated in Table 9.1, \\nthat a simple calculation leads to the result that the sum of the contribu-\\ntions of the features to the change in a prediction equals the change. \\nNon-linear models, as already mentioned, do not have this convenient \\nproperty.  However, what are known as Shapley values show that a \\nmore complicated calculation of the contributions of features leads to a \\nresult where the property does hold.  Shapley values are based on the \\nwork of Lloyd Shapley in the early 1950s concerned with game theory.9  \\nWe can illustrate the nature of the calculations with a simple exam-\\nple. Suppose that there are three features.  When the features have their \\n                                                             \\n9 See L. S. Shapley, “A value for n-person games.” Rand Corporation, 1952.  \\nTemperature\\nSales\\n194                                                                                                                                      Chapter 9 \\n \\n \\n \\naverage values a black-box model gives a prediction of 100. In a situa-\\ntion, which we will refer to as “current”, the features have values that \\nare different from average and the black-box model leads to a predic-\\ntion of 140. What is the contribution of the features to the 40 in-\\ncrease?  \\nTo investigate this, we re-run the model to calculate the predic-\\ntion for every situation where some features have their average val-\\nues and some have their current values. We suppose that the results \\nare as indicated in Table 9.2. \\nWe next consider the sequence in which the features move from \\ntheir average values to their current values. Denote XYZ by the situation \\nwhere feature X changes first, then feature Y changes, and then feature \\nZ changes. There are six possibilities in our example: 123, 132, 213, 231, \\n312, and 321. In the first two cases (123 and 132), feature 1 is changed \\nfrom average to current while the other two features stay at their aver-\\nage values. From the first and fifth row of Table 9.2, the contribution of \\nfeature 1 is 110−100 = 10 in this situation. In the third case (213), fea-\\nture 1 is changed after feature 2 has been changed, but before feature 3 \\nis changed. From rows 3 and 7 of Table 9.2, the contribution of feature 1 \\nis 137 −125 =12 in this situation. Other similar calculations are shown \\nin Table 9.3. It can be seen that the total average contribution of the fea-\\ntures (= 10 + 18.5 + 11.5) equals the total increase, 40, that has to be \\nexplained. This is always the case. \\n \\nTable 9.2   Results from running model with different combinations of \\naverage and current values \\nFeature 1 \\nValue \\nFeature 2 \\nValue \\nFeature 3 \\nValue \\nPrediction \\nAverage \\nAverage \\nAverage \\n100 \\nAverage \\nAverage \\nCurrent \\n120 \\nAverage \\nCurrent \\nAverage \\n125 \\nAverage \\nCurrent \\nCurrent \\n130 \\nCurrent \\nAverage \\nAverage \\n110 \\nCurrent \\nAverage \\nCurrent \\n128 \\nCurrent \\nCurrent \\nAverage \\n137 \\nCurrent \\nCurrent \\nCurrent \\n140 \\n \\n \\nModel Interpretability                                                                                                            195 \\n \\n \\n \\n \\nTable 9.3   Contribution of feature for different sequences in which fea-\\ntures are changed from the average value to the current value  \\nSequence \\nFeature 1 \\nContribution \\nFeature 2  \\nContribution \\nFeature 3 \\nContribution \\n123 \\n10 \\n27 \\n  3 \\n132 \\n  10 \\n12 \\n18 \\n213 \\n  12 \\n25 \\n  3 \\n231 \\n  10 \\n25 \\n  5 \\n312 \\n    8 \\n12 \\n20 \\n321 \\n            10 \\n10 \\n20 \\nAverage \\n  10 \\n 18.5 \\n11.5 \\n \\nThis calculation of the contribution of the features using Shapley \\nvalues has nice properties. One is the property we have just illustrated \\nthat the sum of the contributions equals the total change. Other attrac-\\ntive properties are \\n \\n\\uf0b7 If a feature never changes, the prediction its contribution is ze-\\nro.  \\n\\uf0b7 If two features are symmetrical in that they affect the prediction \\nin the same way, they have the same contribution.  \\n\\uf0b7 For an ensemble model where predictions are the average of \\npredictions given by several underlying models, the Shapley \\nvalue is the average of the Shapley values for the underlying \\nmodels.  \\nAs the number of features increases, the number of calculations to \\ndetermine Shapley values increases quite fast making it computational-\\nly expensive to use them.10 Shapley values can be used to explain why \\nany two predictions are different (i.e., the benchmark does not have to \\nbe a prediction based on average feature values). However, they have a \\nlimited ability to explain the workings of the model as a whole. Also, \\ninteractions between features can cause unrealistic combinations of \\nfeature values to be considered.11  \\n                                                             \\n10 When there are m features, there are m! sequences and 2𝑛 different contributions \\nto calculate. \\n11 All models have this problem. We explained in Section 9.1 that this happens in \\nTable 9.1. Changing total basement (sq. ft.) without changing first floor (sq. ft.) cre-\\nates an unrealistic set of feature values.   \\n196                                                                                                                                      Chapter 9 \\n \\n \\n \\n \\n9.5   LIME \\n \\nLocal Interpretable Model-agnostic Explanations (LIME) are an al-\\nternative approach to explaining the predictions made by black-box \\nmodels.12 LIME tries to understand a model’s prediction by creating a \\nsimpler more interpretable model that works well for values of the fea-\\ntures that are close to those being considered. \\nThe procedure is as follows: \\n \\n\\uf0b7 Perturb the current values of the features to create sample val-\\nues for the features \\n\\uf0b7 Run the black-box model to get predictions from the samples \\n\\uf0b7 Train an easy-to-interpret model (e.g. linear regression or deci-\\nsion trees) using the samples and their predictions  \\nPredictions in the region of interest can then be explained using the \\nnew model. The samples can be weighted according to their closeness \\nto the current values of the features. The new model will often contain \\nless features than the original model (e.g., because it uses Lasso).  \\n \\nSummary \\n \\nAn important aspect of machine learning is interpretability. Some \\nmodels such as k-nearest neighbors, linear regression, and decision \\ntrees are fairly easy to interpret. Logistic regression is not quite as \\nstraightforward, but there is an analytic formula relating inputs to out-\\nputs and so any required property of the model can easily be derived.  \\nModels such as neural networks are black boxes. There is no simple way \\nof understanding how outputs are related to inputs.  \\nOne question that can be asked of a model is “What happens if the \\nvalue of feature j is changed with all other features remaining the \\nsame?” This is easy to answer in the case of linear regression. The \\nweight of feature j times the change in the value of the feature equals \\nthe impact of the change on the prediction. This is true for both small \\n                                                             \\n12 See M.T. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust you? Explaining the \\npredictions of any classifier.” Proceedings of the 22nd ACM SIGKDD international \\nconference on knowledge discovery and data mining (2016). \\nModel Interpretability                                                                                                            197 \\n \\n \\n \\nand large changes and does not depend on current feature values. In the \\ncase of logistic regression, it is sometimes easier to work in terms of \\nodds rather than probabilities when estimating the impact of feature-\\nvalue changes. \\nIn black-box models, the relationship between a feature’s value and \\npredictions can be highly non-linear. To understand the relationship for \\na particular prediction, an analyst can repeatedly run a model to inves-\\ntigate the effect of changing the value of the feature while keeping all \\nother feature values fixed. To provide a broader understanding of the \\nfeature’s effect, the values of the other features can be randomized in \\nsome way. \\nTo understand why one set of feature values produces a different \\nprediction from another set of feature values, one can consider the con-\\ntribution of each feature separately. However, when the model is non-\\nlinear the sum of the contributions does not equal the total difference \\nthat is to be explained. Shapley values provide a way of overcoming this \\nproblem so that the overall change in a prediction is exactly allocated to \\nthe contributions of different features. \\nA recent idea in model interpretability is referred to as LIME. This \\ninvolves understanding how a model works when features have values \\nclose to their current values. The approach is to perturb the current \\nvalues and re-run the model so that a new data set describing output \\nfrom the model in the region of interest is created. An interpretable \\nmodel such as linear regression or decision trees is then fitted to this \\ndata set. \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n9.1    Which of the models introduced in this book are most difficult to \\ninterpret? \\n9.2   In what ways is a linear model simpler to interpret than a non-\\nlinear model? \\n9.3 \\nHow much is it worth to have an extra 5,000 square feet of back \\nyard in Iowa? \\n9.4    In logistic regression, what is the equation for the sensitivity of \\nthe probability of a negative outcome to a very small change in \\nthe feature value? \\n \\n9.5   Explain what is meant by “odds against” and “odds on.” Why \\nmight they be useful concepts for interpreting logistic regression \\nmodels? \\n198                                                                                                                                      Chapter 9 \\n \\n \\n \\n9.6    “Interactions between features create problems when the contri-\\nbutions of features to the change in a prediction is calculated.” \\nExplain this statement. \\n9.7 \\nExplain what a partial dependence plot is and how it is calculated. \\n9.8 \\nWhat are the advantages of using Shapley values in model inter-\\npretability? \\n9.9     Explain the LIME approach to model interpretability. \\n9.10  How many different sequences have to be considered when Shap-\\nley values are calculated for four features?  \\n \\n \\nEXERCISES \\n \\n9.11 For the logistic regression model in Table 3.9, use Shapley values \\nto calculate the contribution of each feature to the probability of a \\npositive result for the person considered in Section 9.2 (no home, \\nincome equals $60,000, debt-to-income ratio is 5, and a credit \\nscore is 670). Use a person with average feature values as the \\nbenchmark.  \\n9.12 Use the LIME approach to calculate a local model for the person \\nconsidered in Section 9.2 (no home, income equals $60,000, debt-\\nto-income ratio is 5, and a credit score is 670) \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n199 \\n \\n \\n \\n \\nChapter 10 \\nApplications in Finance \\n \\n \\n \\n \\nUntil a few years ago, my research was almost exclusively concerned \\nwith finance, in particular derivatives markets. It then became apparent \\nto me that machine learning was having a bigger and bigger impact on \\nfinance and derivatives. I started to learn about machine learning and \\nthis book is one of the results of that.  \\nThis chapter will give a flavor for some of the applications of machine \\nlearning that are starting to revolutionize finance by describing two ap-\\nplications in detail. These applications are simplified versions of research \\nthat I have been involved in. (They involve derivatives, but readers with \\nlittle knowledge of this area need not be concerned as the chapter pro-\\nvides the necessary background.) Data and Python code for the applica-\\ntions is at  \\nANDY LANDU NGOMA \\nThe reader is encouraged to use this to explore the applications further.  \\nThe chapter concludes by summarizing a few of the many other ways \\nthat machine learning is used in finance. \\n \\n \\n10.1   Derivatives  \\n \\nMost financial and other transactions involve the immediate, or al-\\nmost immediate, exchange of an asset for cash.   Derivative transactions \\n200                                                                                                                                   Chapter 10 \\n \\nare by their nature different. They involve two parties agreeing to an ex-\\nchange in the future rather than immediately. \\nThe exchange agreed to by the two parties in a derivative transaction \\nusually involves one or more financial assets. The value of the transaction \\ntherefore depends on (or derives from) the value of these underlying fi-\\nnancial assets. Examples of underlying financial assets that are fre-\\nquently used in derivative transactions are stocks, portfolios of stocks, \\ncommodities, and currencies.  \\nAn important and popular derivative is an option.  This gives one side \\nthe right (but not the obligation) to buy an asset from the other side, or \\nsell an asset to the other side, for a certain price at a certain future time.1  \\nThe price in the contract is known as the strike price. \\nThe right to buy is termed a call option.  An example of a call option is \\nwhere Party A obtains the right to buy a certain stock for $50 in six \\nmonths from Party B. The current price of the stock might be greater than \\nor less than $50. The option will be exercised if the stock price in six \\nmonths, ST, is greater than $50 for a gain equal to ST −50.2 Party A is re-\\nferred to as having bought the call option or having a long position in the \\ncall option. Party B is referred to as having sold the call option or having \\na short position in the call option.  Party A would pay an amount upfront \\nto Party B to acquire the option.  \\nThe right to sell is termed a put option. An example of a put option is \\nwhere Party A obtains the right to sell a stock for $50 in six months. (As \\nin the case of the call option, the current price of the stock might be more \\nor less than $50.) The option will be exercised if the stock price in six \\nmonths, ST, is less than $50 for a gain equal to 50−ST.3 Party A is referred \\nto as having bought the put option or having a long position in the put \\noption. Party B is referred to as having sold the put option or having a \\nshort position in the put option.  As in the case of a call option, Party A \\nwould pay the cost of the option to Party B upfront.  \\nFigure 10.1 shows the payoffs to the purchaser from call and put op-\\ntions as a function of the asset price at option maturity when the strike \\nprice is $50. It can be seen that the payoff is fundamentally different from \\ninvesting in the stock itself for the life of the option. In the case of an in-\\nvestment in the stock, the gain from a certain increase in the price equals \\nthe loss from the same decrease in price.  In the case of an option this is \\n                                                           \\n1 The options that we will consider throughout this chapter are all what are termed \\nEuropean options. These can be exercised at only one future time.  \\n2 Party A could monetize this gain by buying the stock under the terms of the option \\nand immediately selling it in the market.  \\n3 Party A could monetize this gain by buying the stock in the market and immediately \\nselling it under the terms of the put option. \\n201 \\n                                         Applications in Finance \\n \\n \\n \\nnot so. There is a lack of symmetry. The payoff from favorable price \\nmovements during the life of the option can be very high whereas that \\nfrom unfavorable movements is at worst zero. To put this another way, \\nthe purchaser of an option cannot lose more than the price paid for it \\nwhile the possible gain can be substantial. \\n \\nFigure 10.1   Payoff from (a) a call option to buy a stock for $50 and (b) \\na put option to sell a stock for $50 \\n \\n(a)                                                                      (b) \\n \\nWhen the volatility of the price of the underlying asset is high, big \\nmovements in the asset price are possible and the lack of symmetry just \\nmentioned is valuable to the option holder. When the volatility is low, the \\nlack symmetry is still present, but it is less valuable because big price \\nmovements are less likely.  \\nThe price that has to be paid for an option by the buyer to the seller \\ntherefore depends on an estimate of the volatility of the asset price. As \\nthe volatility increases, the price increases. This dependence of option \\nprices on volatility is what makes the analysis of options and other deriv-\\natives more complicated (and more interesting) than that of other sim-\\npler instruments. \\nDefine K as the strike price of an option and S as the price of the un-\\nderlying asset. When K < S, a call option is commonly referred to as in-\\nthe-money because, if it were possible to exercise it immediately, there \\nwould be a positive payoff.  When K > S, a call option is similarly referred \\nto as out-of-the-money.  For a put option, the reverse is true: when K < S, \\na put option is out-of-the-money and when K > S, it is in-the money. The \\nextent to which an option is in- or out-of-the-money (i.e., the relative val-\\nues of K and S) is referred to as the option’s moneyness.   \\n \\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n20\\n40\\n60\\n80\\n100\\nOption \\nPayoff \\nAsset Price\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n20\\n40\\n60\\n80\\n100\\nOption \\nPayoff\\nAsset Price\\n202                                                                                                                                   Chapter 10 \\n \\n10.2   Delta \\n \\nAn important parameter in derivatives markets is delta. This is the \\nsensitivity of the value of a derivatives portfolio dependent on a particu-\\nlar underlying asset to the price of that asset.  If a small increase in the \\nprice of the underlying asset equal to \\uf044S causes the value the portfolio to \\nincrease by \\uf044P, the delta of the portfolio is ∆𝑃/∆𝑆. \\nA portfolio with a delta of zero has the property that it is not sensitive \\nto small changes in the price of the underlying asset and is referred to as \\ndelta-neutral. Traders usually try and make their portfolios delta-neutral, \\nor close to delta-neutral, each day. They can do this by trading the under-\\nlying asset. Suppose that the delta measured for a derivatives portfolio is \\n−4,000, indicating that a small increase, \\uf044S, in the price of the underlying \\nasset will lead to a decrease in the value of the portfolio equal to 4,000 ×\\n∆𝑆. Taking a long position in 4,000 units of the asset will lead to a delta-\\nneutral portfolio. The gain (loss) on the derivatives is then offset by the \\nloss (gain) on the new position taken. \\nThe most famous model for valuing options is the Black−Scholes− \\nMerton model, which we used to illustrate neural networks in Chapter 6. \\nThis is a relationship between the price of the option and the following \\nvariables. \\n  \\n\\uf0b7 Asset price, S \\n\\uf0b7 Strike price, K \\n\\uf0b7 Risk-free interest rate, r \\n\\uf0b7 Volatility, \\uf073  \\n\\uf0b7 Life of the option, T \\n\\uf0b7 The income expected by the market on the asset. We will assume \\nthat the underlying asset provides a constant yield (i.e., income \\nas a percent of its price is constant) and will denote this yield by  \\nq \\n \\nThe Black−Scholes−Merton model assumes that the rate of return in \\na short period of time, \\uf044t, is normally distributed with a mean of \\uf06d\\uf044t and \\na standard deviation of σ√∆𝑡. (Magically, as can be seen from equation \\n(6.3), the mean return does not enter into the equation for the price of \\nthe option.)  The delta of call and put options are \\nDelta (call) = 𝑒−𝑞𝑇𝑁(𝑑1) \\nDelta (put) = 𝑒−𝑞𝑇[𝑁(𝑑1) −1] \\nwhere \\n203 \\n                                         Applications in Finance \\n \\n \\n \\n𝑑1 = ln(𝑆𝐾\\n⁄ ) + (𝑟−𝑞+ 𝜎2 2\\n⁄ )𝑇\\nσ√𝑇\\n \\n \\nThe delta of a long position in a call option and a put option as a func-\\ntion of S/K  is shown in Figure 10.2. Suppose for simplicity that q = 0. For \\na call option, delta is close to zero when it is deeply out of the money (S/K \\nconsiderably less than 1) and close to one when it is deeply in the money \\n(S/K considerably greater than one). For a put option, the pattern of del-\\ntas is similar except that, instead of ranging from 0 to 1, they range from \\n−1 to 0 \\nIn addition to using delta for hedging, traders also use delta to meas-\\nure moneyness.4  We will do this in what follows. A call option with a \\ndelta of 0.5 is referred to as at-the-money. When delta is greater than 0.5, \\nit is in-the-money and, when delta is less than 0.5, it is out-of-the-money.   \\nSimilarly, a put option with a delta of −0.5 is referred to as at-the-money, \\none with a delta less than −0.5 is in-the-money, and one with a delta \\ngreater than −0.5 is out-of-the-money.  \\n \\nFigure 10.2   Variation of delta with S/K when q = 0 for (a) a long position \\nin a call option and (b) a long position in a put option \\n \\n \\n(a)                                                                      (b) \\n \\n \\n10.3   Volatility Surfaces \\n \\nOf the six variables listed in the previous section that determine op-\\ntion prices in the Black−Scholes model, all except \\uf073 are known. (The in-\\ncome expected by the market on an asset can be estimated from futures \\n                                                           \\n4 As mentioned earlier, a common definition of moneyness simply reflects whether \\nS > K or S < K. Delta is a more sophisticated measure preferred by traders. \\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0.5\\n0.7\\n0.9\\n1.1\\n1.3\\n1.5\\nDelta\\nS/K\\n-1\\n-0.8\\n-0.6\\n-0.4\\n-0.2\\n0\\n0.5\\n0.7\\n0.9\\n1.1\\n1.3\\n1.5\\nDelta\\nS/K\\n204                                                                                                                                   Chapter 10 \\n \\nor forward contracts.) Option traders therefore play a game where they \\nobserve an option price in the market and then calculate from the \\nBlack−Scholes−Merton model the value of \\uf073\\uf020that is consistent with the \\nprice. This value of \\uf073 is known as the option’s implied volatility.5 Very of-\\nten option prices are quoted in terms of their implied volatilities.6  The \\nprice of the option must then be obtained from the implied volatility by \\nsubstituting it into the Black−Scholes−Merton model, together with the \\nother known parameters.  \\nThe Black−Scholes−Merton model assumes that the volatility, \\uf073, is \\nconstant. From this, it follows that, if market participants used the Black-\\nScholes−Merton model with the same volatility when pricing all options \\non an asset, the implied volatilities calculated from the options would be \\nthe same and equal to the volatility used. In practice, the Black−Scholes \\n−Merton model does not provide a perfect description of how traders \\nprice options.  Nevertheless, implied volatilities continue to be calculated \\nand quoted by market participants. At any given time, each option trad-\\ning in the market has a price, and therefore an implied volatility, associ-\\nated with it.   \\nAn important activity for traders is keeping track of implied volatili-\\nties. The implied volatility of an option at any given time is a function of \\ntwo variables: \\n\\uf0b7 \\nits moneyness (which, as explained earlier, is measured as delta) \\n\\uf0b7 \\nits time to maturity, T.  \\nThe three-dimensional function relating implied volatility to delta and T \\nis referred to as the volatility surface. The volatility surface, estimated \\nfrom all options on the S&P 500 on two different dates, is shown in Figure \\n10.3.  \\n \\n10.4   Understanding Volatility Surface Movements \\n \\nAs illustrated in Figure 10.3, the volatility surface is highly non-linear. \\nThe change in the volatility surface from one day to the next is also highly \\nnon-linear. Understanding how the volatility surface moves is important \\nfor a number of reasons: \\n \\n                                                           \\n5 The implied volatility for a call options with a certain strike price and maturity is \\nthe same as that for a put option with the same strike price and time to maturity. \\n6 The key advantage of doing this is that, when the asset price changes, the implied \\nvolatility often stays the same. When quotes are in terms of implied volatility they \\nare therefore more stable.  \\n205 \\n                                         Applications in Finance \\n \\n \\n \\n\\uf0b7 \\nIt can help a trader hedge her exposure.7  \\n\\uf0b7 \\nIt can help a quant determine a stochastic volatility model re-\\nflecting how options are priced in the market. \\n\\uf0b7 \\nIt can help a trader adjust implied volatilities in a market where \\nasset prices are changing fast. \\n \\nA neural network is a natural tool for using empirical data to model vol-\\natility surface movements. \\nFigure 10.3   Volatility surface observed for options on the S&P 500 on \\n(a) January 31, 2019 and (b) June 25, 2019 \\n     \\n        \\n                           \\n                                (a)                                                            (b) \\nWhen the price of an asset declines, all implied volatilities calculated \\nfrom options on the asset tend to increase, and vice versa.  However, the \\nimplied volatilities do not all change by the same amount. This leads to \\nmany variations in the pattern of implied volatilities.   \\nInterestingly, two competing theories about the phenomenon that im-\\nplied volatilities tend to be negatively correlated with asset returns have \\nbeen proposed by researchers: \\n \\n                                                           \\n7 As the application in this section shows, when the asset price increases, volatility \\ntends to decrease, and vice versa. A trader might want to use a “volatility-adjusted \\ndelta” for hedging that takes into account how volatility is expected to change when \\nthe price of the underlying asset changes. \\n \\n206                                                                                                                                   Chapter 10 \\n \\n\\uf0b7 \\nOne theory, applicable to stocks and stock indices, is that when \\nthe asset price declines (increases), the impact of debt in the cap-\\nital structure becomes more (less) pronounced and volatility in-\\ncreases (decreases). \\n\\uf0b7 \\nAnother theory, known as the “volatility feedback effect hypoth-\\nesis” argues that when volatility increases (decreases), investors \\nin the asset require a higher (lower) return as compensation for \\nrisk. As a result the price of the asset declines.  \\n \\nIn the first theory, the causality is from asset price changes to volatility. \\nIn the second theory, it is the other way around. On balance, the empirical \\nevidence seems to favor the second theory, but for our purposes the rea-\\nson for the relationship does not matter. We are merely interested in us-\\ning data to quantify the relationship.  \\nOur data consists of call options on the S&P 500 between 2014 and \\n2019.  To keep the size of the data set manageable, we randomly sample \\n100 options per day, recording quotes for each option on both the day it \\nis sampled and the next day.8  \\nThe neural network has the structure shown in Figure 6.3. There are \\nthree features: \\n \\n\\uf0b7 The percentage change in the S&P 500 from one day to the next \\n\\uf0b7 The time to maturity \\n\\uf0b7 The delta of the option \\n \\nThe target is the change in the implied volatility. The objective is to \\nminimize the mean squared error between the predicted change in the \\nimplied volatility and the actual change. The results we will present here \\nuse three hidden layers and 20 nodes per layer. \\nTable 10.1 shows a sample of the data. There were 125,700 observa-\\ntions in total. These were split randomly into a training set (75,420 ob-\\nservations or 60% of total), validation set (25,140 observations or 20% \\nof total), and test set (25,140 observations or 20% of total).  As men-\\ntioned in Chapter 6, the performance of a neural network is improved \\nwhen the inputs are scaled.  We used Z-score scaling (with the mean and \\nstandard deviation taken from the training set). Table 10.2 shows the \\ndata from Table 10.1 after this scaling has been applied.  \\n                                                           \\n8 The analysis presented here is a simplified version of that carried out by J.Cao, J. \\nChen, and J. Hull, “A neural network approach to understanding implied volatility \\nmovements,” forthcoming, Quantitative Finance, available at ssrn 3288067. Their \\nanalysis used data between 2010 and 2017 and was based on 2.07 million observa-\\ntions on 53,653 call options. \\n207 \\n                                         Applications in Finance \\n \\n \\n \\nIn constructing a machine learning model, it is always useful to have \\na simpler model as a benchmark. In this case, we use the following model \\n \\nExpected change in implied volatility = 𝑅𝑎+ 𝑏𝛿+ 𝑐𝛿2\\n√𝑇\\n         (10.1)  \\n \\nwhere R is the return on the asset (= change in price divided by initial \\nprice), T is the option’s time to maturity, \\uf064 is the option’s moneyness \\n(measured as delta) and a, b, and c are constants. This model was sug-\\ngested by Hull and White (2017) and is quite popular with practitioners.9 \\nThe a, b, and c can be estimated by regressing implied volatility changes \\nagainst 𝑅√𝑇,\\n⁄\\n  𝑅𝛿√𝑇,\\n⁄\\n and 𝑅𝛿2 √𝑇.\\n⁄\\n  \\n \\nTable 10.1   Sample of the original data. The return on the S&P 500 and \\nthe change in the implied volatility are between the day specified and the \\nnext business day. (See Excel file) \\nDate \\nReturn (%) on \\nS&P 500 \\nMaturity \\n(years) \\nDelta \\nChange in implied \\nvolatility (bps) \\nJan 28, 2015 \\n0.95 \\n0.608 \\n0.198 \\n−31.1 \\nSept 8, 2017 \\n1.08 \\n0.080 \\n0.752 \\n−54.9 \\nJan 24, 2018 \\n0.06 \\n0.956 \\n0.580 \\n  −1.6 \\nJun 24, 2019 \\n     −0.95 \\n2.896 \\n0.828 \\n  40.1 \\n \\nTable 10.2   Data in Table 10.1 after using Z-score scaling for the inputs. \\n(See Excel file) \\nDate \\nReturn (%) on \\nS&P 500 \\nMaturity \\n(years) \\nDelta \\nChange in implied \\nvolatility (bps) \\nJan 28, 2015 \\n1.091 \\n−0.102 \\n−1.539 \\n−31.1 \\nSept 8, 2017 \\n1.247 \\n−0.695 \\n  0.445 \\n−54.9 \\nJan 24, 2018 \\n0.027 \\n  0.289 \\n−0.171 \\n  −1.6 \\nJun 24, 2019 \\n   −1.176 \\n  2.468 \\n   0.716 \\n  40.1 \\n \\nFigure 10.4 shows results obtained by the neural network for the training \\nset and the validation set. (The mean squared errors have been smoothed \\nin producing Figure 10.4 by averaging them over 50 successive epochs.) \\n                                                           \\n9 See J. Hull and A. White, “Optimal delta hedging for options,” Journal of Banking \\nand Finance, Sept 2017: 180-190. \\n208                                                                                                                                   Chapter 10 \\n \\nFollowing the approach explained in Chapter 6, we continue training un-\\ntil there is no improvement in the results from the validation set. In our \\nexample this happens after the 5,826th epoch.  The test set results indi-\\ncated a modest 14% improvement over the model in equation (10.1). \\n \\nFigure 10.4   Mean squared errors as the number of epochs is increased \\nfor the training set and the validation set \\n \\nOnce a first model has been implemented, it is natural to look for ad-\\nditional features that can improve results further. In this case, the VIX \\nindex, observed on day t to predict changes between day t and t+1, is \\nfound to produce a considerable improvement.10 (By trying Exercise \\n10.12, readers can verify this for themselves).  We can deduce from this \\nthat movements in the implied volatility surface tend to be different in \\nhigh and low volatility environments.  \\n \\n \\n10.5   Using Reinforcement Learning for Hedging \\n \\nIn Section 10.2, we explained how delta is used for hedging a portfolio \\nof derivatives dependent on an underlying asset. For the purposes of our \\nnext application, we suppose that we do not know how to calculate delta \\nand want to use reinforcement learning to calculate the optimal hedging \\nstrategy for a short position in an option.  \\n                                                           \\n10 See results in J.Cao, J. Chen, and J. Hull, “A neural network approach to under-stand-\\ning implied volatility movements,” forthcoming, Quantitative Finance, available at \\nssrn 3288067. \\n0.00006\\n0.000065\\n0.00007\\n0.000075\\n0.00008\\n0.000085\\n0.00009\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\n7000\\n8000\\nMean Squared \\nError\\nEpochs\\nTraining Set\\nValidation Set\\n209 \\n                                         Applications in Finance \\n \\n \\n \\nSuppose we want to hedge the sale of 10 call options where S = 100, \\nK = 100, \\uf06d\\uf020= 0, r = 0, q = 0, \\uf073\\uf020= 20%, and T = 10 days or 0.04 years.11  The \\nhedge is accomplished by owning 0, 1, 2,…, or 10 shares (i.e., there are 11 \\npossible positions). If the hedger knew that the option would be exer-\\ncised because the final price of the stock was certain to be greater than \\n100, it would be optimal for her to buy 10 shares. Similarly, if the hedger \\nknew that the option would be not be exercised because the final price of \\nthe stock was certain to be less than 100, it would be optimal for the \\nhedger to hold no shares. In practice of course, the outcome is uncertain \\nand, when each hedging decision is made, the hedger must choose a po-\\nsition between 0 and 10 shares.12  \\nWe assume that the behavior of the stock price is that underlying \\nBlack−Scholes−Merton model. As mentioned earlier, this means that the \\nrate of return in the stock price in a short period of time, \\uf044t, is normally \\ndistributed with mean \\uf06d\\uf044t and standard deviation 𝜎√∆𝑡.  The stock price \\ncalculated from the model is rounded to the nearest integer. \\nWe assume that the hedge position (i.e., number of shares owned) can \\nbe changed once a day so that a total of 10 decisions have to be made. The \\nstates for the reinforcement learning algorithm at the time hedging deci-\\nsion is made on a day are defined by  \\n \\n\\uf0b7 The number of shares currently held (i.e., the position taken on \\nthe previous day) \\n\\uf0b7 The stock price \\nAs mentioned there are 11 possible values for the number of shares cur-\\nrently held. A total of 15 possible price states are considered:  \\n≤93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, ≥107 \\nTo simplify matters we assume that at most five shares can be bought \\nand five shares can be sold.  For example, if the current holding is 7, pos-\\nsible actions (i.e., changes in the position) are −5, −4, −3, −2, −1, 0, +1, +2, \\nand +3. Similarly, if the current holding is 2, possible actions are −2, −1, \\n0, +1, +2, +3, +4, and +5.  \\nThe objective function is to minimize the variance of the hedging cost. \\nThe expected cost in each period (regardless of the decision taken) can \\nbe assumed to be zero. The changes in the value of the hedged position \\n                                                           \\n11 We assume 250 trading days in a year.   \\n12 In practice, an option contract is to buy or sell 100 shares and the shares them-\\nselves are bought or sold in multiples of 100. We assume that only 11 different posi-\\ntions can be taken to simplify the analysis.  \\n210                                                                                                                                   Chapter 10 \\n \\non successive days are assumed to be independent. Because the interest \\nrate is zero, this means that the objective is to minimize \\n \\n∑𝐻𝑖\\n2\\n𝑖\\n \\nwhere Hi is change in the value of the hedger’s position during the ith day \\nand the summation is taken over all periods from the current one on-\\nward.   If Ni  is the number of shares held at the beginning of the ith day, \\nSi is the stock price at the beginning of the ith day, and ci is the value of \\nthe call option at the beginning of the ith day (obtained from the Black-\\nScholes−Merton equation),  \\n𝐻𝑖= 𝑁𝑖(𝑆𝑖+1 −𝑆𝑖) −10(𝑐𝑖+1 −𝑐𝑖) \\nOur Python implementation is at  \\nANDY LANDU NGOMA \\nWe generated three million paths for the stock price for the training set. \\n(The fact that three million paths are necessary for such a small problem \\nemphasizes the point made in Chapter 7 that reinforcement learning is \\n“data hungry.”) We then used a further 100,000 paths for the test set.  In-\\nitially the probability of exploration was 100%. The probability of explo-\\nration decreases as the algorithm proceeds so that the probability of ex-\\nploration during a trial is 0.999999 times its value at the preceding trial. \\nThe minimum value for the probability of exploration was 5%. \\nWe find that the reinforcement learning model tracks delta hedging \\nreasonably well. The mean absolute difference between the position \\ntaken by the reinforcement learning algorithm and that taken by delta \\nhedging across all days was 0.32 for the test set.  The standard deviation \\nof the cost of writing the option and hedging it was about 9% higher \\nwhen the algorithm was used than when delta hedging was used.  \\n \\n \\n10.6   Extensions \\n \\nTwo criticisms might be levelled at the analysis just presented. First, \\nthe problem does not need to be solved with reinforcement learning be-\\ncause the equation for delta under Black−Scholes−Merton assumptions \\nis well known (see Section 10.2). Second, the problem is not really a \\nmulti-period one. The delta each day can be calculated independently of \\nthe delta on each other day.  \\nHowever, there are extensions of the analysis that make it relevant. In \\npractice, a trader faces bid–ask spreads, i.e., the ask price at which she \\n211 \\n                                         Applications in Finance \\n \\n \\n \\ncan buy an instrument is generally higher than the bid price at which she \\ncan sell the instrument. If bid−ask spreads are not negligible, the trader \\nwants to use a strategy where the cost of buying and selling is weighed \\nagainst the reduction in risk.  The objective function can then be \\n \\n∑(𝐶𝑖+ 𝛼𝐻𝑖\\n2)\\n𝑖\\n \\nwhere Ci is the transaction cost (arising from the bid−ask spread) in-\\ncurred in period i and Hi is as before the change in the value of the \\nhedger’s position on day i with the summation being taken over all hedg-\\ning periods from the current one onward. The parameter\\uf020\\uf061 defines the \\ntrade-off between expected costs and variance of costs.  \\nDelta hedging is not optimal when transaction costs are considered \\nand the problem is then a genuine multi-period problem that requires \\nreinforcement learning.13  The code which accompanies the analysis in \\nSection 10.5 can be extended to consider this problem (see Exercise \\n10.14). \\nEven when trading the underlying asset entails negligible bid−ask \\nspreads, a trader may want to use reinforcement learning for hedging \\nvolatility risk. The exposure to volatility is measured by what is known \\nas vega. Changing vega requires taking a position in another derivative \\ndependent on the same underlying asset. Trading derivatives almost in-\\nvariably involves non-negligible bid−ask spreads, making the use of re-\\ninforcement learning to derive a hedging strategy particularly relevant. \\nThe reinforcement learning algorithm can be used with a number of \\ndifferent stochastic processes (For vega hedging a process where the vol-\\natility is stochastic is obviously necessary). In practice, there is some un-\\ncertainty about the process that the asset will follow. One approach sug-\\ngested by Cao et al (2019) is to use a mixture of processes (with each one \\nbeing equally likely to be chosen for creating a path) and train the model \\non the mixture.14 The hedging scheme should then work reasonably well \\non all the processes.  \\n \\n                                                           \\n13 This problem is considered by  J. Cao, J. Chen, J. Hull and Z. Poulos, “Deep hedging \\nof derivatives using reinforcement learning,” Working paper, 2019, available at ssrn \\n3514586; H. Buehler, L. Gonon, J. Teichmann, B. Wood, “Deep hedging,” 2019, availa-\\nble at ssrn: 3120710; and P.N. Kolm and G. Ritter, “Dynamic replication and hedging: \\na reinforcement learning approach,” Journal of Financial Data Science, Winter 2019: \\n159-171.  \\n14 See by  J. Cao, J. Chen, J. Hull and Z. Poulos, “Deep hedging of derivatives using rein-\\nforcement learning,” Working paper, 2019, available at ssrn 3514586. \\n212                                                                                                                                   Chapter 10 \\n \\n10.7   Other Finance Applications \\n \\nThere are many other applications of machine learning in finance. For \\nexample: \\n\\uf0b7 Machine learning is used extensively in investing. As described \\nin Chapter 8, sentiment analysis can be used to determine the \\ninfluence of opinions in blogs, tweets, news articles, etc. on asset \\nreturns. As mentioned, Renaissance Technologies, a secretive \\nhedge fund run by mathematicians, has used machine learning \\nfor investing very profitably for many years. Many other hedge \\nfunds now use machine learning to make portfolio management \\ndecisions, sometimes without any human intervention.15   \\n\\uf0b7 Machine learning is sometimes used by private equity compa-\\nnies. Data on start ups, can be used to determine the features \\nthat predict success (e.g., the age of the company, sales growth, \\nthe characteristics of the management team, and so on).  \\n\\uf0b7 Natural language processing is sometimes used by human re-\\nsource professionals for dealing with the large number of re-\\nsumes that are received. \\n\\uf0b7 Handling loan applications has been a very successful applica-\\ntion of machine learning. We have illustrated how this can be \\ndone with data from Lending Club earlier in this book. Banks \\nthroughout the world are moving routine lending decisions \\nfrom human loan officers to algorithms such as those we have \\nillustrated. (This is in spite of the fact that humans can do some \\nthings algorithms cannot such as meet with the borrower and \\nassess the borrower’s character.)  New accounting standards \\nsuch as IFRS 9 require that the valuation of loans on the balance \\nsheet reflect expected losses. This provides an extra incentive \\nfor banks to come up with robust methods for estimating prob-\\nabilities of default and recovery rates. \\n\\uf0b7 Identifying fraudulent transactions, money laundering, and mis-\\nconduct by employees or agents is an important activity for fi-\\nnancial institutions. This is proving to be an area where ma-\\nchines can outperform humans. \\n\\uf0b7 As indicated in Chapter 7, reinforcement learning can be used \\nfor order execution. When a large buy or sell order is to be plac-\\n                                                           \\n15 For a discussion of this, see: M. Lopez de Prado, Advances in machine learning, John \\nWiley and Sons, 2018  and S. Gu, B. Kelly, and D. Xiu, “Empirical asset pricing and \\nmachine learning,” 2019, available at dachxiu.chicagobooth.edu/download/ML.pdf \\n \\n213 \\n                                         Applications in Finance \\n \\n \\n \\ned, a trader has to find a balance between (a) placing a large or-\\nder all at once and potentially moving the market and (b) \\nspreading the required trade over a period of time and risking \\nadverse market moves. \\n\\uf0b7 Many transactions require collateral and often there are alter-\\nnative assets that can be used for this purpose (e.g., Treasury \\nbonds, Treasury bills, and corporate bonds). Machine learning \\ncan assist in making optimal collateral decisions. \\n\\uf0b7 Some derivatives have to be valued using Monte Carlo simula-\\ntion or other quite slow numerical procedures. This creates \\nproblems when scenarios and other analyses are carried out to \\ninvestigate the risks in a portfolio. As explained in Chapter 6, one \\napproach is to create a neural network to replicate the value of \\nthe derivative obtained from the numerical procedure.  Valuing \\nthe derivative then involves working forward through the net-\\nwork, which is very fast. There is a lot of computational effort \\ninvolved in generating the training set, but this can be a good \\ninvestment.16  \\n \\nSummary \\n \\nThere are many applications of machine learning in finance. Many of \\nthese applications involve derivatives. In Chapter 6, we saw how a neural \\nnetwork can replicate a method for pricing derivatives. Once it has been \\ndeveloped, the neural network provides very fast pricing. This can be at-\\ntractive when traditional ways of pricing the derivative are computation-\\nally very time consuming.  \\nIn this chapter, we have looked at two applications of machine learn-\\ning in some detail. One involves trying to understand volatility move-\\nments. The relationship between movements in an option’s implied vol-\\natility and the price of the underlying asset is highly non-linear. Neural \\nnetworks can be used in conjunction with historical data to quantify this \\nrelationship. \\nThe other application involves hedging. When there are trading costs, \\nhedging involves complex multi-period decision making and is ideally \\nsuited to reinforcement learning. In this case, the only viable approach is \\nto use a model to generate training data because historical data is not \\n                                                           \\n16 For a fuller description, see R. Ferguson and A. Green, “Deeply learning deriva-\\ntives,” October 2018, available at ssrn: 3244821. \\n214                                                                                                                                   Chapter 10 \\n \\navailable in the volume required. The application in this chapter illus-\\ntrates how several million paths for the underlying asset price can be \\nused to determine a viable hedging strategy. \\nThere are many other applications of machine learning in finance in-\\nvolving the algorithms covered in this book. We have used lending deci-\\nsions to illustrate classification in Chapter 3, 4, and 5. Other applications \\ninclude those in areas such as fraud detection, order execution, and in-\\nvestment.  \\n \\n \\nSHORT CONCEPT QUESTIONS \\n \\n10.1 What is the difference between a call option and a put option? \\n10.2  Why does the price of a derivative depend on volatility? \\n10.3 What is meant by the moneyness of an option? How is it measured? \\n10.4 What are the six variables that the price of an option depends on \\nin the Black-Scholes model? \\n10.5 Explain what delta-neutrality means.  \\n10.6 What is an implied volatility? \\n10.7 What is a volatility surface? \\n10.8 How does the volatility surface usually move when the price of an \\nasset changes? Do these movements increase or reduce a deriva-\\ntive trader’s exposure to movements in the price of the underlying \\nasset when a call option has been sold? \\n10.9 Under what circumstances does reinforcement learning lead to an \\nimprovement over delta hedging? \\n10.10 Why is reinforcement learning likely to be particularly useful for \\nhedging against movements in volatility? \\n \\nEXERCISES \\n \\n10.11 In the application in Section 10.4, use the Python code available at \\nANDY LANDU NGOMA \\nto test the effect of: \\n(i) Changing the number of hidden layers from three to (a) one \\nand (b) five.  \\n(ii) Changing the number of nodes per layer from 20 to (a) 10 \\nand (b) 40. \\n(iii) Using a different activation function such as relu.  \\n(iv) Using min-max scaling (for this test, a useful resource is  \\nSklearn’s MinMaxScaler). \\n215 \\n                                         Applications in Finance \\n \\n \\n \\n10.12 Use the Python code at \\nANDY LANDU NGOMA \\nto repeat the analysis in Section 10.4 adding the value of the VIX \\nindex (observed on the first of the two days that the implied vola-\\ntility and S&P 500 are observed) as a feature. Values for the VIX \\nindex can be downloaded from the Yahoo Finance website.  \\n10.13 Use the Python code at \\nANDY LANDU NGOMA \\nto carry out the analysis in Section 10.5 for a 20-day option and \\ncompare your results with those for a 10-day option. \\n10.14 Use the Python code at \\nANDY LANDU NGOMA \\nto extend the application in Section 10.5 along the lines suggested \\nin Section 10.6. Assume that the cost of trading is 1% of the value \\nof what is traded and that the \\uf061 parameter defining the trade off \\nbetween the mean cost and variance of cost is 0.15. Compare your \\nresults with those from delta hedging. \\n  \\n \\n \\n \\n \\n    \\n\\xa0\\n217 \\n \\n \\n \\n \\n \\nChapter 11 \\n \\nIssues for Society \\n \\n \\n \\n \\n \\nComputers have been used to automate tasks such as record keeping \\nand sending out invoices for many years, and for the most part society \\nhas benefited from this.   But it is important to emphasize that the inno-\\nvations we have talked about in this book involve more than just the \\nautomation of tasks.   They allow machines to learn. Their aim is to al-\\nlow machines to make decisions and interact with the environment sim-\\nilarly to the way human beings do.  Indeed, in many cases the aim is to \\ntrain machines so that they improve on the way human beings carry out \\ncertain tasks.  \\nWe have mentioned the success of Google’s AlphaGo in beating the \\nworld champion Go player, Ke Jie. Go is a very complex game. It has too \\nmany moves for the computer to calculate all the possibilities. AlphaGo \\nused a deep learning strategy to approximate the way the best human \\nplayers think about their moves and then improve on it. The key point \\nhere is that AlphaGo’s programmers did not teach AlphaGo how to play \\nGo. They taught it to learn how to play Go. \\nTeaching machines to use data to learn and behave intelligently rais-\\nes a number of difficult issues for society. Who owns the data used by \\nmachine learning algorithms? What biases are present in machine \\nlearning algorithms? Can machines be taught to distinguish right from \\nwrong?  Should the algorithms underlying machine learning be more \\ntransparent?  What are the implications of humans no longer being the \\n218                                                                                                                                   Chapter 11 \\n \\n \\n \\nmost intelligent entities on earth?  This chapter considers these ques-\\ntions.  \\n \\n11.1   Data Privacy \\n \\nIssues associated with data privacy have received a great deal of \\npublicity, partly as a result of the activities of Cambridge Analytica. This \\ncompany worked for both Donald Trump’s 2016 presidential campaign \\nand for an organization campaigning for the U.K. to leave the European \\nUnion. It managed to acquire and use personal data on millions of Face-\\nbook users without obtaining permission from them. The data was de-\\ntailed enough for Cambridge Analytica to create profiles and determine \\nwhat kind of advertisements or other actions would be most effective in \\npromoting the interests of the organizations that had hired it.  \\nMany governments are concerned about issues concerned with data \\nprivacy. The European Union has been particularly proactive and \\npassed the General Data Protection Regulation (GDPR) which came into \\nforce in May 2018.1 It recognizes that data is valuable and includes in its \\nrequirements the following: \\n  \\n\\uf0b7 A person must provide consent to a company before the company \\ncan use the person’s data for other than the purpose for which it \\nwas collected. \\n\\uf0b7 If there is a data breach, notifications to everyone affected are \\nmandatory within 72 hours. \\n\\uf0b7 Citizens have a “right to explanation” about the application of al-\\ngorithms to their data. \\n\\uf0b7 Data must be safely handled across borders. \\n\\uf0b7 Companies must appoint a data protection officer.  \\n \\nFines for non-compliance with GDPR can be as high as 20 million eu-\\nros or 4% of a company’s global revenue. It is likely that other govern-\\nments will pass similar legislation to GDPR in the future.  Interestingly, \\nit is not just governments that are voicing concerns about the need to \\nregulate the way data is used by companies. Mark Zuckerberg, Face-\\nbook’s CEO, considers that rules are needed to govern the internet and \\nhas expressed support for GDPR.2 \\n                                                             \\n1 See ANDY LANDU NGOMA \\n2Zuckerberg’s views were outlined in a Washington Post article on March 30, 2019:  \\nIssues for Society                                                                                                                      219 \\n \\n \\n \\n11.2   Biases \\n \\nHuman beings exhibit biases. Some are risk averse; others are risk \\ntakers. Some are naturally caring; others are insensitive. It might be \\nthought that one advantage of machines is that they take logical deci-\\nsions and are not subject to biases at all.  Unfortunately, this is not the \\ncase. Machine learning algorithms exhibit many biases.  \\nOne bias is concerned with the data that has been collected. It might \\nnot be representative. A classic example here (from a time well before \\nthe advent of machine learning) is an attempt by Literary Digest to pre-\\ndict the result of the United States presidential election in 1936. The \\nmagazine polled ten million people (a huge sample) and received 2.4 \\nmillion responses. It predicted that Landon (a republican) would beat \\nRoosevelt (a democrat) by 57.1% to 42.9%. In fact, Roosevelt won. \\nWhat went wrong? The answer is that Literary Digest used a biased \\nsample consisting of Literary Digest readers, telephone users, and those \\nwith car registrations.  These were predominantly republican support-\\ners.3 More recently, we can point to examples where facial recognition \\nsoftware was trained largely on images of white people and therefore \\ndid not recognize other races well, resulting in misidentifications by \\npolice forces using the software.4  \\nThere is a natural tendency of machine learning to use readily avail-\\nable data and to be biased in favor of existing practices. We encountered \\nthis in the data used in Chapters 3, 4, and 5 for classifying loans. The \\ndata available for making lending decisions in the future is likely to be \\nthe data on loans that were actually made in the past. It would be nice \\nto know how the loans that were not made in the past would have \\nworked out, but this data by its nature is not available.  Amazon experi-\\nenced a similar bias when developing recruiting software. 5  Its existing \\nrecruits were predominantly male and this led to the software being\\n                                                                                                                                        \\nANDY LANDU NGOMA\\nnew-rules-lets-start-in-these-four-areas/2019/03/29/9e6f0504-521a-11e9-a3f7-\\n78b7525a8d5f_story.html?noredirect=on&utm_term=.2365e1f19e4e \\n3 See P. Squire, “Why the 1936 Literary Digest Poll Failed,” The Public Opinion Quar-\\nterly, 52, 1 (Spring 1988): 125−133. \\n4 See R. McCullom, 2017, “Facial Recognition Software is Both Biased and Under-\\nstudied,” at \\n ANDY LANDU NGOMA \\n5 For an account of this, see: ANDY LANDU NGOMA\\njobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-\\nbias-against-women-idUSKCN1MK08G. \\n220                                                                                                                                   Chapter 11 \\n \\n \\n \\n biased against women. As a result, its use was discontinued. \\nChoosing the features that will be considered in a machine learning \\nexercise is a key task. In most cases, it is clearly unacceptable to use fea-\\ntures such as race, gender, or religious affiliation.  But data scientists \\nalso have to be careful not to include other features that are highly cor-\\nrelated with these sensitive features. For example, if a particular neigh-\\nborhood has a high proportion of black residents, using “neighborhood \\nof residence” as a feature when developing an algorithm for loan deci-\\nsions may lead to racial biases. \\nThere are many other ways in which an analyst can (consciously or \\nunconsciously) exhibit biases when developing a machine learning al-\\ngorithm.  For example, the way in which data is cleaned, the choice of \\nmodels, and the way the results from an algorithm are interpreted and \\nused can be subject to biases. \\n \\n11.3   Ethics  \\n \\nMachine learning raises many ethical considerations.  Some people \\nfeel that China has gone too far with its Social Credit System, which is \\nintended to standardize the way citizens are assessed.  \\nShould machine learning be used in warfare? It is perhaps inevitable \\nthat it will be. Google canceled Project Maven, which was a collabora-\\ntion with the U.S. Department of Defense to improve drone strike tar-\\ngeting, after thousands of Google employees signed an open letter con-\\ndemning the project. However, the U.S. and other nations continue to \\nresearch how AI can be used for military purposes.  \\nCan machine learning algorithms be programmed to behave in a \\nmorally responsible and ethical way? One idea here is to create a new \\nmachine learning algorithm and provide it with a large amount of data \\nlabeled as “ethical” or “non-ethical” so that it learns to identify non-\\nethical data. When new data arrives for a particular project, the algo-\\nrithm is used to decide whether or not it is ethically appropriate to use \\nthe data.  The thinking here is that if a human being can learn ethical \\nbehavior so can a machine. (Indeed, some have argued that machines \\ncan learn to be more ethical than humans.)   \\nAn interesting ethical dilemma arises in connection with driverless \\ncars.  If an accident is unavoidable, what decision should be taken?  How \\nshould an algorithm choose between killing a senior citizen and young-\\ner person? How should it choose between killing a jaywalker and some-\\none who is obeying the rules for crossing roads? How should it choose \\nbetween hitting a cyclist wearing a helmet and one who is not?  Dilem-\\nIssues for Society                                                                                                                      221 \\n \\n \\n \\nmas such as these, which involve choosing who lives and who dies, are \\nsometimes referred to as the “trolley problem.”6   \\nThe interaction of human beings with machine learning technologies \\ncan sometimes lead to unexpected results with inappropriate and un-\\nethical behavior being learned. In March 2016, Microsoft released Tay \\n(short for “thinking about you”), which was designed to learn by inter-\\nacting with human beings on Twitter so that it would mimic the lan-\\nguage patterns of a 19-year-old American girl.  Some Twitter users be-\\ngan tweeting politically incorrect phrases. Tay learned from these and \\nas a result sent racist and sexually charged messages to other Twitter \\nusers.  Microsoft shut down the service just 16 hours after it was re-\\nleased.  \\n \\n11.4   Transparency \\n \\nIn recent years a lot of progress has been made in making machine \\nlearning algorithms more transparent. We discussed this in Chapter 9. \\nWhen making predictions, it is important to develop ways of making the \\nresults of machine learning algorithms accessible to those who are af-\\nfected by the results.  It is also important for companies to understand \\nthe algorithms they use so that they can be confident that decisions are \\nbeing made in a sensible way. There is always a risk that algorithms \\nappear to be making intelligent decisions when they are actually taking \\nadvantage of obscure correlations.  We gave two examples of this in \\nChapter 9.  The German horse Hans appeared intelligent because there \\nwas a correlation between the correct answer and the expressions on \\nthe questioner’s face as the horse stomped its foot.  Software might dis-\\ntinguish polar bears and dogs because of the background (ice or \\ngrass/trees), not to the images of the animals themselves.  \\n \\n11.5   Adversarial Machine Learning \\n \\nAdversarial machine learning refers to the possibility of a machine \\nlearning algorithm being attacked with data designed to fool it. Argua-\\nbly it is easier to fool a machine than a human being! A simple example \\nof this is an individual who understands how a spam filter works and \\ndesigns an email to get past it.  Spoofing in algorithmic trading is a form \\nof adversarial machine learning.  A spoofer attempts to (illegally) ma-\\n                                                             \\n6 The original trolley problem was a thought experiment in ethics concerning a run-\\naway trolley which will either kill five people or, if a lever is pulled, kill one person.   \\n222                                                                                                                                   Chapter 11 \\n \\n \\n \\nnipulate the market by feeding it with buy or sell orders and canceling \\nbefore execution.  Another example of adversarial machine learning \\ncould be a malevolent individual who targets driverless cars, placing a \\nsign beside a road that will confuse the car’s algorithm and lead to acci-\\ndents. \\nOne approach to limiting adversarial machine learning is to generate \\nexamples of it and then train the machine not to be fooled by them. \\nHowever, it seems likely that humans will have to monitor machine \\nlearning algorithms for some time to come to ensure that the algorithms \\nare not being manipulated. The dangers of adversarial machine learning \\nreinforce the points we have already made that machine learning algo-\\nrithms should not be black boxes without any interpretation. Transpar-\\nency and interpretability of the output is important.  \\n \\n11.6   Legal Issues \\n \\nWe can expect machine learning algorithms to give rise to many is-\\nsues that have not previously been considered by the legal system.  We \\nhave already mentioned issues concerned with the ownership and use \\nof data. As it becomes more evident that data is a valuable commodity, it \\nis likely that class action lawsuits concerned with the misuse of data \\nwill become common.  We have explained how machine learning algo-\\nrithms can exhibit biases. We are likely to see more class action lawsuits \\nfrom groups that, rightly or wrongly, feel that an algorithm is biased \\nagainst them. \\n Driverless cars are likely to become an important mode of transpor-\\ntation in the near future. If a driverless car hits a pedestrian, who is the \\nguilty party? It could be any of \\n \\n\\uf0b7 \\nthe person who programmed the car’s algorithm \\n\\uf0b7 \\nthe manufacturer of the car  \\n\\uf0b7 \\nthe owner of the car \\n \\nContract law may have to be modified because in the future many \\ncontracts are likely to be from one machine to another (with both ma-\\nchines using learning algorithms). What if there is a dispute? Could \\nthere be a challenge in a court of law concerning whether a machine has \\nthe authority to execute a contract?   \\nIt is not inconceivable that in the future machines will be given \\nrights (just as companies have rights today).  Consider the situation \\nwhere a great deal of experience and intelligence (far exceeding that of \\nIssues for Society                                                                                                                      223 \\n \\n \\n \\nany human being) is stored in a certain machine. Should a human being \\nbe allowed to shut down the machine so that the experience and intelli-\\ngence is lost? \\n \\n11.7   Man vs. Machine \\n \\nHuman progress has been marked by a number of industrial revolu-\\ntions: \\n \\n1. \\nSteam and water power (1760−1840) \\n2. \\nElectricity and mass production (1840−1920) \\n3. \\nComputers and digital technology (1950−2000) \\n4. \\nArtificial intelligence (2000−present) \\n \\nThere can be no doubt that the first three industrial revolutions have \\nbrought huge benefits to society. The benefits were not always realized \\nimmediately but they have eventually produced big improvements in \\nour quality of life. At various times there were concerns that jobs tradi-\\ntionally carried out by humans would be moved to machines and that \\nunemployment would result.  There were upheavals in society, but they \\ndid not lead to permanent unemployment. Some jobs were lost during \\nthe first three industrial revolutions, but others were created. For ex-\\nample, the first industrial revolution led to people leaving rural life-\\nstyles to work in factories. The second industrial revolution changed the \\nnature of the work done in factories with the introduction of assembly \\nlines. The third industrial revolution led to more jobs involving the use \\nof computers. The impact of the fourth industrial revolution is not yet \\nclear.  \\nIt is worth noting that the third industrial revolution did not require \\nall employees to become computer programmers. It did require people \\nin many jobs to learn how to use computers and work with software. \\nWe can expect the fourth industrial revolution to be similar to the third \\nin that many individuals will have to learn new skills related to the use \\nof artificial intelligence. \\nWe are now reaching the stage where machine learning algorithms \\ncan make many routine decisions as well as, if not better than, human \\nbeings.  But the key word here is “routine.” The nature of the decision \\nand the environment must be similar to that in the past. If the decision \\nis non-standard or the environment has changed so that past data is not \\nrelevant, we cannot expect a machine learning algorithm to make good \\ndecisions. Driverless cars provide an example here. If we changed the \\n224                                                                                                                                   Chapter 11 \\n \\n \\n \\nrules of the road (perhaps on how cars can make right or left turns), it \\nwould be very dangerous to rely on a driverless car that had been \\ntrained using the old rules. \\nA key task for human beings is likely to be managing large data sets \\nand monitoring machine learning algorithms to ensure that decisions \\nare not made on the basis of inappropriate data. Just as the third indus-\\ntrial revolution did not require everyone to become a computer pro-\\ngrammer, the fourth industrial revolution will not require everyone to \\nbecome a data scientist. However, for many jobs it will be important to \\nunderstand the language of data science and what data scientists do. \\nToday, many jobs involve using programs developed by others for car-\\nrying out various tasks. In the future, they may involve monitoring the \\noperation of machine learning algorithms that have been developed by \\nothers.  \\nA human plus a trained machine is likely to be more effective than a \\nhuman or a machine on its own for some time to come.  However, we \\nshould not underestimate future advances in machine learning.  Even-\\ntually machines will be smarter than human beings in almost every re-\\nspect.  A continuing challenge for the human race is likely to be how to \\npartner with machines in a way that benefits rather than destroys socie-\\nty.  \\n  \\n  \\n  \\n \\n225 \\n \\n \\n \\n \\n \\n \\nAnswers to End of Chapter  \\nQuestions \\n \\n \\n \\nChapter 1 \\n \\n1.1 \\nMachine learning is a branch of artificial intelligence where intel-\\nligence is created by learning from large data sets. \\n1.2 \\nOne type of prediction is concerned with estimating the value of a \\ncontinuous variable. The other is concerned with classification. \\n1.3 \\nUnsupervised learning is concerned with identifying patterns \\n(clusters) in data.  \\n1.4 \\nReinforcement learning is concerned with situations where a se-\\nquence of decisions has to be made in a changing environment. \\n1.5 \\nSemi-supervised learning is concerned with making predictions \\nwhere some of the available data have values for the target and \\nsome do not. \\n1.6 \\nA validation set is used. If the answers given by the validation set \\nstart to get worse as model complexity is increased there is over-\\nfitting. \\n1.7 \\nThe validation set is used to compare models so that one that has \\ngood accuracy and generalizes well can be chosen. The test set is \\nheld back to provide a final test of the accuracy of the chosen \\nmodel. \\n1.8 \\nA categorical feature is a non-numerical feature where data is as-\\nsigned to one of a number of categories.  \\n1.9 \\nThe bias-variance trade-off is the trade-off between (a) under-fit-\\nting and missing key aspects of the relationship, and (b) over-fit-\\nting so that idiosyncrasies in the training data are picked up.  The \\n226                                                                                                                                       Answers \\n \\n \\nlinear model is under-fitting and therefore gives a bias error. The \\nfifth order-polynomial model is over-fitting and therefore gives a \\nvariance error. \\n1.10 Data cleaning can involve (a) correcting for inconsistent recording, \\n(b) removing observations that are not relevant, (c) removing du-\\nplicate observations, (d) dealing with outliers, and (e) dealing with \\nmissing data.   \\n1.11 Bayes’ theorem deals with the situation where we know the prob-\\nability of X conditional on Y and we want the probability of Y con-\\nditional on X.  \\n1.12 For a polynomial of degree three, the standard deviation of the er-\\nror for the training set and the validation set are 31,989 and \\n35,588, respectively. For a polynomial of degree 4, the standard \\ndeviation of the error for the training set and the validation set are \\n21,824 and 37,427, respectively. As the degree of the polynomial \\nincreases, we obtain more accuracy for the training set but there \\nis a bigger difference between the performance of the model for \\nthe training set and the validation set. \\n1.13 Using Bayes’ theorem \\n \\n𝑃(Spam|Word) = 𝑃(Word|Spam)𝑃(Spam)\\n𝑃(Word)\\n    \\n \\n             = 0.4 × 0.25\\n0.125\\n= 0.8 \\n \\nThere is an 80% chance that an email containing the word is \\nspam. \\n \\n \\nChapter 2 \\n \\n2.1 \\nFeature scaling is necessary in unsupervised learning to ensure \\nthat features are treated as being equally important. In the Z-score \\nmethod, each feature is scaled so that it has a mean of zero and a \\nstandard deviation of one. In min-max scaling each feature is \\nscaled so that the lowest value is zero and the highest is one. Min-\\nmax scaling does not work well when there are outliers because \\nthe rest of the scaled values are then close together. But it may \\nwork better than the Z-score method when features have been \\nmeasured on different scales with lower and upper bounds.   \\n2.2 \\nThe distance is √(6 −2)2 + (8 −3)2 + (7 −4)2 = 7.07. \\nEnd of Chapter Questions                                                                                                     227 \\n \\n \\n2.3 \\nThe center is obtained by averaging feature values. It is the point \\nthat has values 4, 5.5, and 5.5 for the three features. \\n2.4 \\nWe choose k points as cluster centers, assign observations to the \\nnearest cluster center, re-compute cluster centers, re-assign ob-\\nservations to cluster centers, and so on.  \\n2.5 \\nIn the elbow method we look for the point at which the marginal \\nimprovement in inertia (i.e., within cluster sum of squares) when \\nan additional cluster is introduced is small. In the silhouette \\nmethod, we calculate for each value of k and for each observation \\ni  \\na(i): the average distance from other observations in its own \\ncluster, and \\nb(i): the average distance from observations in the nearest clus-\\nter.  \\nThe observation’s silhouette is \\n  \\n𝑠(𝑖) =\\n𝑏(𝑖) −𝑎(𝑖)\\nmax{𝑎(𝑖), 𝑏(𝑖)} \\n \\nand the best value of k is the one for which the average silhouette \\nacross all observations is greatest. \\n2.6 \\nAs the number of features increases, the sum of the squared differ-\\nences between feature values has more terms and therefore tends \\nto increase. When the ten additional features are created by mis-\\ntake, the distance between two observations increases by √2 be-\\ncause every squared difference is calculated twice. \\n2.7 \\nIn hierarchical clustering we start by putting every observation in \\nits own cluster. At each step, we find the two closest clusters and \\njoin them to create a new cluster. The disadvantage is that it is \\nslow. The advantage is that it identifies clusters within clusters.  \\n2.8 \\nDistribution-based clustering involves assuming that observations \\nare created by a mixture of distributions and using statistical \\nmethods to separate them. Density-based clustering involves add-\\ning new points to a cluster that are close to several points already \\nin the cluster. It can lead to non-standard shapes for the clusters.  \\n2.9 \\nPrincipal components analysis (PCA) is useful when there are a \\nnumber of highly correlated features. It has the potential to explain \\nmost of the variability in the data with a small number of new fac-\\ntors (which can be considered as manufactured features) that are \\nuncorrelated with each other. \\n2.10   A factor loading is the amount of each original feature in the factor.   \\nEach observation can be expressed as a linear combination of the \\n228                                                                                                                                       Answers \\n \\n \\nfactors.  A factor score for an observation is the amount of the fac-\\ntor in the observation.    \\n2.11 We can verify the numbers in Table 2.5 as follows: \\n \\n \\nPeace \\nindex \\nLegal risk in-\\ndex \\nGDP \\nAverage for 14 high- \\nrisk countries \\n  2.63 \\n 4.05 \\n−3.44 \\nAverage for all coun-\\ntries \\n  2.00 \\n 5.60 \\n  2.37 \\nSD for all countries \\n  0.45 \\n 1.49 \\n   3.24 \\nNormalized average \\nfor 14 countries \\n  1.39 \\n      −1.04 \\n−1.79 \\n \\nFor example, for the peace index, (2.63−2.00)/0.45 = 1.39. \\n2.12 Define X1, X2, X3, and X4 as the corruption index, peace index, legal \\nindex, and GDP growth rate.  If these are normalized Table 2.11 \\nshows that the factors are: \\n \\n0.594X1−0.530X2+0.585X3+0.152X4 \\n \\n \\nand \\n               \\n0.154X1+0.041X2+0.136X3−0.978X4 \\n \\nThe first factor assigns roughly equal weight to the three indices \\nand little weight to GDP growth rate. The second factor assigns \\nnearly all the weight to GDP growth rate. This shows that GDP \\ngrowth rate is providing a different type of information from the \\nother three measures.  \\nIf we want to create factors using non-normalized data, we can di-\\nvide each coefficient by the standard deviation of the correspond-\\ning feature value.  The first factor becomes: \\n \\n0.031X1−1.185X2+0.394X3+0.047X4 \\n \\nand the second one becomes: \\n \\n0.008X1+0.092X2+0.091X3−0.302X4 \\n \\n \\n \\nEnd of Chapter Questions                                                                                                     229 \\n \\n \\nChapter 3 \\n \\n3.1 \\nThe objective in “plain vanilla” linear regression is to minimize the \\nmean squared error of the forecasted target values. \\n3.2 \\nIn the case of Ridge regression we add a constant times the sum of \\nthe squares of the coefficients to the mean squared error. In the \\ncase of Lasso regression we add a constant times the sum of the \\nabsolute values of the coefficients. In the case of Elastic Net regres-\\nsion we add a constant times the sum of the squares of the coeffi-\\ncients and a different constant times the sum of the absolute val-\\nues of the coefficients. \\n3.3 \\nRidge regression reduces the magnitude of the coefficients when \\nthe correlation between features is high. Lasso regression sets to \\nzero the values of the coefficients of variables that have little effect \\non prediction results. \\n3.4 \\nA single dummy variable which equals one if the house has air con-\\nditioning and zero otherwise could be used. \\n3.5  \\nWe could use a single dummy variable which equals 0 for no slope, \\n1 for gentle slope, 2 for moderate slope, and 3 for steep slope. \\n3.6 \\nWe would create a dummy variable for each neighborhood. The \\ndummy variable equals one if the house is in the neighborhood and \\nzero otherwise. \\n3.7 \\nRegularization is designed to avoid over-fitting by reducing the \\nweights (i.e. coefficients) in a regression. L1 regularization is Lasso \\nwhere a constant times the sum of the absolute values of the coef-\\nficients is added to the objective function.  It makes some of the \\nweights zero. L2 regularization is Ridge where a constant times the \\nsum of the squares of the coefficients is added to the objective \\nfunction. It reduces the absolute magnitude of the weights. \\n3.8 \\nThe sigmoid function is: \\n𝑓(𝑦) =\\n1\\n1 + 𝑒−𝑦 \\n \\n3.9  \\nThe objective in logistic regression is to maximize \\n \\n∑\\nln(𝑄) +\\n∑\\nln (1 −𝑄)\\nNegative \\nOutcomes\\nPositive \\nOutcomes\\n \\n \\nwhere Q is the estimated probability of a positive outcome. \\n3.10 The true positive rate is the proportion of positive outcomes that \\nare predicted correctly. The false positive rate is the proportion of \\n230                                                                                                                                       Answers \\n \\n \\nnegative outcomes that are predicted incorrectly. The precision is \\nthe proportion of positive predictions that are correct.  \\n3.11 In the ROC curve the true positive rate is plotted against the false \\npositive rate. It shows the trade-off between correctly predicting \\npositive outcomes and failing to predict negative outcomes. \\n3.12 The dummy variable trap is the problem that when a categorical \\nvariable is hot encoded and there is a bias (constant term), there \\nare many sets of parameters that give equally good best fits to the \\ntraining set. The problem is solved with regularization.  \\n3.13  The plain vanilla linear regression result for salary (‘000s), Y, is: \\n \\n    \\n 𝑌= 178.6 −20,198.5𝑋1 + 89,222.3𝑋2 \\n−151, 267.2𝑋3 +  116,798.2𝑋4 −34,494.8𝑋5 \\n \\n \\n \\nWith an mse of 604.9.  For Ridge we have: \\n \\n\\uf06c\\uf020\\nA \\nb1 \\nb2 \\nb3 \\nb4 \\nb5 \\nmse \\n0.02 \\n178.6 \\n102.5 \\n56.2 \\n10.0 \\n−33.4 \\n−72.9 \\n889.5 \\n0.05 \\n178.6 \\n78.2 \\n43.4 \\n9.7 \\n−21.1 \\n−48.2 \\n1,193.9 \\n0.10 \\n178.6 \\n57.3 \\n33.3 \\n10.2 \\n−10.7 \\n−28.9 \\n1,574.0 \\n \\n           For Lasso we have \\n\\uf06c\\uf020\\nA \\nb1 \\nb2 \\nb3 \\nb4 \\nb5 \\nmse \\n0.02 \\n178.6 \\n0.0 \\n175.6 \\n0.0 \\n264.0 \\n−380.3 \\n711.8 \\n0.05 \\n178.6 \\n0.0 \\n250.8 \\n0.0 \\n0.0 \\n−190.2 \\n724.4 \\n0.10 \\n178.6 \\n0.0 \\n249.7 \\n0.0 \\n0.0 \\n−189.1 \\n724.6 \\n \\n3.14 (a) The objective function in equation (3.7) has the nice property \\nthat \\n1 −𝑄= 1 −\\n1\\n1 + exp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n) \\n \\n=\\nexp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n)\\n1 + exp(−𝑎−∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n) =\\n1\\n1 + exp(𝑎+ ∑\\n𝑏𝑗𝑋𝑗\\n𝑚\\n𝑗=1\\n) \\n \\n \\nWhen default is made the positive outcome, the maximum likeli-\\nhood objective function leads to the sign of the bias and the sign of \\neach of the weights changing. However, estimates of the probabil-\\nity of default and no-default are unchanged. \\n \\n \\n(b)  When Z=0.25, the confusion matrix is \\nEnd of Chapter Questions                                                                                                     231 \\n \\n \\n \\n \\nPredict positive \\n(default) \\nPredict negative \\n(no default) \\nOutcome positive \\n(default) \\n1.62% \\n16.26% \\nOutcome nega-\\ntive (no default) \\n4.53% \\n77.59% \\n \\nWhen Z=0.20, the confusion matrix is \\n \\n \\n \\nPredict positive \\n(default) \\nPredict negative \\n(no default) \\nOutcome positive \\n(default) \\n8.13% \\n9.75% \\nOutcome negative \\n(no default) \\n26.77% \\n55.34% \\n \\n            When Z=0.15, the confusion matrix is \\n \\nPredict positive \\n(default) \\nPredict  negative \\n(no default) \\nOutcome positive \\n(default) \\n14.15% \\n3.74% \\nOutcome negative \\n(no default) \\n53.47% \\n28.65% \\n \\nThe ratios become: \\n \\n \\nZ = 0.25 \\nZ = 0.20 \\nZ = 0.15 \\nAccuracy \\n79.21% \\n63.47% \\n42.80% \\nTrue Positive Rate \\n   9.07% \\n45.46% \\n79.11% \\nTrue Negative Rate \\n   94.48% \\n67.39% \\n34.89% \\nFalse Positive Rate \\n   5.52% \\n32.61% \\n65.11% \\nPrecision \\n   26.37% \\n23.29% \\n20.93% \\nF-score \\n   13.50% \\n30.80% \\n33.10% \\n \\n(c) The ROC curve can be calculated by changing the data so that a \\ndefault is labeled as 1 and no-default is labeled as 2. The ROC curve \\nis  \\n \\n232                                                                                                                                       Answers \\n \\n \\n \\n \\nDefine TPR(new), FPR(new), and Z(new)  as the true positive rate, \\nfalse positive rate, and Z-value when default is the positive out-\\ncome and TPR(old), FPR(old), Z(old) as the true positive rate, false \\npositive rate, and Z-value when no-default is the positive outcome. \\nWhen the Z(new) equals one minus the Z(old)), we have \\n \\nTPR(new) =1− FPR(old) \\n \\nFPR(new) =1 −TPR(old) \\n \\n            This symmetry leads to AUC being unchanged.  \\n \\n \\nChapter 4 \\n \\n4.1 \\nIn the decision tree approach the features are considered one-by-\\none in order of importance whereas in the regression approach \\nthey are considered all at once. The decision tree approach does \\nnot assume linearity and is more intuitive. It is also less sensitive \\nto outlying observations than linear regression. \\n4.2 \\nWhen there are n alternative outcomes entropy is  \\n \\n−∑𝑝𝑖\\n𝑛\\n𝑖=1\\nln (𝑝𝑖) \\n \\nwhere 𝑝𝑖 is the probability of the ith outcome. \\n4.3  \\nWhen there are n alternative outcomes the Gini measure is defined \\nas \\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nTrue Positive Rate\\nFalse Positive Rate\\nEnd of Chapter Questions                                                                                                     233 \\n \\n \\n   \\n1 −∑𝑝𝑖\\n2\\n𝑛\\n𝑖=1\\n \\n \\nwhere 𝑝𝑖 is the probability of the ith outcome. \\n4.4 \\nInformation gain is measured as the reduction in either entropy or \\nthe Gini measure. \\n4.5 \\nThe threshold is the value that maximizes the information gain. \\n4.6 \\nThe naïve Bayesian classifier assumes that, for observations in a \\nclass, feature values are independent. \\n4.7 \\nThe ensemble method is a way of combining multiple algorithms \\nto make a single prediction. \\n4.8 \\nA random forest is an ensemble of decision trees. The different de-\\ncision trees are created by using a subset of features or a subset of \\nobservations or by changing threshold values. \\n4.9  \\nBagging involves sampling from observations or features so that \\nthe same algorithm is used on different training sets. Boosting in-\\nvolves creating models sequentially with each model attempting \\nto correct the error made by the previous model. \\n4.10 The decision tree algorithm is transparent in that it is easy to see \\nwhy a particular decision was made. \\n4.11  Predict that loans are good only if FICO > 717.5 and Income > \\n$48,750.  The confusion matrix is:  \\n \\n \\nPredict  no default \\nPredict default \\nOutcome positive \\n(no default) \\n12.94% \\n69.19% \\nOutcome negative \\n(default) \\n1.42% \\n16.45% \\n \\n4.12 Conditional on a good loan, the probability density for a FICO score \\nof 660 is \\n1\\n√2𝜋× 31.29\\nexp(−(660 −696.19)2\\n2 × 31.292\\n) = 0.0065 \\n \\nConditional on a good loan, the probability density function for an \\nincome of 40 is \\n1\\n√2𝜋× 59.24\\nexp(−(40 −79.83)2\\n2 × 59.242 ) = 0.0054 \\n  \\n234                                                                                                                                       Answers \\n \\n \\nConditional on a default the probability density for a FICO score of \\n660 is \\n1\\n√2𝜋× 24.18\\nexp(−(660 −686.65)2\\n2 × 24.182\\n) = 0.0090 \\n \\nSimilarly, conditional on a default the probability density for an in-\\ncome of 40 is  \\n1\\n√2𝜋× 48.81\\nexp(−(40 −68.47)2\\n2 × 48.812 ) = 0.0069 \\n \\nThe probability of the loan being good is \\n \\n0.0065 × 0.0054 × 0.8276\\n𝑄\\n= 2.90 × 10−5\\n𝑄\\n \\n \\nwhere Q is the joint probability density of FICO = 660 and income \\n=40. The probability of the loan defaulting is \\n \\n0.0069 × 0.0090 × 0.1724\\n𝑄\\n= 1.07 × 10−5\\n𝑄\\n \\nThe probability of the loan defaulting is therefore \\n1.07\\n1.07 + 2.90 \\nor about 27%. \\n \\n \\nChapter 5 \\n \\n5.1 \\nThe objective in SVM classification is to find a pathway that mini-\\nmizes an objective function which is a function of the cost of viola-\\ntions and the width of the pathway.  The center of the pathway is \\nused to classify observations. \\n5.2 \\nIn a hard margin classification, the objective is to find the widest \\npath that has no misclassified observations (assuming such a path-\\nway exists).  In a soft margin classification, an objective function \\nincorporates a trade-off between the width of the pathway and the \\nextent of the misclassification. \\n5.3 \\nThe initial equations are \\nEnd of Chapter Questions                                                                                                     235 \\n \\n \\n∑𝑤𝑗𝑥𝑗= 𝑏𝑢\\n𝑚\\n𝑗=1\\n \\n∑𝑤𝑗𝑥𝑗= 𝑏𝑑\\n𝑚\\n𝑗=1\\n \\nWithout loss of generality parameters can be scaled so that the \\nequations become  \\n∑𝑤𝑗𝑥𝑗= 𝑏+ 1\\n𝑚\\n𝑗=1\\n \\nand \\n∑𝑤𝑗𝑥𝑗= 𝑏−1\\n𝑚\\n𝑗=1\\n \\n5.4 \\nThe width of the pathway decreases as we give more weight to vi-\\nolations. \\n5.5 \\nFor a positive outcome observation, it is \\n \\nmax (𝑏+ 1 −∑\\n𝑤𝑗\\n𝑚\\n𝑗=1\\n𝑥𝑖𝑗, 0) \\n \\nfor negative outcome observations it is \\n  \\nmax (∑\\n𝑤𝑗\\n𝑚\\n𝑗=1\\n𝑥𝑖𝑗−𝑏+ 1, 0) \\n \\n5.6 \\nWe transform the features and create new features so that linear \\nclassification can be used.   \\n5.7  \\nA landmark is a point in feature space, which may or may not cor-\\nrespond to an observation, and the Gaussian radial bias function is \\na synthetic feature whose value for an observation declines as the \\nobservation becomes further away from landmark. \\n5.8 \\nIn SVM regression the objective is to find a pathway with a pre-\\nspecified width through the space defined by the target and the \\nfeatures. The pathway is designed to include as many observations \\nas possible. Observations outside the pathway give rise to viola-\\ntions. The objective function minimizes the extent of the violations \\nand incorporates some regularization. Similarly to Ridge regres-\\nsion, the regularization is designed to avoid weights that are large \\n236                                                                                                                                       Answers \\n \\n \\n in magnitude. \\n5.9 \\nThe differences are as follows: \\n\\uf0b7 \\nThe relationship between the target and the features is repre-\\nsented by a pathway rather than a single line. \\n\\uf0b7 \\nThe prediction error is counted as zero when an observation \\nlies within the pathway \\n\\uf0b7 \\nErrors for observations outside the pathway are calculated as \\nthe difference between the target value and the closest point \\nin the pathway that is consistent with the feature values. \\n\\uf0b7 \\nThere is some regularization built into the objective function. \\n5.10 \\nThe table produced by Sklearn’s SVM package is as follows. The \\nnumbers produced by Excel may be slightly different. \\n \\nC \\nw1 \\nw2 \\nB \\nLoans mis-\\nclassified \\nWidth of \\npathway \\n0.01 \\n0.042 \\n0.015 \\n3.65 \\n20% \\n44.8 \\n0.001 \\n0.038 \\n0.013 \\n3.40 \\n20% \\n49.8 \\n0.0005 \\n0.019 \\n0.008 \\n1.90 \\n20% \\n96.8 \\n0.0003 \\n0.018 \\n0.004 \\n1.71 \\n30% \\n105.6 \\n0.0002 \\n0.018 \\n0.002 \\n1.63 \\n40% \\n212.7 \\n \\n \\nChapter 6 \\n \\n6.1 \\nA hidden layer is a set of intermediate values used when the out-\\nputs are calculated from the inputs in a neural network.  The set \\nof inputs form the input layer. The set of outputs form the output \\nlayer. A neuron is one element within a hidden layer for which a \\nvalue is calculated. An activation function is the function used for \\nthe calculation of the values at neurons in one layer from values \\nat neurons in the previous layer. \\n6.2 \\nThe sigmoid function for calculating the value at a neuron is \\n \\n𝑓(𝑦) =\\n1\\n1 + 𝑒−𝑦 \\n \\n             where y is a constant (the bias) plus a linear combination of the \\nvalues at the neurons in the previous layer. \\n6.3 \\nThe universal approximation theorem states that any continuous \\nnon-linear function can be approximated to arbitrary accuracy us-\\ning a neural network with one layer. \\nEnd of Chapter Questions                                                                                                     237 \\n \\n \\n6.4 \\nWhen the target is numerical the suggested final activation func-\\ntion is linear. When observations are being classified, the sug-\\ngested activation function is the sigmoid function. \\n6.5 \\nThe learning rate is the size of the step taken down the valley once \\nthe line of steepest descent has been identified. \\n6.6 \\nIf the learning rate is too low the steepest descent algorithm will \\nbe too slow. When it is too high there are liable to be oscillations \\nwith the minimum not being found. \\n6.7 \\nThe results for the validation set are produced at the same time as \\nthe results for the training set. The algorithm is stopped when the \\nresults for the validation set start to worsen. This is to avoid over-\\nfitting.  \\n6.8 \\nWhen a derivative is normally valued using a computationally \\nslow numerical procedure such as Monte Carlo simulation, an \\nANN can be created for valuation. Data relating the derivative’s \\nvalue to the inputs is created using the numerical procedure. The \\nANN is then trained on the data and used for all future valuations.  \\n6.9 \\nIn a regular ANN the value at a node in one layer is related to val-\\nues at all nodes in the previous layer. In a CNN it is related to a \\nsmall subset of the nodes in the previous layer. \\n6.10 \\nIn an RNN there is a time sequence to the data. The nodes in one \\nlayer are related to values calculated for the same nodes at the \\nprevious time as well as to the nodes in the previous layer. \\n6.11  The number of parameters is 6×10+10×11×1+11×1= 181 \\n6.12 \\nWhen the starting point is 1.0000 and the learning rate is 0.0002 \\nwe obtain: \\n \\nIteration \\nValue of b \\nGradient \\nChange in b \\n0 \\n1.0000 \\n−11,999 \\n2.3998 \\n1 \\n3.3998 \\n−4342.2 \\n0.8684 \\n2 \\n4.2682 \\n−1571.4 \\n0.3143 \\n3 \\n4.5825 \\n−568.6 \\n0.1137 \\n4 \\n4.6962 \\n−205.8 \\n0.0412 \\n5 \\n4.7374 \\n−74.5 \\n0.0149 \\n6 \\n4.7523 \\n−26.9 \\n0.0054 \\n7 \\n4.7577 \\n−9.8 \\n0.0020 \\n8 \\n4.7596 \\n−3.5 \\n0.0007 \\n9 \\n4.7603 \\n−1.3 \\n0.0003 \\n10 \\n4.7606 \\n−0.5 \\n0.0001 \\n11 \\n4.7607 \\n−0.2 \\n0.0000 \\n12 \\n4.7607 \\n−0.1 \\n0.0000 \\n \\n238                                                                                                                                       Answers \\n \\n \\n6.13 The sigmoid function has an argument of zero so that V1 = V2 = V3 = \\n0.5. The house value is calculated as 3 × 0.5 × 100 = 150. \\n \\n \\nChapter 7 \\n \\n7.1 \\nIn reinforcement learning the objective is to calculate the best \\nstrategy for taking a sequence of decisions through time in a \\nchanging environment.  Supervised learning involves one or more \\nestimates being made from features at a single point in time.  \\n7.2 \\nExploitation involves taking the best action identified so far. Ex-\\nploration involves randomly selecting a different action. If an al-\\ngorithm just involved exploitation it might never find the best ac-\\ntion. If it just involved exploration, it would not benefit from what \\nit has learned. \\n7.3 \\nDynamic programming involves working from the horizon date  \\nback to the beginning, working out the best action for each of the \\nstates that can arise. \\n7.4  \\nThe optimal strategy is to leave your opponent with 4n+1 matches \\nwhere n is an integer. When there are 8 matches the optimal strat-\\negy is to pick up 3 matches. After 1000 games this has been iden-\\ntified as the best strategy, but not convincingly so. After 5,000 and \\n25,000 games, the best decisions become more clearly differenti-\\nated.  \\n7.5  \\nIn the Monte Carlo approach each trial involves actions being \\ntaken with exploration and exploitation. The new observation on \\nthe value of taking a particular action in a particular state is the \\ntotal future reward (possibly discounted) from the time of the ac-\\ntion until the horizon date \\n7.6 \\nIn temporal difference learning each trial involves actions being \\ntaken with exploration and exploitation. The new observation on \\nthe value of taking a particular action in a particular state is de-\\ntermined by looking one period ahead and using the most recent \\nestimate of the value of being in the state that will be reached.   \\n7.7 \\nWhen there are many actions or many states (or both) the ac-\\ntion/state matrix does not fill up quickly and values can be esti-\\nmated using an ANN. \\n7.8 \\nDeep Q-learning is when an ANN is used in conjunction with tem-\\nporal difference learning.  \\n7.9   \\nIn the case of the Monte Carlo approach we update as follows: \\n \\nQ(8,1) = 0.272+0.05(1.000−0.272) = 0.308 \\nEnd of Chapter Questions                                                                                                     239 \\n \\n \\nQ(6,1) = 0.155+0.05(1.000−0.155) = 0.197 \\nQ(4,1) = 0.484+0.05(1.000−0.484) = 0.510 \\nQ(2,1) = 0.999+0.05(1.000−0.999) = 0.999 \\n \\nIn the case of the temporal difference approach we update as fol-\\nlows: \\n \\nQ(8,1) = 0.272+0.05(0.155−0.272) = 0.266 \\nQ(6,1) = 0.155+0.05(1.000−0.155) = 0.197 \\nQ(4,1) = 0.484+0.05(0.999−0.484) = 0.510 \\nQ(2,1) = 0.999+0.05(1.000−0.999) = 0.999 \\n \\n \\nChapter 8 \\n \\n8.1 \\nA sentiment analysis involves processing textual data from such \\nsources as social media and surveys to determine whether it is \\npositive, negative, or neutral about a particular product, company, \\nperson, event, etc. \\n8.2  \\nThere are some publicly available data sets where opinions have \\nbeen labeled. These are sometimes used for training. Otherwise, it \\nis necessary to manually label the opinions used for training and \\ntesting. \\n8.3 \\nThe text must be split into words. Punctuation must be removed. \\nVery common words, such as “the”, “a” and “and” (referred to as \\nstop words) can be removed.  Stemming can be applied to replace \\na word by its stem (e.g., “sleeping” by “sleep”). Lemmatization can \\nbe used to reduce a word to its root (e.g., “better” to “good”). \\nSpelling mistakes can be corrected. Abbreviations can be replaced \\nby the full word. Rare words can be removed.  \\n8.4  \\nStemming involves removing suffices such as “ing” and “s”. Lem-\\nmatization involves searching for the root word. \\n8.5       A bag-of-words model represents text by the frequency with \\nwhich words occur.  \\n8.6 \\nNegatives such as “not” mean that wrong conclusions are liable to \\nbe reached if a bag-of-words model merely looks at the frequency \\nwith which a single word appears. An improvement is to look at \\nword pairs (bigrams). \\n8.7 \\nThe naïve Bayes classifier assumes that the occurrence of one \\nword is independent of the occurrence of another word. \\n240                                                                                                                                       Answers \\n \\n \\n8.8 \\nA trigram is a group of three words. \\n8.9 \\n(a) Logistic regression provides a probability of a positive and neg-\\native sentiment while SVM does not. \\n \\n(b) Logistic regression does not require the assumption that the \\noccurrence of one word is independent of the occurrence of an-\\nother word. \\n8.10 Laplace smoothing is designed to deal with the problem that a \\nword appears in an opinion but not in one class of the training set \\nobservations. It changes the probability of that class from zero to \\na small number. \\n8.11 TF of a word in a document is the number of times the word ap-\\npears in the document divided by the number of words in the doc-\\nument. IDF of a word is log(N/n) where N is the total number of \\ndocuments and n is the number of documents containing the word. \\nIn information retrieval, TF is multiplied by IDF to provide, for \\neach document that might be retrieved, a score for each word in a \\nsearch request. \\n8.12 A word vector is a set of numbers describing the meaning of a \\nword. It does this by quantifying the extent to which the word \\ntends to appear in close proximity to other words. \\n8.13 In this case, p1 = 0.667, p2 = 0.5, p3 = 0.667, q1 = 0.5, q2 = 0.25, and \\nq3 = 0.75. Also Prob(Pos) = 0.6 and Prob(Neg) = 0.4. The probabil-\\nity of a positive sentiment is: \\n \\n0.667 × 0.5 × 0.667 × 0.6\\n0.667 × 0.5 × 0.667 × 0.6 + 0.5 × 0.25 × 0.75 × 0.4 = 0.78 \\n \\n \\nChapter 9 \\n \\n9.1 \\nNeural networks, SVM models, and ensemble models are difficult \\nto interpret. \\n9.2 \\nThe weights of a linear model have a simple interpretation. They \\nshow the effect of changing the value of one feature while keeping \\nthe others the same. \\n9.3  \\nFrom Table 9.1, each extra square foot of lot size is worth $0.3795. \\nit follows that an extra 5,000 square feet is worth 5,000 × $0.3795 \\nor $1,897.50. \\n9.4 \\nIt is the minus the sensitivity of a positive outcome or \\n \\n−\\nexp[− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)]\\n{1 + exp [− (𝑎+ 𝑏1𝑋1 + 𝑏2𝑋2 + ⋯+ 𝑏𝑚𝑋𝑚)]}2 𝑏𝑗𝑢 \\nEnd of Chapter Questions                                                                                                     241 \\n \\n \\n9.5 \\n“Odds against” shows the profit of $1 bet that an event will happen \\nwhen the $1 is forfeited if it does not happen. “Odds on” shows the \\namount that must be staked to provide a $1 profit if an event hap-\\npens and a total loss if it does not happen.  The natural logarithms \\nof odds on and odds against are linear in the features in a logistic \\nregression. \\n9.6 \\nThe contributions calculated for a feature usually assume that the \\nfeature changes with all other features remaining fixed. If there are \\ninteractions between features, it may be unrealistic to assume that \\none feature can change without other features changing. \\n9.7 \\nA partial dependence plot shows what happens on average when \\none feature changes. Results are averaged over random changes in \\nthe other features. \\n9.8 \\nShapley values are designed so that when there is a change the sum \\nof the contributions to the change of each feature equals the total \\nchange. \\n9.9 \\nLIME finds a simple model that fits a complicated model for values \\nof the features that are close to the currently observed values. \\n9.10 4! = 24.  \\n \\n \\nChapter 10 \\n \\n10.1 A call option is an option to buy at a certain price. A put option is \\nan option to sell at a certain price. \\n10.2 The payoffs for a derivative are not symmetrical. The impact of a \\ncertain increase in the price of the underlying asset is not the \\nsame as the impact of the same decrease in the price of the un-\\nderlying asset. \\n10.3 The moneyness of an option is a measure of the extent to which it \\nseems likely that the option will be exercised. It can be measured \\nby comparing S to K. Traders tend to measure moneyness of an op-\\ntion in terms of its delta. \\n10.4 The six variables are the stock price, strike price, risk-free rate, \\ndividend yield, volatility, and time to maturity. \\n10.5 A portfolio is delta neutral if its value is virtually unchanged when \\na small change is made to the price of the underlying asset with all \\nother variables being kept the same. \\n10.6 An implied volatility is the volatility which, when substituted into \\nthe Black−Scholes−Merton formula (or its extensions) gives the \\nmarket price of an option. \\n242                                                                                                                                       Answers \\n \\n \\n10.7 A volatility surface is a three-dimensional chart showing the rela-\\ntionship between (a) the implied volatility of an option and (b) its \\nstrike price and time to maturity. \\n10.8 Volatilities tends to increase (decrease) when the price of the un-\\nderlying asset decreases (increases). An increase in the price of the \\nunderlying asset causes a call option price to increase. However, \\nthe increase tends to be accompanied by a decrease in volatility \\nwhich reduces the call option price. A decrease in the price of the \\nunderlying asset causes a call option price to decrease. However, \\nthe decrease tends to be accompanied by an increase in volatility \\nwhich increases the call option price. The relationship between the \\nvolatility surface movements and the asset price movements \\ntherefore tends to reduce a trader’s exposure to movements in the \\nasset price. \\n10.9 There are improvements when the cost of trading is not negligible.  \\n10.10 When securities to hedge volatility are traded, bid−offer spreads \\ntend to be high. \\n \\n \\n243 \\n \\n \\n \\n \\n \\n \\n \\nGlossary of Terms \\n \\n \\n \\nAccuracy ratio:  The percentage of observations that are classified \\ncorrectly \\nActivation function: Function used to relate values at neurons in \\none layer to values at neurons in previous layer \\nAdaBoost: Boosting where weights for misclassified observations \\nare increased \\nAdam:  Short for adaptive moment estimation. It is a popular way \\nof choosing learning rates in neural networks \\nAdaptive learning rate: Learning rate in a neural network that is \\ndesigned to adapt to circumstances \\nAdversarial machine learning: Developing data to fool a machine \\nlearning algorithm \\nAgglomerative clustering:  See hierarchical clustering \\nAlphaGo: Program developed by Google to play the board game Go \\nANN: See neural network \\nArtificial intelligence: The development of intelligence artificially. \\nMachine learning is one branch of artificial intelligence. \\nArtificial neural network: See neural network \\nAUC:  Area under the ROC curve \\nAutoencoder: The use of a neural network to reduce the number \\n244                                                                                                                               Glossary \\n \\n \\n \\nof features in data \\nBackpropagation: A fast way of calculating partial derivatives in a \\nneural network by working from the end to the beginning \\nBag-of-words: A natural language processing model where the \\nwords in text are considered without regard to their order \\nBagging: Training the same algorithm on different random subsets \\nof data \\nBayes’ theorem:  A theorem for inverting conditionality, i.e., deriv-\\ning Prob(𝑌|𝑋) from Prob(𝑋|𝑌) \\nBayesian learning:  Using Bayes’ theorem to update probabilities \\nBias: Constant term \\nBias-variance trade-off: The trade-off between using a simple \\nmodel that under-fits the training set and a too complicated \\nmodel that over-fits \\nBigram:  A sequence of two words \\nBlack-box model: Model that is difficult to interpret \\nBoosting: Iterative training procedure where one algorithm at-\\ntempts to correct errors in a previous algorithm \\nCategorical feature: Non-numerical feature that falls into one of a \\nnumber of categories \\nCentroid:  The center of a cluster \\nClass:  One of two or more groups into which data is divided \\nClass imbalance:  Situation where the number of observations in \\none class is quite different from the number in another class \\nClassification model:  Model for dividing observations into classes \\nClassification threshold: Threshold for deciding which class an \\nobservation belongs to \\nCNN: See convolutional neural network \\nClustering:  Process of dividing data into clusters so as to under-\\nstand it better \\nConfusion matrix: A table for classification models showing which \\nprediction for the test set are correct and which are incorrect \\nConvolutional neural network: A type of neural network particu-\\nlarly suitable for image recognition where there are one or \\nmore rectangular arrays of numbers at each layer \\nCost function: The objective function to be minimized \\nDecision Tree: A way of considering features one by one \\nDecoding:  The second part of an autoencoder \\nDeep Q-learning: Combining reinforcement learning with a neural \\nGlossary                                                                                                                                   245 \\n \\n \\n \\nnetwork to determine Q-values \\nDeep reinforcement learning: See deep Q-learning \\nDensity-based clustering: A way of forming non-standard cluster \\npatterns \\nDistribution-based clustering: A way of clustering by fitting ob-\\nservations to a mixture of distributions \\nDropouts: Neurons removed in a gradient descent algorithm to \\nmake training faster \\nDummy variable:  A variable that has a value of 0 or 1 to handle a \\ncategorical feature  \\nDummy variable trap:  A situation where the introduction of \\ndummy variables leads to no unique best fit set of parameters in \\na linear regression. \\nDynamic programming: Calculating optimal decisions by working \\nback from the horizon date \\nElastic Net regression: Combination of Lasso and Ridge regres-\\nsion \\nElbow method:  Procedure for choosing the optimal number of \\nclusters by observing the effect on inertia \\nEncoding: The first part of an autoencoder \\nEnsemble learning: Learning by combining the results from sev-\\neral algorithms \\nEntropy: A measure of uncertainty \\nEpisode: A trial in reinforcement learning \\nEpoch: A set of iterations that make one complete use of the train-\\ning set \\nExploitation: Pursuing the best action identified so far \\nExploration: Choosing an action at random \\nExponentially weighted moving average: An average from past \\nobservations where the weight given to the observation n peri-\\nods ago is \\uf06c times the weight given to the observation n−1 peri-\\nods ago (\\uf06c\\uf020< 1) \\nF1-score: See F-score \\nFactor loading: The amount of a feature in a factor when a princi-\\npal components analysis is carried out \\nFactor score: The amount of a factor in an observation when a \\nprincipal components analysis is carried out \\nFalse negative rate: Percentage of positive outcomes predicted as \\nnegative in classification \\nFalse positive rate: Percentage of negative outcomes predicted as \\n246                                                                                                                               Glossary \\n \\n \\n \\npositive in classification \\nFeature: A variable used in a machine learning algorithm \\nFeature map: A component of a layer in a convolutional neural \\nnetwork \\nFeature scaling: A procedure to ensure that the features are \\nmeasured on similar scales \\nFICO score: Credit score in the United States \\nF-score: A measure calculated from the precision and true positive \\nrate in a classification \\nGap statistic: A way of choosing the number of clusters by com-\\nparing the clustered data with randomly distributed data \\nGDPR: See General Data Protection Regulation \\nGeneral Data Protection Regulation: Regulation introduced by \\nthe European Union \\nGeneralize:  A model developed using the training set generalizes \\nwell if its results are almost as good for the validation set \\nGini measure:  A measure of uncertainty \\nGradient boosting: Boosting where a new predictor is fitted to the \\nerrors of previous predictors. \\nGradient descent algorithm: Calculates minimum by taking steps \\ndown a multi-dimensional valley \\nGreedy action: See exploitation \\nHans: German horse that seemed to be intelligent \\nHard margin classification: SVM classification when there are no \\nviolations \\nHidden layer: A layer of neurons between the input layer and the \\noutput layer \\nHierarchical clustering: A way of building up clusters one obser-\\nvation at a time \\nHyperbolic tangent function: An activation function sometimes \\nused in neural networks \\nHyperparameter: Parameter used to train a model, but not to \\nmake predictions \\nHyperplane:  A boundary separating an n-dimensional space into \\ntwo regions. The boundary has n-1 dimensions \\nImbalanced data set:  See class imbalance  \\nInertia: The within-cluster sum of squares when data is clustered \\nInformation gain: Reduction in uncertainty \\nInput layer: The set of feature values that are input to a neural \\nnetwork \\nGlossary                                                                                                                                   247 \\n \\n \\n \\nInstance:  An observation \\nIteration:  A single update of weights during training \\nKernel trick: Short cut when new features are created from exist-\\ning features \\nk-means algorithm: An algorithm for finding clusters \\nL1 regularization:  See Lasso regression \\nL2 regularization:  See Ridge regression \\nLabel: Value of a target \\nLandmark: A point in feature space used to create a new feature \\nLasso regression: Regularization by adding the sum of the abso-\\nlute values of the weights to the objective function in a linear \\nregression \\nLayer: Term used to describe the inputs or the outputs or a set of \\nintermediate neurons in a neural network \\nLearning rate: Step size in a gradient descent algorithm \\nLearning rate decay:  Reduction in learning rate as the gradient \\ndescent algorithm proceeds \\nLemmatization: Mapping a word to its root word. For example, \\n“better” could be mapped to “good” \\nLIME: A way of interpreting a black-box model for values of fea-\\ntures that are close to their current value  \\nLinear regression: Regression where relationship between target \\nand features is assumed to be linear \\nLogistic regression: Version of regression used for classification \\nLong short-term memory: A way of testing the importance of val-\\nues from previous observations and using them for updating as \\nappropriate \\nLSTM: See long short-term memory \\nMachine learning:  Creating intelligence by learning from large \\nvolumes of data \\nMAE:  See mean absolute error \\nMaximum likelihood method: Determination of parameters by \\nmaximizing the probability of observations occurring \\nMean squared error:  The arithmetic average of the squares of the \\nerrors  \\nMini-batch stochastic gradient descent: Gradient descent where \\na subset of data is used on each iteration \\nMin-max scaling: Feature scaling by subtracting the minimum \\nfeature value and dividing by the difference between the maxi-\\nmum and the minimum feature values \\n248                                                                                                                               Glossary \\n \\n \\n \\nMomentum: The use of exponentially weighted moving averages \\nfor gradients in gradient descent algorithms \\nMonte Carlo method:  Updating Q-values in reinforcement learn-\\ning by looking at total future rewards (possibly with discount-\\ning) \\nMSE: See mean squared error \\nMulti-armed bandit problem: The problem of choosing which is \\nbest of a number of levers that give rewards \\nNaïve Bayes classifier: A way of calculating conditional prob-\\nabilities (or conditional values for a numerical variable) if fea-\\ntures in a category are independent \\nNatural language processing: The application of machine learn-\\ning to written or spoken words \\nNegative class: One of two groups into which data is divided \\nNeural network: Network of functions used to create a non-linear \\nmodel relating targets to features \\nNeuron: Node containing an intermediate value in a neural net-\\nwork \\nNim: Game to illustrate reinforcement learning \\nNLP: See natural language processing \\nNormalization:  Scaling feature values to make them comparable \\nn-step bootstrapping: Updating Q-values in reinforcement learn-\\ning by observing value after n time steps \\nObjective function:  A function that is to be maximized or mini-\\nmized by an algorithm \\nOne-hot encoding: Conversion of a categorical feature to numeri-\\ncal variables \\nOutliers:  Observations that are distant from most other observa-\\ntions  \\nOutput layer: The set of targets that are output from a neural net-\\nwork \\nOver-fitting:  A model that fits noise in the training set and does \\nnot generalize well to the validation set \\nPartial dependence plot: Plot of average value of target against a \\nfeature when other features are allowed to take random values \\nPositive class:  One of two groups into which data is divided \\nPrecision: Percentage of positive predictions that turn out to be \\npositive in a classification \\nPrincipal components analysis: A way of replacing data on corre-\\nlated features with a smaller number of uncorrelated factors \\nProject Maven: Project between Google and U.S. Department of \\nGlossary                                                                                                                                   249 \\n \\n \\n \\nDefense which was canceled by Google \\nP-value: Probability in a linear regression of obtaining a t-statistic \\nas large as the one observed if the corresponding feature has no \\nexplanatory power \\nQ-learning: Learning the optimal actions for different states in re-\\ninforcement learning  \\nQ-value:  The best value identified so far for a particular state and \\naction in reinforcement learning \\nRadial bias function: Function of the distance of an observation \\nfrom a landmark. Used to create a new feature to determine a \\nnon-linear pathway in SVM \\nRandom forest: Ensemble of decision trees \\nRBF: See radial bias function \\nRecall: See true positive rate \\nReceiver Operating Curve: Plot of true positive rate against false \\npositive rate in a classification \\nReceptive field: A rectangular array of nodes in the layer of a con-\\nvolutional neural network \\nRecurrent neural network: Neural network where weights are \\ngiven to values calculated from the immediately preceding ob-\\nservation \\nRegularization: Simplification of a model that can avoid over-\\nfitting and reduce the magnitude of weights \\nReinforcement learning: Developing a multistage decision strate-\\ngy in a changing environment \\nRelu: An activation function used in neural networks \\nReward:  Payoff in reinforcement learning \\nRidge regression: Regularization by adding the sum of the \\nsquared weights to objective function in a linear regression \\nRMSE:  See root mean squared error \\nRNN: See recurrent neural network \\nROC:  See receiver operating curve \\nRoot mean squared error:  Square root of the mean squared er-\\nror \\nR-squared statistic: Proportion of variance in target explained by \\nthe features in a linear regression \\nSemi-supervised learning: Predicting the value of the target \\nwhen only part of the available training data includes values for \\nthe target \\nSensitivity: See True positive rate \\n250                                                                                                                               Glossary \\n \\n \\n \\nSentiment analysis:  Determining a group’s attitude (positive or \\nnegative) to a service, product, organization, or topic \\nShapley values: Values that determine the contributions of fea-\\ntures to a change in such a way that the total of the contribu-\\ntions equals the total change \\nSigmoid function: S-shaped function with values between zero \\nand one used in logistic regression and neural networks \\nSilhouette method: A way of calculating the number of clusters \\nbased on the distances between observations \\nSMOTE: Synthetic Minority Over-sampling Technique. A way of \\novercoming class imbalance.  \\nSoft margin classification: SVM classification when there are vio-\\nlations \\nSpecificity: See true negative rate \\nSpoofing: Illegal attempt to manipulate the market \\nStandardization:  See normalization \\nStemming: Removing suffices such as “ing” from a word \\nStep size:  See learning rate \\nStop word: A word such as “the” or “a” that does not add much to \\nthe meaning of text   \\nStopping rule: A rule for stopping learning in a neural network \\nwhen validation results worsen \\nSupervised learning: Predicting the value of one or more targets \\nSupport vector: Observation at the edge of the pathway \\nSVM classification: Construction of a pathway to classify observa-\\ntions \\nSVM regression: Using a pathway to predict the value of a contin-\\nuous target variable \\nTarget: A variable about which predictions are made \\nTay: Program introduced by Microsoft for interacting with female \\nteenagers \\nTemporal difference learning: Updating Q-values in reinforce-\\nment learning by observing the value at the next time step \\nTest set: Data set used to determine the accuracy of the model that \\nis finally chosen \\nTokenization: Splitting text into its constituent words \\nTraining set:  Data set used to estimate parameters for models \\nthat are tested \\nTrue negative rate:  Percentage of negative outcomes correctly \\npredicted in a classification \\nGlossary                                                                                                                                   251 \\n \\n \\n \\nTrue positive rate: Percentage of positive outcomes correctly \\npredicted in a classification \\nt-statistic: Value of a parameter divided by its standard deviation \\nin a linear regression \\nUnder-fitting:  Using a model that does not capture all key rela-\\ntionships in the training set \\nUniversal approximation theorem: A theorem showing that a \\nneural network with a single hidden layer can reproduce any \\ncontinuous function to arbitrary accuracy if it has enough neu-\\nrons \\nUnlabeled data:  Data without target values \\nUnsupervised learning: Finding patterns in data, often by using \\nclustering \\nValidation set: Data set used to determine how well a model de-\\nrived from the training set works on different data \\nWeight: Coefficient of a feature value in a linear or logistic re-\\ngression or coefficient of value at one layer to determine \\nvalue at the next layer in a neural network \\nWhite-box model: Model that is easy to interpret \\nZ-score scaling: Feature scaling by subtracting the mean and di-\\nviding by the standard deviation \\n \\n \\n\\xa0\\n \\n253 \\n \\n \\n \\n \\n \\n \\n  Index \\n    References to items in Glossary of Terms are bolded \\nAccuracy ratio, 73-74, 243 \\nActivation function, 122, 243 \\nAdaBoost, 99, 243 \\nAdam, 132, 243 \\nAdaptive learning rate, 132, 243   \\nAdversarial machine learning, 221-222, 243 \\nAgglomerative clustering, 37-38, 243 \\nAI, See artificial intelligence \\nAlexa, 165 \\nAlphaGo, 159, 217, 243 \\nANN, See artificial neural network \\nArea under curve (AUC), 74-75, 89 \\nArtificial neural network, See neural network  \\nArtificial intelligence, 1-2, 243 \\nAt-the-money, 201 \\nAUC, See area under curve \\nAutoencoder, 138-140, 243 \\nBackpropagation, 130, 244 \\nBagging, 98-99, 244 \\nBag-of-words model, 170-176, 244 \\n254                                                                                                                                               Index \\n \\n \\nBalanced data set, 69-70, 103-104 \\nBayes’ theorem, 16-19, 91, 244 \\nBayesian learning, 91-94, 244 \\nBeautiful soup, 167 \\nBellman, Richard, 157 \\nBias, 50, 123, 186, 219-220, 244   \\nBiases, human, 219-220 \\nBias-variance trade-off, 14, 244 \\nBid−ask spread, 210-211 \\nBigram, 172, 246 \\nBlack-box model, 192-193, 244 \\nBlack−Scholes-Merton model, 4, 133-136, 202-204 \\nBoosting, 99, 244 \\nCall option, 200 \\nCambridge Analytica, 218 \\nCategorical feature, 15, 52-53, 62, 186, 244 \\nCentroid, 26, 244 \\nChess, 2, 6 \\nClass imbalance, 69-70, 103-104, 244 \\nClassification model, 5, 67-76, 244 \\nClassification threshold, 71-72, 244 \\nCluster center, 26 \\nCluster, choosing number of, 28-30  \\nClustering, 23-39, 244 \\nCNN, See convolutional neural network \\nCoca Cola, 166 \\nCollateral, 213 \\nComputational linguistics, 165 \\nConditional probability, 16-19 \\nConfusion matrix, 72-76, 89-90, 244 \\nContract law, 222 \\nConvolutional neural network, 140-142, 244 \\nCost function, 123, 244 \\nCountry risk, 31-35 \\nCredit decisions, See Lending Club example \\nIndex                                                                                                                                              255 \\n \\n \\nCurse of dimensionality, 31 \\nData cleaning, 14-16 \\nData privacy, 218 \\nData science, 2-3 \\nDecision tree, 81-100, 176-177, 244  \\nDecoding, 138, 244 \\nDelta, 202-203, 208-211 \\nDelta-neutral, 202 \\nDeep Q-learning, 159, 244  \\nDeep reinforcement learning, See deep Q-learning \\nDensity-based clustering, 39, 245 \\nDerivatives, 133-137, 161, 199-214 \\nDimensionality, curse of, 31 \\nDiscount factor, 153 \\nDistance measure, 25-27, 31 \\nDistribution-based clustering, 38, 245 \\nDriverless cars, 6, 159, 220-222 \\nDropouts, 132, 245 \\nDummy variable, 52, 63, 67, 71, 245 \\nDummy variable trap, 53, 245 \\nDuplicate observations, 15 \\nDynamic programming, 157-158, 245  \\nEfficient markets hypothesis, 167 \\nElastic Net regression, 60-61, 66, 69, 245 \\nElbow method, 28-29, 245 \\nEncoding, 138, 245 \\nEnsemble learning, 98-99, 245 \\nEntropy, 83-84, 245 \\nEpoch, 131, 245 \\nEthics, 220-221 \\nEuclidean distance, 25-26, 31 \\nExploitation, 148, 245 \\nExploration parameter, 148-152 \\nExploration, 148, 245 \\nF1-score, See F-score \\n256                                                                                                                                               Index \\n \\n \\nFacial recognition, 219  \\nFactor, 40-42 \\nFactor loading, 40-41, 245 \\nFactor score, 40-41, 245 \\nFalse negative rate, 73-74, 90, 245 \\nFalse positive rate, 73-74, 90, 245 \\nFeature, 5, 20, 246 \\nFeature map, 142, 246 \\nFeature scaling, 24-25, 32-33, 246 \\nFICO score, 71, 86-88, 246 \\nFraudulent transactions, 17, 212 \\nF-score, 73-74, 246 \\nGap statistic, 30, 246 \\nGaussian radial bias function, 113 \\nGDPR, See General Data Protection Regulation \\nGeneral Data Protection Regulation, 184, 218, 246 \\nGillette, 166 \\nGini measure, 84-85, 246 \\nGNMT, See Google Neural Machine Translation \\nGo, 2, 6, 159, 217 \\nGoogle Neural Machine Translation, 2, 165, 180 \\nGradient boosting, 99, 246 \\nGradient descent algorithm, 50-51, 126-133, 246  \\nGreedy action, See exploitation \\nHans, 184-185, 221, 246 \\nHard margin classification, 103-109, 246  \\nHealthcare, 159-160 \\nHedge funds, 1 \\nHedging, 161, 210-213 \\nHidden layer, 122, 125, 246  \\nHierarchical clustering, 37-38, 246 \\nHouse price example, 62-66, 95-97, 115-116, 121-123 \\nHyperbolic tangent function, 122, 246 \\nHyperparameter, 55, 87-88, 246 \\nHypothesis testing, 19 \\nIndex                                                                                                                                              257 \\n \\n \\nIDF, See inverse document frequency \\nIFRS 9, 212 \\nImage recognition, 141-142 \\nImbalanced data set, See class imbalance  \\nImplied volatility, 204-208 \\nIndustrial revolutions, 223-224 \\nInertia, 28-29, 34, 246 \\nInformation gain, 84-88, 246 \\nInformation retrieval, 177-178 \\nInput layer, 122, 246 \\nIn-the-money, 201 \\nInstance, 7, 20, 247 \\nInterpretability, See model interpretability \\nInverse document frequency, 177-178 \\nInvesting, 160-161, 166-167, 212 \\nIowa house prices, See house price example \\nKernel trick, 114, 247 \\nk-means algorithm, 25-35, 247 \\nk-nearest-neighbors algorithm, 76 \\nL1 regularization, 58, 131, 247 \\nL2 regularization, 54, 131, 247 \\nLabel, 5-6, 20, 247  \\nLabeled data, 5-6, 168-169 \\nLandmark, 113, 247 \\nLanguage translation, 2, 165 \\nLaplace smoothing, 175 \\nLasso regression, 58-60, 64-66, 68, 187, 247 \\nLayer,122, 247 \\nLeaf, 87-88, 97 \\nLearning rate, 128-132, 247 \\nLearning rate decay, 132, 247 \\nLegal issues, 222-223 \\nLemmatization, 170, 247 \\nLending Club example, 70-76, 85-90, 93-94, 123, 191, 212 \\nLIME, 196, 247 \\n258                                                                                                                                               Index \\n \\n \\nLinear model, 10-12 \\nLinear regression, 47-66, 117, 185-189, 247   \\nLinear regression, multiple features, 49-52 \\nLinear regression, one feature, 48-49 \\nLinkage, 45 \\nLiterary Digest, 219 \\nLocal minimum, 131-132 \\nLogistic regression, 66-76, 176-177, 189-192, 247 \\nLogit regression, See logistic regression \\nLSTM, See long short-term memory \\nLong short-term memory, 142-143, 247 \\nmae, See mean absolute error \\nMaximum likelihood method, 68, 177, 247 \\nMean absolute error, 123, 135, 247 \\nMean squared error, 47-48, 64, 95-97, 123, 247 \\nMini-batch stochastic gradient descent, 131, 247 \\nMin-max scaling, 24-25, 247 \\nMissing data, 16 \\nModel interpretability, 183-197 \\nMomentum, 131, 248 \\nMoneyness, 201 \\nMonte Carlo method, 157-158, 248 \\nmse, See mean squared error \\nMulti-armed bandit problem, 148-152, 248  \\nNaïve Bayes classifier, 91-94, 172-176, 248 \\nNatural language processing, 165-180, 212, 248 \\nNatural language toolkit, 167, 169 \\nNeural network, 121-144, 176-177, 206-208, 248 \\nNeuron, 122, 125-126, 248   \\nNim, 154-159, 248 \\nNLP, See natural language processing \\nNLTK, See natural language toolkit \\nNormalization, 24, 248 \\nn-step bootstrapping, 159, 248 \\nNoughts and crosses, See tic tac toe \\nIndex                                                                                                                                              259 \\n \\n \\nOdds against, 190-191 \\nOdds on, 190-191 \\nOne-hot encoding, 52-53, 248 \\nOrder execution, 160, 212-213 \\nOutliers, 15-16, 248 \\nOut-of-sample testing, 6 \\nOut-of-the-money, 201 \\nOutput layer, 122, 248 \\nOver-fitting, 7, 9, 12-14, 248 \\nPartial dependence plot, 193, 248 \\nPasting, 99 \\nPathway, 104-112, 114-116 \\nPCA, See principal components analysis \\nPension funds, 1 \\nPixel, 141 \\nPolynomial model, 7-11, 51, 55-61 \\nPortfolio management, 160-161, 166-167 \\nPrecision, 73-74, 248 \\nPre-processing, 169-170 \\nPrincipal components analysis, 39-43, 138-140, 248 \\nPrivate equity, 212 \\nProject Maven, 220, 248 \\nPut option, 200 \\nP-value, 51-52, 249 \\nPythagoras’ theorem, 25 \\nPython, 4 \\nQ-learning, 159, 249  \\nQuadratic model, 10-12 \\nQuadratic programming, 109, 110 \\nQ-value, 149-159, 249  \\nRadial bias function, 113, 249 \\nRandom forest, 99, 249 \\nRBF, See radial bias function \\nRecall, See true positive rate \\nReceiver operating curve, 74-75, 89-90, 249 \\n260                                                                                                                                               Index \\n \\n \\nReceptive field, 141, 249 \\nRecurrent neural network, 142-143, 249 \\nRegularization, 53-61, 114-117, 131, 187, 249 \\nRegression statistics, 51-52 \\nReinforcement learning, 6, 20, 147-162, 210-213, 249 \\nRelu function, 122, 249 \\nRenaissance Technologies, 167 \\nResource management, 159 \\nRewards, in reinforcement learning, 147, 152-153, 249 \\nRidge regression, 54-58, 64-65, 68, 187, 249   \\nrmse, See root mean square error \\nRNN, See recurrent neural network \\nROC, See receiver operating curve \\nRoot mean square error, 7, 12, 97, 249 \\nR-squared statistic, 51, 186, 249 \\nSalary vs. age example, 7-13, 48-49, 55-61, 126-130 \\nScaling, See feature scaling \\nScreen scraping, See web scraping \\nSemi-supervised learning, 5-6, 20, 249 \\nSensitivity, See true positive rate \\nSentiment analysis, 166-177, 250 \\nSequential decisions, 147-148 \\nShapley values, 193-195, 250 \\nSigmoid function, 67, 122-126, 250 \\nSilhouette, average score, 30, 35 \\nSilhouette method, 29-30, 250 \\nSiri, 165 \\nSMOTE, 69, 250 \\nSpam, 168 \\nSocial Credit System, 220 \\nSoft margin classification, 109-112, 250 \\nSpecificity, See true negative rate \\nSpoofing, 221-222, 250 \\nStandardization, 24, 250 \\nStemming, 169-170, 250 \\nIndex                                                                                                                                              261 \\n \\n \\nStop word, 169, 250 \\nStopping rule, 133, 250 \\nStrike price, 200 \\nSupervised learning, 4-5, 20, 47-144, 250 \\nSupport vector, 105, 250 \\nSVM classification, 103-114, 176-177, 250 \\nSVM classification, linear, 103-112 \\nSVM classification, non-linear, 112-114 \\nSVM regression, 114-117, 250 \\nTarget, 5, 20, 250 \\nTay, 221, 250 \\nTemporal difference learning, 157-159, 250  \\nTerm frequency, 177-178 \\nTest set, 7, 12-13, 62, 70, 168, 250 \\nTF, See term frequency \\nTF-IDF, 177-178 \\nThreshold, 84-88, 95-96 \\nTic tac toe, 2 \\nTokenization, 169, 250 \\nTrading strategy, 160-161 \\nTraffic lights, 159 \\nTraining set, 7-14, 62, 70, 135, 168, 250 \\nTransparency, 183-197, 221 \\nTrigram, 172 \\nTrolley problem, 221 \\nTrue negative rate, 73-74, 90, 250 \\nTrue positive rate, 73-74, 90, 251 \\nt-statistic, 51-52, 186, 251 \\nUnder-fitting, 12-14, 251 \\nUnigram, 172 \\nUnited Airlines, 166 \\nUniversal approximation theorem, 124, 251 \\nUnlabeled data, 5-6, 251 \\nUnsupervised learning, 5, 20, 23-44, 251 \\nValidation set, 7-14, 62, 70, 135, 168, 251 \\n262                                                                                                                                               Index \\n \\n \\nVoice recognition, 141 \\nVolatility, 201-208 \\nVolatility feedback effect hypothesis, 206 \\nVolatility surface, 203-208 \\nWard’s method, 37 \\nWeb data extraction, See web scraping \\nWeb harvesting, See web scraping \\nWeb scraping, 167 \\nWeight, 50, 122, 251 \\nWhite-box model, 185, 251 \\nWord embedding, 179 \\nWord sequences, 179-180 \\nWord vector, 179 \\nZ-score normalization, See Z-score scaling \\nZ-score scaling, 24-25, 33, 55-56, 64, 187, 206-207, 251 \\nZuckerberg, Mark, 218 \\n \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f404b4",
   "metadata": {
    "papermill": {
     "duration": 0.008551,
     "end_time": "2025-03-04T18:22:15.181186",
     "exception": false,
     "start_time": "2025-03-04T18:22:15.172635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6656602,
     "sourceId": 10735705,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 67.758761,
   "end_time": "2025-03-04T18:22:15.809688",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-04T18:21:08.050927",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
